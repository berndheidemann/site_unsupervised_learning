{"config":{"lang":["de"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Unsupervised Learning","text":"<p>Willkommen zur Lernsituation Unsupervised Learning!</p> <p>Diese Lernmaterialien richten sich an angehende Fachinformatiker/innen f\u00fcr Daten- und Prozessanalyse und vermitteln die grundlegenden Techniken des maschinellen Lernens ohne Labels.</p>"},{"location":"#dein-lernpfad","title":"\ud83d\uddfa\ufe0f Dein Lernpfad","text":"<pre><code>flowchart LR\n    A[Phase 1&lt;br/&gt;Einf\u00fchrung] --&gt; B[Phase 2&lt;br/&gt;Anwendung]\n    B --&gt; C[Phase 3&lt;br/&gt;Vertiefung]\n    C --&gt; D[Phase 4&lt;br/&gt;Projekt]\n\n    A1[UL-01 bis UL-06&lt;br/&gt;Country-Data] --&gt; A\n    B1[UL-07&lt;br/&gt;Mall Customers] --&gt; B\n    C1[UL-08&lt;br/&gt;Wine Quality] --&gt; C\n    D1[UL-09/10&lt;br/&gt;Spotify Tracks] --&gt; D</code></pre>"},{"location":"#struktur-der-materialien","title":"\ud83d\udcda Struktur der Materialien","text":""},{"location":"#infoblatter-nachschlagewerke","title":"Infobl\u00e4tter (Nachschlagewerke)","text":"<p>Die Infobl\u00e4tter dienen als Referenz und erkl\u00e4ren Konzepte und Syntax.</p> Nr. Infoblatt Thema I-01 Einf\u00fchrung Unsupervised Learning Supervised vs. Unsupervised, Anwendungsf\u00e4lle I-02 Datenvorverarbeitung Skalierung, Feature Selection I-03 K-Means Clustering Algorithmus, Elbow-Methode, Silhouette I-04 Hierarchisches Clustering Dendrogramme, Linkage-Methoden I-05 PCA (Dimensionsreduktion) Hauptkomponentenanalyse, Varianz I-06 Weitere Algorithmen DBSCAN, GMM, Vergleich I-07 Cluster-Evaluation Metriken, Interpretation I-08 Gro\u00dfe Datenmengen Sampling, Mini-Batch K-Means"},{"location":"#arbeitsblatter-ubungen","title":"Arbeitsbl\u00e4tter (\u00dcbungen)","text":"<p>Die Arbeitsbl\u00e4tter enthalten praktische Aufgaben mit steigendem Schwierigkeitsgrad.</p> Phase 1: Einf\u00fchrungPhase 2-4: Anwendung &amp; ProjektOptional Nr. Arbeitsblatt Thema Datensatz Level UL-01 Einf\u00fchrung Was ist Clustering? Country-data \u2b50 UL-02 Vorverarbeitung Skalierung, Exploration Country-data \u2b50 UL-03 K-Means Algorithmus, Elbow Iris + Country \u2b50\u2b50 UL-04 PCA Dimensionsreduktion Iris \u2b50\u2b50 UL-05 Interpretation Cluster-Profile Country-data \u2b50\u2b50 UL-06 Hierarchisch Dendrogramme Country-data \u2b50\u2b50 Nr. Arbeitsblatt Thema Datensatz Level UL-07 Kundensegmentierung Eigenst\u00e4ndige Analyse Mall Customers \u2b50\u2b50\u2b50 UL-08 Weinqualit\u00e4t Algorithmenvergleich Wine Quality \u2b50\u2b50\u2b50 UL-09 Musik-Clustering Projektvorbereitung Spotify Tracks \u2b50\u2b50\u2b50\u2b50 UL-10 Abschlussprojekt Vollst\u00e4ndige Analyse Spotify Tracks \u2b50\u2b50\u2b50\u2b50\u2b50 Nr. Arbeitsblatt Thema Datensatz Level UL-OPT-01 Kreditkarten DBSCAN, GMM CustomerData \u2b50\u2b50\u2b50\u2b50 UL-OPT-02 Big Data Gro\u00dfe Datenmengen US-Accidents \u2b50\u2b50\u2b50\u2b50\u2b50"},{"location":"#lernziele","title":"\ud83c\udfaf Lernziele","text":"<p>Nach Bearbeitung der Materialien kannst du:</p> <p>Wissen</p> <ul> <li> Den Unterschied zwischen Supervised und Unsupervised Learning erkl\u00e4ren</li> <li> Die Funktionsweise von K-Means, Hierarchischem Clustering und DBSCAN beschreiben</li> <li> PCA als Dimensionsreduktionsmethode erkl\u00e4ren</li> <li> Metriken zur Cluster-Evaluation benennen und interpretieren</li> </ul> <p>K\u00f6nnen</p> <ul> <li> Daten f\u00fcr Clustering vorbereiten (Skalierung, Feature Selection)</li> <li> Die optimale Clusteranzahl bestimmen (Elbow, Silhouette)</li> <li> Verschiedene Clustering-Algorithmen mit scikit-learn anwenden</li> <li> Cluster-Ergebnisse visualisieren und interpretieren</li> </ul> <p>Bewerten</p> <ul> <li> Algorithmen f\u00fcr verschiedene Anwendungsf\u00e4lle ausw\u00e4hlen</li> <li> Clustering-Ergebnisse kritisch bewerten</li> <li> Gesch\u00e4ftliche Empfehlungen aus Analysen ableiten</li> </ul>"},{"location":"#datensatze","title":"\ud83d\udcc1 Datens\u00e4tze","text":"Datensatz Beschreibung Umfang Verwendet in <code>Country-data.csv</code> L\u00e4nderdaten (Wirtschaft, Gesundheit) 167 L\u00e4nder, 9 Features UL-01 bis UL-06 <code>Mall_Customers.csv</code> Kundensegmentierung 200 Kunden, 5 Features UL-07 Wine Quality Weinqualit\u00e4t (rot/wei\u00df) ~6.500 Zeilen, 11 Features UL-08 Spotify Tracks Audio-Features von Songs ~100k Tracks UL-09, UL-10 <code>CustomerData.csv</code> Kreditkartennutzung ~900 Kunden, 17 Features UL-OPT-01 US-Accidents Verkehrsunf\u00e4lle USA 7,7 Mio. Datens\u00e4tze UL-OPT-02 <p>Iris-Dataset</p> <p>Das Iris-Dataset ist in scikit-learn integriert und wird mit <code>load_iris()</code> geladen.</p>"},{"location":"#zeitplanung","title":"\u23f1\ufe0f Zeitplanung","text":"Phase Arbeitsbl\u00e4tter Zeitaufwand Phase 1: Einf\u00fchrung UL-01 bis UL-06 6-8 Stunden Phase 2: Anwendung UL-07 2-3 Stunden Phase 3: Vertiefung UL-08 2-3 Stunden Phase 4: Projekt UL-09, UL-10 4-6 Stunden Gesamt 14-20 Stunden"},{"location":"#voraussetzungen","title":"\ud83d\udd27 Voraussetzungen","text":"<p>F\u00fcr die Bearbeitung ben\u00f6tigst du:</p> <ul> <li> Python 3.8 oder h\u00f6her</li> <li> NumPy, Pandas, Matplotlib, Seaborn</li> <li> scikit-learn, scipy</li> <li> Jupyter Notebook (IBM Server oder lokal)</li> </ul> <pre><code># Installation pr\u00fcfen\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nprint(\"Alle Bibliotheken erfolgreich importiert! \u2713\")\n</code></pre>"},{"location":"#tipps-fur-erfolgreiches-lernen","title":"\ud83d\udca1 Tipps f\u00fcr erfolgreiches Lernen","text":"<p>So arbeitest du mit den Materialien</p> <p>Dokumentation im Jupyter Notebook:</p> <ul> <li>Beantworte alle Fragen in Markdown-Zellen direkt im Notebook</li> <li>F\u00fcge nach jedem Code-Block eine neue Markdown-Zelle f\u00fcr deine Antworten ein</li> <li>Dokumentiere auch Beobachtungen und Experimente</li> <li>So entsteht automatisch eine vollst\u00e4ndige Dokumentation deiner Arbeit</li> </ul> <pre><code>## Meine Antwort zu Aufgabe 2\n\na) Die Spanne von income ist etwa **500x gr\u00f6\u00dfer** als die von child_mort...\n\nb) Das Feature income dominiert, weil...\n</code></pre> <p>Praktische Tipps</p> <ol> <li>Infobl\u00e4tter parallel nutzen \u2013 Sie erkl\u00e4ren die Theorie zu jedem Arbeitsblatt</li> <li>Visualisieren, visualisieren, visualisieren \u2013 Clustering lebt von Grafiken</li> <li>Fehlerbox beachten \u2013 Jedes AB hat typische Fehler mit L\u00f6sungen</li> <li>Interpretieren \u00fcben \u2013 Cluster finden ist einfach, verstehen ist schwer</li> <li>Reflexionsfragen beantworten \u2013 Sie vertiefen das Verst\u00e4ndnis</li> </ol> <p>H\u00e4ufiger Anf\u00e4ngerfehler</p> <p>Daten nicht skalieren! Clustering-Algorithmen messen Abst\u00e4nde \u2013 ohne Skalierung dominieren Features mit gro\u00dfen Werten.</p> <p>Viel Erfolg bei der Bearbeitung! \ud83c\udf93</p>"},{"location":"arbeitsblaetter/ul-01-einfuehrung/","title":"UL-01: Einf\u00fchrung in Unsupervised Learning","text":"<p>Advance Organizer</p> <p>In diesem Arbeitsblatt lernst du die Grundidee von Unsupervised Learning kennen. Anders als bei Supervised Learning gibt es hier keine \"richtigen Antworten\" \u2013 der Algorithmus findet selbst Muster in den Daten. Diese Technik wird in der Praxis h\u00e4ufig f\u00fcr Kundensegmentierung, Anomalie-Erkennung und Datenexploration eingesetzt.</p> <p>Dein Ziel: Am Ende kannst du erkl\u00e4ren, wann Clustering sinnvoll ist, und hast erste Hypothesen \u00fcber m\u00f6gliche Gruppen im Datensatz formuliert.</p>"},{"location":"arbeitsblaetter/ul-01-einfuehrung/#lernziele","title":"Lernziele","text":"<p>Nach Bearbeitung dieses Arbeitsblatts kannst du:</p> <ul> <li> Den Unterschied zwischen Supervised und Unsupervised Learning erkl\u00e4ren</li> <li> Anwendungsf\u00e4lle f\u00fcr Clustering nennen</li> <li> Einen ersten Datensatz laden und explorieren</li> </ul>"},{"location":"arbeitsblaetter/ul-01-einfuehrung/#aufgabe-1-theorie-was-ist-unsupervised-learning","title":"Aufgabe 1: Theorie \u2013 Was ist Unsupervised Learning?","text":"<p>Lies das Infoblatt Einf\u00fchrung Unsupervised Learning und beantworte folgende Fragen:</p> <ol> <li>Was ist der Hauptunterschied zwischen Supervised und Unsupervised Learning?</li> <li>Nenne drei Anwendungsf\u00e4lle f\u00fcr Clustering aus der Praxis.</li> <li>Was versteht man unter \"Cluster\"?</li> </ol>"},{"location":"arbeitsblaetter/ul-01-einfuehrung/#aufgabe-2-country-datensatz-laden","title":"Aufgabe 2: Country-Datensatz laden","text":"<p>Lade den Country-Datensatz und verschaffe dir einen ersten \u00dcberblick.</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n## Datensatz laden\ndf = pd.read_csv('Country-data.csv')\n\n# Erste Zeilen anzeigen\ndf.head()\n</code></pre> <p>Deine Aufgaben:</p> <p>a) Wie viele L\u00e4nder enth\u00e4lt der Datensatz?</p> <p>b) Welche Features (Spalten) gibt es?</p> <p>c) Welcher Datentyp hat jede Spalte?</p> <pre><code># Anzahl der Zeilen und Spalten\n\n\n# Spalten\u00fcbersicht mit Datentypen\n</code></pre>"},{"location":"arbeitsblaetter/ul-01-einfuehrung/#aufgabe-3-data-dictionary-verstehen","title":"Aufgabe 3: Data Dictionary verstehen","text":"<p>Lies die Datei <code>data-dictionary.csv</code> oder die folgende Tabelle, um die Features zu verstehen:</p> Feature Beschreibung country Name des Landes child_mort Kindersterblichkeit (pro 1000 Geburten) exports Exporte (% des BIP) health Gesundheitsausgaben (% des BIP) imports Importe (% des BIP) income Pro-Kopf-Einkommen inflation Inflationsrate (%) life_expec Lebenserwartung (Jahre) total_fer Geburtenrate (Kinder pro Frau) gdpp BIP pro Kopf <p>Frage: Welche Features k\u00f6nnten zusammenh\u00e4ngen? Notiere deine Vermutungen!</p>"},{"location":"arbeitsblaetter/ul-01-einfuehrung/#aufgabe-4-deskriptive-statistik","title":"Aufgabe 4: Deskriptive Statistik","text":"<p>Erstelle eine \u00dcbersicht der wichtigsten statistischen Kennzahlen.</p> <pre><code># Deskriptive Statistik f\u00fcr alle numerischen Features\ndf.describe()\n</code></pre> <p>Analysiere:</p> <p>a) Welches Feature hat die gr\u00f6\u00dfte Streuung (Standardabweichung)?</p> <p>b) Gibt es Features mit sehr verschiedenen Skalen? (Hinweis: Vergleiche min/max-Werte)</p> <p>c) F\u00fcr welches Feature ist der Unterschied zwischen Median und Mittelwert besonders gro\u00df? Was bedeutet das?</p>"},{"location":"arbeitsblaetter/ul-01-einfuehrung/#aufgabe-5-pairplot-erstellen","title":"Aufgabe 5: Pairplot erstellen","text":"<p>Ein Pairplot zeigt alle paarweisen Zusammenh\u00e4nge. Da wir 9 numerische Features haben, w\u00e4hle zun\u00e4chst nur vier aus:</p> <pre><code># Pairplot f\u00fcr ausgew\u00e4hlte Features\nselected_features = ['child_mort', 'income', 'life_expec', 'gdpp']\n\nsns.pairplot(df[selected_features])\nplt.tight_layout()\nplt.savefig('pairplot_countries.png', dpi=150)\nplt.show()\n</code></pre> <p>Beobachtungen:</p> <p>a) Welche Features korrelieren positiv miteinander?</p> <p>b) Welche Features korrelieren negativ?</p> <p>c) Erkennst du bereits \"Gruppen\" von L\u00e4ndern in den Scatterplots?</p>"},{"location":"arbeitsblaetter/ul-01-einfuehrung/#aufgabe-6-reflexion-mogliche-landergruppen","title":"Aufgabe 6: Reflexion \u2013 M\u00f6gliche L\u00e4ndergruppen","text":"<p>Reflexion</p> <p>Basierend auf deiner bisherigen Exploration:</p> <ol> <li>Welche Gruppen von L\u00e4ndern k\u00f6nnte es geben?</li> <li>Anhand welcher Features w\u00fcrdest du die Gruppen unterscheiden?</li> <li>Wie viele Gruppen vermutest du?</li> </ol> <p>Notiere deine Hypothesen! Du wirst sie sp\u00e4ter mit den Clustering-Ergebnissen vergleichen.</p>"},{"location":"arbeitsblaetter/ul-01-einfuehrung/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"<p>Typische Probleme und L\u00f6sungen</p> <p>\u274c <code>FileNotFoundError: No such file or directory</code> \u2192 Pr\u00fcfe den Dateipfad! Liegt die CSV im richtigen Ordner? <pre><code># Aktuellen Pfad pr\u00fcfen\nimport os\nprint(os.getcwd())\nprint(os.listdir())\n</code></pre></p> <p>\u274c Pairplot zu langsam \u2192 W\u00e4hle nur 3-4 Features aus: <pre><code>sns.pairplot(df[['feature1', 'feature2', 'feature3']])\n</code></pre></p> <p>\u274c Daten nicht verstanden \u2192 Lies zuerst das Data Dictionary! Ohne Verst\u00e4ndnis der Features ist keine sinnvolle Analyse m\u00f6glich.</p> <p>\u274c <code>TypeError</code> bei describe() \u2192 Pr\u00fcfe, ob alle Spalten den erwarteten Typ haben: <pre><code>df.dtypes\n</code></pre></p>"},{"location":"arbeitsblaetter/ul-01-einfuehrung/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das hast du gelernt</p> <ul> <li>Unsupervised Learning findet Muster ohne vorgegebene Labels</li> <li>Der Country-Datensatz enth\u00e4lt Wirtschafts- und Gesundheitsdaten von 167 L\u00e4ndern</li> <li>Erste visuelle Exploration zeigt bereits Hinweise auf m\u00f6gliche Cluster</li> <li>Features haben sehr unterschiedliche Skalen \u2013 das wird wichtig f\u00fcr das n\u00e4chste Arbeitsblatt!</li> </ul>"},{"location":"arbeitsblaetter/ul-01-einfuehrung/#nachste-schritte","title":"N\u00e4chste Schritte","text":"<p>Im n\u00e4chsten Arbeitsblatt lernst du, warum die unterschiedlichen Skalen ein Problem sind und wie du das mit Skalierung l\u00f6st.</p> <p>\u27a1\ufe0f Weiter zu UL-02: Datenvorverarbeitung</p>"},{"location":"arbeitsblaetter/ul-02-vorverarbeitung/","title":"UL-02: Datenvorverarbeitung f\u00fcr Clustering","text":"<p>Advance Organizer</p> <p>Clustering-Algorithmen messen Abst\u00e4nde zwischen Datenpunkten. Wenn ein Feature in Millionen gemessen wird und ein anderes in Prozent, dominiert das gr\u00f6\u00dfere Feature die Berechnung \u2013 das verf\u00e4lscht die Ergebnisse! Hier lernst du, wie du Daten so vorbereitest, dass alle Features fair ber\u00fccksichtigt werden.</p> <p>Dein Ziel: Du verstehst, warum Skalierung notwendig ist, und kannst sie selbstst\u00e4ndig anwenden. Dieses Wissen brauchst du f\u00fcr ALLE folgenden Arbeitsbl\u00e4tter.</p>"},{"location":"arbeitsblaetter/ul-02-vorverarbeitung/#lernziele","title":"Lernziele","text":"<p>Nach Bearbeitung dieses Arbeitsblatts kannst du:</p> <ul> <li> Die Notwendigkeit der Skalierung verstehen und erkl\u00e4ren</li> <li> StandardScaler und MinMaxScaler anwenden</li> <li> Auswirkungen der Skalierung visualisieren</li> </ul>"},{"location":"arbeitsblaetter/ul-02-vorverarbeitung/#aufgabe-1-warum-ist-skalierung-wichtig","title":"Aufgabe 1: Warum ist Skalierung wichtig?","text":"<p>Betrachte zwei Features aus dem Country-Datensatz:</p> <pre><code>import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('Country-data.csv')\n\n# Zwei Features vergleichen\nprint(f\"Income: Min={df['income'].min():.0f}, Max={df['income'].max():.0f}\")\nprint(f\"Child Mort: Min={df['child_mort'].min():.1f}, Max={df['child_mort'].max():.1f}\")\n</code></pre> <p>Fragen:</p> <p>a) Um welchen Faktor ist die Spanne von <code>income</code> gr\u00f6\u00dfer als die von <code>child_mort</code>?</p> <p>b) Wenn ein Clustering-Algorithmus den euklidischen Abstand berechnet, welches Feature dominiert dann?</p> <p>c) Ist das fair, wenn beide Features gleich wichtig f\u00fcr die Analyse sein sollen?</p>"},{"location":"arbeitsblaetter/ul-02-vorverarbeitung/#aufgabe-2-standardscaler-anwenden","title":"Aufgabe 2: StandardScaler anwenden","text":"<p>Der StandardScaler transformiert jedes Feature so, dass es Mittelwert 0 und Standardabweichung 1 hat.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\n# Nur numerische Spalten ausw\u00e4hlen\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\nX = df[numeric_cols]\n\n# StandardScaler anwenden\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# In DataFrame umwandeln\ndf_scaled = pd.DataFrame(X_scaled, columns=numeric_cols)\n</code></pre> <p>Deine Aufgaben:</p> <p>a) Pr\u00fcfe, ob die Transformation korrekt ist:</p> <pre><code># Mittelwert und Standardabweichung der skalierten Daten\nprint(\"Mittelwerte:\")\nprint(df_scaled.mean().round(2))\n\nprint(\"\\nStandardabweichungen:\")\nprint(df_scaled.std().round(2))\n</code></pre> <p>b) Was sollten die Werte sein? Stimmt das Ergebnis?</p>"},{"location":"arbeitsblaetter/ul-02-vorverarbeitung/#aufgabe-3-verteilungen-vornach-skalierung-vergleichen","title":"Aufgabe 3: Verteilungen vor/nach Skalierung vergleichen","text":"<p>Visualisiere, wie sich die Skalierung auswirkt:</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Vorher: Income\naxes[0,0].hist(df['income'], bins=30, edgecolor='black')\naxes[0,0].set_title('Income (Original)')\naxes[0,0].set_xlabel('Wert')\n\n# Nachher: Income\naxes[0,1].hist(df_scaled['income'], bins=30, edgecolor='black', color='orange')\naxes[0,1].set_title('Income (Skaliert)')\naxes[0,1].set_xlabel('Z-Score')\n\n# Vorher: Child Mortality\naxes[1,0].hist(df['child_mort'], bins=30, edgecolor='black')\naxes[1,0].set_title('Child Mortality (Original)')\naxes[1,0].set_xlabel('Wert')\n\n# Nachher: Child Mortality\naxes[1,1].hist(df_scaled['child_mort'], bins=30, edgecolor='black', color='orange')\naxes[1,1].set_title('Child Mortality (Skaliert)')\naxes[1,1].set_xlabel('Z-Score')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Beobachtungen:</p> <p>a) Wie ver\u00e4ndert sich die Form der Verteilung durch Skalierung?</p> <p>b) Wie ver\u00e4ndert sich der Wertebereich?</p> <p>c) Was bedeutet ein Z-Score von 2? Was bedeutet -1?</p>"},{"location":"arbeitsblaetter/ul-02-vorverarbeitung/#aufgabe-4-minmaxscaler-als-alternative","title":"Aufgabe 4: MinMaxScaler als Alternative","text":"<p>Der MinMaxScaler transformiert auf einen Bereich von 0 bis 1:</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\n# Wende den MinMaxScaler analog zum StandardScaler an\nminmax_scaler = MinMaxScaler()\nX_minmax = minmax_scaler.fit_transform(X)\ndf_minmax = pd.DataFrame(X_minmax, columns=numeric_cols)\n\n# Wertebereich pr\u00fcfen\nprint(\"Minimum pro Feature:\")\nprint(df_minmax.min().values)\n\nprint(\"\\nMaximum pro Feature:\")\nprint(df_minmax.max().values)\n</code></pre> <p>Erwartete Ausgabe: Alle Minima sollten 0.0 sein, alle Maxima 1.0.</p> <p>Vergleich StandardScaler vs. MinMaxScaler:</p> Aspekt StandardScaler MinMaxScaler Wertebereich Unbegrenzt (meist -3 bis +3) Fest [0, 1] Umgang mit Ausrei\u00dfern Robust, Ausrei\u00dfer beeinflussen weniger Empfindlich, Ausrei\u00dfer stauchen Daten Wann verwenden? K-Means, PCA, die meisten Algorithmen Neuronale Netze, Bilddaten <p>Wann welchen Scaler?</p> <ul> <li>StandardScaler: Standard f\u00fcr die meisten Clustering-Algorithmen</li> <li>MinMaxScaler: Wenn alle Werte im Bereich [0,1] sein sollen</li> </ul>"},{"location":"arbeitsblaetter/ul-02-vorverarbeitung/#aufgabe-5-korrelationsmatrix-erstellen","title":"Aufgabe 5: Korrelationsmatrix erstellen","text":"<p>Korrelationen helfen, redundante Features zu identifizieren:</p> <pre><code># Korrelationsmatrix berechnen\ncorrelation = df[numeric_cols].corr()\n\n# Als Heatmap visualisieren\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, fmt='.2f')\nplt.title('Korrelationsmatrix')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Analysiere:</p> <p>a) Welche Features korrelieren stark positiv (r &gt; 0.7)?</p> <p>b) Welche Features korrelieren stark negativ (r &lt; -0.7)?</p> <p>c) Sollte man stark korrelierte Features beide behalten? Warum (nicht)?</p>"},{"location":"arbeitsblaetter/ul-02-vorverarbeitung/#aufgabe-6-daten-fur-clustering-vorbereiten","title":"Aufgabe 6: Daten f\u00fcr Clustering vorbereiten","text":"<p>Erstelle einen vorbereiteten Datensatz f\u00fcr die n\u00e4chsten Arbeitsbl\u00e4tter:</p> <pre><code># 1. Nur numerische Spalten\nX = df.select_dtypes(include=[np.number])\n\n# 2. Skalieren\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 3. L\u00e4ndernamen f\u00fcr sp\u00e4tere Zuordnung\ncountries = df['country'].values\n\n# Pr\u00fcfen\nprint(f\"Shape: {X_scaled.shape}\")\nprint(f\"Mittelwert \u2248 0: {X_scaled.mean():.10f}\")\nprint(f\"Std \u2248 1: {X_scaled.std():.4f}\")\n</code></pre> <p>Pr\u00fcfe dein Ergebnis:</p> <p>a) Stimmen Mittelwert (\u22480) und Standardabweichung (\u22481)? Wenn nicht, was k\u00f6nnte falsch sein?</p> <p>b) Warum speichern wir <code>countries</code> separat? Was w\u00fcrde passieren, wenn wir die L\u00e4ndernamen verlieren?</p> <p>Checkpoint</p> <p>Ab jetzt arbeitest du mit <code>X_scaled</code> \u2013 den skalierten Daten!</p>"},{"location":"arbeitsblaetter/ul-02-vorverarbeitung/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"<p>Typische Probleme und L\u00f6sungen</p> <p>\u274c <code>ValueError: could not convert string to float</code> \u2192 Du hast kategoriale Spalten (z.B. L\u00e4ndernamen) nicht entfernt! <pre><code># Nur numerische Features\ndf_numeric = df.select_dtypes(include=[np.number])\n</code></pre></p> <p>\u274c Skalierung auf gesamten DataFrame angewendet \u2192 Die country-Spalte kann nicht skaliert werden. <pre><code># Falsch:\nscaler.fit_transform(df)  # Fehler!\n\n# Richtig:\nscaler.fit_transform(df[numeric_cols])\n</code></pre></p> <p>\u274c Fit und Transform verwechselt \u2192 Erst <code>fit()</code> lernt die Parameter, dann <code>transform()</code> wendet sie an. <pre><code># Erst fit, dann transform (zwei Schritte)\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\n# Oder in einem Schritt:\nX_scaled = scaler.fit_transform(X)\n</code></pre></p> <p>\u274c Originaldaten \u00fcberschrieben \u2192 Speichere skalierte Daten in neuer Variable! <pre><code># Falsch: df = scaler.fit_transform(df)\n# Richtig: X_scaled = scaler.fit_transform(X)\n</code></pre></p>"},{"location":"arbeitsblaetter/ul-02-vorverarbeitung/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das hast du gelernt</p> <ul> <li>Ohne Skalierung dominieren Features mit gro\u00dfen Werten das Clustering</li> <li>StandardScaler transformiert auf Mittelwert=0, Std=1</li> <li>MinMaxScaler transformiert auf den Bereich [0,1]</li> <li>Korrelationsmatrix zeigt redundante Features</li> <li>Immer erst kategoriale Daten entfernen, dann skalieren!</li> </ul>"},{"location":"arbeitsblaetter/ul-02-vorverarbeitung/#nachste-schritte","title":"N\u00e4chste Schritte","text":"<p>Jetzt sind deine Daten bereit f\u00fcr das erste Clustering! Im n\u00e4chsten Arbeitsblatt lernst du K-Means kennen.</p> <p>\u27a1\ufe0f Weiter zu UL-03: K-Means Clustering</p>"},{"location":"arbeitsblaetter/ul-03-kmeans/","title":"UL-03: K-Means Clustering","text":"<p>Advance Organizer</p> <p>K-Means ist DER Standard-Algorithmus f\u00fcr Clustering \u2013 einfach zu verstehen, schnell zu berechnen, und in 90% der F\u00e4lle ein guter Startpunkt. Die Herausforderung: Du musst vorher wissen, wie viele Cluster du suchst! Die Elbow-Methode hilft dir dabei.</p> <p>Dein Ziel: Du kannst K-Means anwenden und die optimale Clusteranzahl begr\u00fcndet w\u00e4hlen. Das ist die Kernkompetenz f\u00fcr alle weiteren Clustering-Aufgaben.</p>"},{"location":"arbeitsblaetter/ul-03-kmeans/#lernziele","title":"Lernziele","text":"<p>Nach Bearbeitung dieses Arbeitsblatts kannst du:</p> <ul> <li> Den K-Means Algorithmus verstehen und erkl\u00e4ren</li> <li> Die Elbow-Methode zur Bestimmung der Clusteranzahl anwenden</li> <li> K-Means Clustering mit scikit-learn durchf\u00fchren</li> </ul>"},{"location":"arbeitsblaetter/ul-03-kmeans/#aufgabe-1-k-means-per-hand-verstehen","title":"Aufgabe 1: K-Means \"per Hand\" verstehen","text":"<p>Bevor wir K-Means anwenden, verstehen wir den Algorithmus am Iris-Dataset \u2013 einem einfachen Datensatz mit nur 4 Features und 3 bekannten Klassen.</p> <pre><code>from sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\n\n# Iris laden\niris = load_iris()\nX_iris = iris.data[:, :2]  # Nur 2 Features f\u00fcr Visualisierung\n\nprint(f\"Shape: {X_iris.shape}\")\nprint(f\"Bekannte Klassen: {iris.target_names}\")\n</code></pre> <p>K-Means Algorithmus Schritt f\u00fcr Schritt:</p> <pre><code>import matplotlib.pyplot as plt\n\n# Schritt 1: Zuf\u00e4llige Startzentren w\u00e4hlen\nk = 3\ncenters = X_iris[np.random.choice(len(X_iris), k, replace=False)]\n\nplt.figure(figsize=(10, 4))\n\n# Iteration 0: Startzustand\nplt.subplot(1, 3, 1)\nplt.scatter(X_iris[:,0], X_iris[:,1], c='gray', alpha=0.5)\nplt.scatter(centers[:,0], centers[:,1], c=['red','green','blue'], marker='X', s=200)\nplt.title('Schritt 1: Zuf\u00e4llige Zentren')\n\n# Schritt 2: Punkte zum n\u00e4chsten Zentrum zuweisen\nfrom scipy.spatial.distance import cdist\ndistances = cdist(X_iris, centers)\nlabels = distances.argmin(axis=1)\n\nplt.subplot(1, 3, 2)\nplt.scatter(X_iris[:,0], X_iris[:,1], c=labels, cmap='viridis', alpha=0.5)\nplt.scatter(centers[:,0], centers[:,1], c=['red','green','blue'], marker='X', s=200)\nplt.title('Schritt 2: Punkte zuweisen')\n\n# Schritt 3: Zentren neu berechnen\nnew_centers = np.array([X_iris[labels==i].mean(axis=0) for i in range(k)])\n\nplt.subplot(1, 3, 3)\nplt.scatter(X_iris[:,0], X_iris[:,1], c=labels, cmap='viridis', alpha=0.5)\nplt.scatter(new_centers[:,0], new_centers[:,1], c=['red','green','blue'], marker='X', s=200)\nplt.title('Schritt 3: Zentren aktualisieren')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Interaktive Visualisierung</p> <p>Probiere den Algorithmus interaktiv aus und beobachte, wie die Zentren wandern: K-Means Visualisierung von Naftali Harris</p> <p>Fragen:</p> <p>a) Was passiert, wenn wir Schritt 2 und 3 wiederholen?</p> <p>b) Wann stoppt der Algorithmus?</p> <p>c) Experimentiere mit der interaktiven Visualisierung: Was passiert bei unterschiedlichen Startpositionen der Zentren?</p> <p>c) Warum ist die Initialisierung (Startzentren) wichtig?</p>"},{"location":"arbeitsblaetter/ul-03-kmeans/#aufgabe-2-elbow-methode-fur-country-daten","title":"Aufgabe 2: Elbow-Methode f\u00fcr Country-Daten","text":"<p>Datensatz-Wechsel</p> <p>In Aufgabe 1 hast du den K-Means Algorithmus am Iris-Dataset verstanden. Jetzt wenden wir das Gelernte auf den Country-Datensatz an \u2013 denselben, den du in UL-01 und UL-02 bereits exploriert und skaliert hast.</p> <p>Wie finden wir die optimale Anzahl an Clustern? Die Elbow-Methode hilft!</p> <pre><code>from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Country-Daten laden und skalieren\ndf = pd.read_csv('Country-data.csv')\nX = df.select_dtypes(include=[np.number])\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Elbow-Methode\ninertias = []\nK_range = range(1, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n\n# Visualisierung\nplt.figure(figsize=(8, 5))\nplt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\nplt.xlabel('Anzahl Cluster (k)')\nplt.ylabel('Inertia (Within-Cluster Sum of Squares)')\nplt.title('Elbow-Methode')\nplt.xticks(K_range)\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre> <p>Analysiere:</p> <p>a) Wo ist der \"Ellbogen\" (Knick) in der Kurve?</p> <p>b) Warum sinkt die Inertia immer weiter, wenn k steigt?</p> <p>c) Welches k w\u00fcrdest du w\u00e4hlen? Begr\u00fcnde!</p>"},{"location":"arbeitsblaetter/ul-03-kmeans/#aufgabe-3-k-means-durchfuhren","title":"Aufgabe 3: K-Means durchf\u00fchren","text":"<p>Jetzt wenden wir K-Means mit der gew\u00e4hlten Clusteranzahl an:</p> <pre><code># K-Means mit optimalem k\nk = 3  # Passe diesen Wert nach deiner Elbow-Analyse an!\n\nkmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(X_scaled)\n\n# Cluster-Labels zum DataFrame hinzuf\u00fcgen\ndf['Cluster'] = labels\n\n# \u00dcbersicht\nprint(df['Cluster'].value_counts())\n</code></pre> <p>Deine Aufgaben:</p> <p>a) \u00c4ndere den Wert von <code>k</code> auf deinen Wert aus der Elbow-Analyse. Wie viele L\u00e4nder sind in jedem Cluster?</p> <p>b) Welche Cluster-Zentren hat der Algorithmus gefunden?</p> <pre><code># Cluster-Zentren (skaliert)\nprint(\"Cluster-Zentren (skaliert):\")\nprint(kmeans.cluster_centers_)\n\n# Zentren zur\u00fcck-transformieren\ncenters_original = scaler.inverse_transform(kmeans.cluster_centers_)\nprint(\"\\nCluster-Zentren (Original-Skala):\")\nprint(pd.DataFrame(centers_original, columns=X.columns))\n</code></pre>"},{"location":"arbeitsblaetter/ul-03-kmeans/#aufgabe-4-silhouette-score-berechnen","title":"Aufgabe 4: Silhouette Score berechnen","text":"<p>Der Silhouette Score misst, wie gut die Cluster getrennt sind:</p> <pre><code>from sklearn.metrics import silhouette_score\n\nscore = silhouette_score(X_scaled, labels)\nprint(f\"Silhouette Score: {score:.3f}\")\n</code></pre> <p>Interpretation:</p> Wert Bedeutung 0.7 - 1.0 Starke Cluster-Struktur 0.5 - 0.7 Vern\u00fcnftige Struktur 0.25 - 0.5 Schwache Struktur &lt; 0.25 Keine klare Struktur <p>Frage: Wie gut ist dein Clustering laut Silhouette Score?</p>"},{"location":"arbeitsblaetter/ul-03-kmeans/#aufgabe-5-verschiedene-k-vergleichen","title":"Aufgabe 5: Verschiedene k vergleichen","text":"<p>Berechne den Silhouette Score f\u00fcr verschiedene k:</p> <pre><code>silhouettes = []\nK_range = range(2, 10)  # min. 2 Cluster f\u00fcr Silhouette\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X_scaled)\n    score = silhouette_score(X_scaled, labels)\n    silhouettes.append(score)\n    print(f\"k={k}: Silhouette Score = {score:.3f}\")\n\n# Visualisierung\nplt.figure(figsize=(8, 5))\nplt.bar(K_range, silhouettes, color='steelblue', edgecolor='black')\nplt.xlabel('Anzahl Cluster (k)')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score f\u00fcr verschiedene k')\nplt.xticks(K_range)\nplt.show()\n</code></pre> <p>Fragen:</p> <p>a) Welches k hat den h\u00f6chsten Silhouette Score?</p> <p>b) Stimmt das mit der Elbow-Methode \u00fcberein?</p> <p>c) Was w\u00e4hlst du als finales k?</p>"},{"location":"arbeitsblaetter/ul-03-kmeans/#aufgabe-6-reflexion-warum-funktioniert-k-means-bei-iris-so-gut","title":"Aufgabe 6: Reflexion \u2013 Warum funktioniert K-Means bei Iris so gut?","text":"<p>Lade das Iris-Dataset und vergleiche die echten Labels mit den K-Means Clustern:</p> <pre><code>from sklearn.datasets import load_iris\n\niris = load_iris()\nX_iris = iris.data\ny_true = iris.target\n\n# K-Means mit k=3\nkmeans = KMeans(n_clusters=3, random_state=42)\nlabels_pred = kmeans.fit_predict(X_iris)\n\n# Vergleich\nfrom sklearn.metrics import adjusted_rand_score\nari = adjusted_rand_score(y_true, labels_pred)\nprint(f\"Adjusted Rand Index: {ari:.3f}\")  # 1.0 = perfekte \u00dcbereinstimmung\n</code></pre> <p>Reflexion:</p> <p>Warum funktioniert K-Means bei Iris so gut? (Tipp: Visualisiere die echten Klassen und die gefundenen Cluster in 2D mit PCA)</p>"},{"location":"arbeitsblaetter/ul-03-kmeans/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"<p>Typische Probleme und L\u00f6sungen</p> <p>\u274c Elbow-Kurve zeigt keinen klaren Knick \u2192 Das ist normal! W\u00e4hle den Punkt, ab dem die Verbesserung deutlich abnimmt.</p> <p>\u274c Unterschiedliche Ergebnisse bei jedem Durchlauf \u2192 Setze <code>random_state=42</code> f\u00fcr Reproduzierbarkeit: <pre><code>KMeans(n_clusters=3, random_state=42)\n</code></pre></p> <p>\u274c Daten nicht skaliert \u2192 K-Means braucht skalierte Daten! Zur\u00fcck zu UL-02.</p> <p>\u274c <code>labels_</code> ist leer oder AttributeError \u2192 Du musst zuerst <code>fit()</code> oder <code>fit_predict()</code> aufrufen! <pre><code># Falsch:\nkmeans = KMeans(n_clusters=3)\nprint(kmeans.labels_)  # Fehler!\n\n# Richtig:\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X_scaled)\nprint(kmeans.labels_)  # Jetzt funktioniert's!\n</code></pre></p> <p>\u274c <code>n_init</code> Warnung in neueren scikit-learn Versionen \u2192 Setze explizit <code>n_init=10</code> oder <code>n_init='auto'</code></p>"},{"location":"arbeitsblaetter/ul-03-kmeans/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das hast du gelernt</p> <ul> <li>K-Means iteriert: Punkte zuweisen \u2192 Zentren aktualisieren \u2192 wiederholen</li> <li>Elbow-Methode: Suche den Knick in der Inertia-Kurve</li> <li>Silhouette Score: Misst Cluster-Qualit\u00e4t (h\u00f6her = besser)</li> <li>Immer <code>random_state</code> setzen f\u00fcr reproduzierbare Ergebnisse</li> <li>K-Means funktioniert gut bei kompakten, kugelf\u00f6rmigen Clustern</li> </ul>"},{"location":"arbeitsblaetter/ul-03-kmeans/#nachste-schritte","title":"N\u00e4chste Schritte","text":"<p>Wie visualisierst du 9 Dimensionen? Im n\u00e4chsten Arbeitsblatt lernst du PCA zur Dimensionsreduktion.</p> <p>\u27a1\ufe0f Weiter zu UL-04: Dimensionsreduktion mit PCA</p>"},{"location":"arbeitsblaetter/ul-04-pca/","title":"UL-04: Dimensionsreduktion mit PCA","text":"<p>Advance Organizer</p> <p>Wie visualisiert man 9 Features gleichzeitig? Gar nicht \u2013 unser Gehirn kann maximal 3 Dimensionen verarbeiten. PCA (Principal Component Analysis) komprimiert viele Features auf wenige \"Hauptkomponenten\", ohne zu viel Information zu verlieren. So kannst du Cluster endlich sehen!</p> <p>Dein Ziel: Du kannst PCA anwenden und verstehst, was die Hauptkomponenten bedeuten. Diese Technik wirst du im Abschlussprojekt brauchen.</p>"},{"location":"arbeitsblaetter/ul-04-pca/#lernziele","title":"Lernziele","text":"<p>Nach Bearbeitung dieses Arbeitsblatts kannst du:</p> <ul> <li> Das PCA-Prinzip verstehen und erkl\u00e4ren</li> <li> Dimensionsreduktion durchf\u00fchren</li> <li> Cluster in 2D visualisieren</li> </ul>"},{"location":"arbeitsblaetter/ul-04-pca/#aufgabe-1-pca-am-iris-dataset","title":"Aufgabe 1: PCA am Iris-Dataset","text":"<p>Das Iris-Dataset hat 4 Features \u2013 perfekt, um PCA zu verstehen:</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Daten laden und skalieren\niris = load_iris()\nX = iris.data\ny = iris.target\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# PCA: 4D \u2192 2D\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"Original Shape: {X_scaled.shape}\")\nprint(f\"Nach PCA Shape: {X_pca.shape}\")\n</code></pre> <p>Frage: Von wie vielen auf wie viele Dimensionen wurde reduziert?</p>"},{"location":"arbeitsblaetter/ul-04-pca/#aufgabe-2-erklarte-varianz-analysieren","title":"Aufgabe 2: Erkl\u00e4rte Varianz analysieren","text":"<p>Wie viel Information haben wir durch die Reduktion verloren?</p> <pre><code># Erkl\u00e4rte Varianz\nprint(\"Erkl\u00e4rte Varianz pro Komponente:\")\nfor i, var in enumerate(pca.explained_variance_ratio_):\n    print(f\"  PC{i+1}: {var:.2%}\")\n\nprint(f\"\\nGesamte erkl\u00e4rte Varianz: {pca.explained_variance_ratio_.sum():.2%}\")\n\n# Visualisierung\nplt.figure(figsize=(8, 4))\nplt.bar(range(1, 3), pca.explained_variance_ratio_, color='steelblue', edgecolor='black')\nplt.xlabel('Hauptkomponente')\nplt.ylabel('Erkl\u00e4rte Varianz')\nplt.title('Erkl\u00e4rte Varianz pro Hauptkomponente')\nplt.xticks([1, 2])\nplt.show()\n</code></pre> <p>Analyse:</p> <p>a) Wie viel Prozent der urspr\u00fcnglichen Information ist in den 2 Hauptkomponenten erhalten?</p> <p>b) Ist das genug? (Faustregel: &gt;70% ist akzeptabel, &gt;85% ist gut)</p>"},{"location":"arbeitsblaetter/ul-04-pca/#aufgabe-3-cluster-in-2d-visualisieren","title":"Aufgabe 3: Cluster in 2D visualisieren","text":"<p>Jetzt k\u00f6nnen wir die echten Iris-Klassen in 2D sehen:</p> <pre><code>plt.figure(figsize=(10, 8))\n\n# Scatterplot mit echten Labels\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n                       c=y, cmap='viridis', alpha=0.8, edgecolors='black')\n\nplt.xlabel('Hauptkomponente 1')\nplt.ylabel('Hauptkomponente 2')\nplt.title('Iris-Dataset nach PCA (echte Klassen)')\nplt.colorbar(scatter, label='Klasse')\nplt.legend(*scatter.legend_elements(), title='Spezies')\nplt.show()\n</code></pre> <p>Beobachtungen:</p> <p>a) Sind die drei Spezies klar voneinander getrennt?</p> <p>b) Welche zwei Spezies \u00fcberlappen am meisten?</p>"},{"location":"arbeitsblaetter/ul-04-pca/#aufgabe-4-k-means-auf-pca-daten","title":"Aufgabe 4: K-Means auf PCA-Daten","text":"<p>Vergleiche K-Means auf Original-Daten vs. PCA-reduzierte Daten:</p> <pre><code>from sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n# K-Means auf Original-Daten (4 Features)\nkmeans_orig = KMeans(n_clusters=3, random_state=42, n_init=10)\nlabels_orig = kmeans_orig.fit_predict(X_scaled)\n\n# K-Means auf PCA-Daten (2 Features)\nkmeans_pca = KMeans(n_clusters=3, random_state=42, n_init=10)\nlabels_pca = kmeans_pca.fit_predict(X_pca)\n\n# Vergleich mit echten Labels\nprint(f\"ARI (Original): {adjusted_rand_score(y, labels_orig):.3f}\")\nprint(f\"ARI (PCA):      {adjusted_rand_score(y, labels_pca):.3f}\")\n</code></pre> <p>Frage: Macht PCA die Cluster besser oder schlechter erkennbar?</p>"},{"location":"arbeitsblaetter/ul-04-pca/#aufgabe-5-country-daten-mit-pca-visualisieren","title":"Aufgabe 5: Country-Daten mit PCA visualisieren","text":"<p>Jetzt wenden wir PCA auf die Country-Daten an (9 Features \u2192 2):</p> <pre><code>import pandas as pd\n\n# Daten laden und vorbereiten\ndf = pd.read_csv('Country-data.csv')\nX = df.select_dtypes(include=[np.number])\ncountries = df['country'].values\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# K-Means durchf\u00fchren\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)  # Experimentiere mit k!\nlabels = kmeans.fit_predict(X_scaled)\n\n# PCA f\u00fcr Visualisierung (auf 2 Dimensionen reduzieren)\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"Erkl\u00e4rte Varianz: {pca.explained_variance_ratio_.sum():.2%}\")\n</code></pre> <p>Deine Aufgaben:</p> <p>a) F\u00fchre den Code aus. Wie viel Prozent der Varianz wird durch 2 Komponenten erkl\u00e4rt? Ist das akzeptabel?</p> <p>b) \u00c4ndere <code>n_clusters</code> auf 4 oder 5 \u2013 was passiert mit der Visualisierung? Welches k passt am besten zu den sichtbaren Gruppen?</p> <p>c) Vergleiche die Visualisierung mit deinem Elbow-Ergebnis aus UL-03 \u2013 best\u00e4tigt der Plot deine k-Wahl?</p> <p>Visualisierung:</p> <pre><code>plt.figure(figsize=(12, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n                       c=labels, cmap='viridis', alpha=0.7, edgecolors='black')\n\n# Einige L\u00e4ndernamen annotieren\nfor i, country in enumerate(countries):\n    if i % 10 == 0:  # Jeden 10. Namen zeigen\n        plt.annotate(country, (X_pca[i, 0], X_pca[i, 1]), fontsize=8, alpha=0.7)\n\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} Varianz)')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} Varianz)')\nplt.title('L\u00e4nder-Clustering nach PCA')\nplt.colorbar(scatter, label='Cluster')\nplt.show()\n</code></pre>"},{"location":"arbeitsblaetter/ul-04-pca/#aufgabe-6-was-bedeuten-die-hauptkomponenten","title":"Aufgabe 6: Was bedeuten die Hauptkomponenten?","text":"<p>Welche Original-Features stecken in den Hauptkomponenten?</p> <pre><code># Loadings: Gewichte der Original-Features\nloadings = pd.DataFrame(\n    pca.components_.T,\n    index=X.columns,\n    columns=['PC1', 'PC2']\n)\nprint(\"Feature-Loadings:\")\nprint(loadings.round(3))\n\n# Als Heatmap\nplt.figure(figsize=(8, 6))\nimport seaborn as sns\nsns.heatmap(loadings, annot=True, cmap='coolwarm', center=0)\nplt.title('Feature-Loadings der Hauptkomponenten')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Interpretiere:</p> <p>a) Welche Features haben den gr\u00f6\u00dften Einfluss auf PC1?</p> <p>b) Welche Features haben den gr\u00f6\u00dften Einfluss auf PC2?</p> <p>c) Was \"repr\u00e4sentiert\" PC1 inhaltlich? (z.B. \"Wohlstand\" wenn income, gdpp, life_expec hoch laden)</p>"},{"location":"arbeitsblaetter/ul-04-pca/#aufgabe-7-scree-plot-wie-viele-komponenten","title":"Aufgabe 7: Scree Plot \u2013 Wie viele Komponenten?","text":"<p>F\u00fcr die Country-Daten: Wie viele Komponenten brauchen wir?</p> <pre><code># PCA mit allen Komponenten\npca_full = PCA()\npca_full.fit(X_scaled)\n\n# Scree Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Erkl\u00e4rte Varianz pro Komponente\naxes[0].bar(range(1, len(pca_full.explained_variance_ratio_)+1), \n            pca_full.explained_variance_ratio_, \n            color='steelblue', edgecolor='black')\naxes[0].set_xlabel('Hauptkomponente')\naxes[0].set_ylabel('Erkl\u00e4rte Varianz')\naxes[0].set_title('Scree Plot')\n\n# Kumulierte Varianz\ncumvar = np.cumsum(pca_full.explained_variance_ratio_)\naxes[1].plot(range(1, len(cumvar)+1), cumvar, 'bo-', linewidth=2)\naxes[1].axhline(y=0.9, color='r', linestyle='--', label='90% Schwelle')\naxes[1].axhline(y=0.95, color='orange', linestyle='--', label='95% Schwelle')\naxes[1].set_xlabel('Anzahl Komponenten')\naxes[1].set_ylabel('Kumulierte erkl\u00e4rte Varianz')\naxes[1].set_title('Kumulierte erkl\u00e4rte Varianz')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKumulierte Varianz:\")\nfor i, var in enumerate(cumvar):\n    print(f\"  {i+1} Komponenten: {var:.1%}\")\n</code></pre> <p>Fragen:</p> <p>a) Wie viele Komponenten braucht man f\u00fcr 90% der Varianz?</p> <p>b) Wie viele f\u00fcr 95%?</p>"},{"location":"arbeitsblaetter/ul-04-pca/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"<p>Typische Probleme und L\u00f6sungen</p> <p>\u274c Daten nicht skaliert vor PCA \u2192 PCA ist extrem empfindlich gegen\u00fcber unterschiedlichen Skalen! <pre><code># IMMER vorher skalieren:\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\npca.fit(X_scaled)\n</code></pre></p> <p>\u274c Zu wenig Varianz erkl\u00e4rt (&lt;70%) \u2192 Verwende mehr Komponenten: <pre><code>PCA(n_components=3)  # oder mehr\n</code></pre></p> <p>\u274c <code>explained_variance_ratio_</code> nicht verstanden \u2192 Die Summe zeigt, wie viel Information erhalten bleibt: <pre><code># z.B. 0.85 = 85% der Original-Information ist erhalten\nprint(pca.explained_variance_ratio_.sum())\n</code></pre></p> <p>\u274c Scatterplot ohne Farben \u2192 F\u00e4rbe nach Cluster-Label: <pre><code>plt.scatter(X_pca[:,0], X_pca[:,1], c=labels, cmap='viridis')\n</code></pre></p>"},{"location":"arbeitsblaetter/ul-04-pca/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das hast du gelernt</p> <ul> <li>PCA reduziert Dimensionen bei minimalem Informationsverlust</li> <li>Explained Variance Ratio: Zeigt, wie viel Information erhalten bleibt</li> <li>Scree Plot: Hilft bei der Wahl der Komponentenzahl</li> <li>Loadings: Zeigen, welche Original-Features in den Komponenten stecken</li> <li>F\u00fcr Visualisierung meist 2-3 Komponenten, f\u00fcr Analyse oft mehr n\u00f6tig</li> </ul>"},{"location":"arbeitsblaetter/ul-04-pca/#nachste-schritte","title":"N\u00e4chste Schritte","text":"<p>Du kannst jetzt Cluster finden und visualisieren. Aber was bedeuten sie? Im n\u00e4chsten Arbeitsblatt lernst du, Cluster zu interpretieren.</p> <p>\u27a1\ufe0f Weiter zu UL-05: Cluster-Interpretation</p>"},{"location":"arbeitsblaetter/ul-05-interpretation/","title":"UL-05: Cluster-Interpretation","text":"<p>Advance Organizer</p> <p>Cluster zu finden ist nur die halbe Miete \u2013 jetzt musst du sie verstehen! Was unterscheidet die Gruppen? Warum sind bestimmte Datenpunkte zusammen? Diese Interpretation ist der wichtigste Schritt f\u00fcr jeden Business Case, denn nur so kannst du Handlungsempfehlungen ableiten.</p> <p>Dein Ziel: Du kannst Cluster-Profile erstellen und inhaltlich interpretieren. Das ist die Basis f\u00fcr jede Pr\u00e4sentation deiner Ergebnisse.</p>"},{"location":"arbeitsblaetter/ul-05-interpretation/#lernziele","title":"Lernziele","text":"<p>Nach Bearbeitung dieses Arbeitsblatts kannst du:</p> <ul> <li> Cluster inhaltlich interpretieren</li> <li> Cluster-Profile erstellen und visualisieren</li> <li> Ergebnisse f\u00fcr Nicht-Techniker aufbereiten</li> </ul>"},{"location":"arbeitsblaetter/ul-05-interpretation/#aufgabe-1-daten-vorbereiten-und-clustern","title":"Aufgabe 1: Daten vorbereiten und clustern","text":"<p>Zuerst f\u00fchren wir das Clustering durch (Wiederholung):</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Daten laden\ndf = pd.read_csv('Country-data.csv')\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n\n# Skalieren und clustern\nX = df[numeric_cols]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\ndf['Cluster'] = kmeans.fit_predict(X_scaled)\n\n# \u00dcbersicht\nprint(df['Cluster'].value_counts().sort_index())\n</code></pre> <p>Pr\u00fcfe dein Ergebnis:</p> <p>a) Wie viele L\u00e4nder sind in jedem Cluster? Sind die Cluster einigerma\u00dfen ausgewogen?</p> <p>b) Was w\u00fcrde passieren, wenn ein Cluster nur 2-3 L\u00e4nder enth\u00e4lt? W\u00e4re das ein Problem?</p>"},{"location":"arbeitsblaetter/ul-05-interpretation/#aufgabe-2-mittelwerte-pro-cluster-berechnen","title":"Aufgabe 2: Mittelwerte pro Cluster berechnen","text":"<p>Das Cluster-Profil zeigt die typischen Eigenschaften jeder Gruppe:</p> <pre><code># Mittelwerte pro Cluster (auf Original-Daten!)\ncluster_means = df.groupby('Cluster')[numeric_cols].mean()\nprint(cluster_means.round(2))\n</code></pre> <p>Fragen:</p> <p>a) Welches Cluster hat das h\u00f6chste durchschnittliche Einkommen (<code>income</code>)?</p> <p>b) Welches Cluster hat die h\u00f6chste Kindersterblichkeit (<code>child_mort</code>)?</p> <p>c) Wie unterscheidet sich die Lebenserwartung (<code>life_expec</code>) zwischen den Clustern?</p>"},{"location":"arbeitsblaetter/ul-05-interpretation/#aufgabe-3-cluster-profile-als-balkendiagramm","title":"Aufgabe 3: Cluster-Profile als Balkendiagramm","text":"<p>Visualisiere die Unterschiede zwischen den Clustern:</p> <pre><code># F\u00fcr alle Features\nfig, axes = plt.subplots(3, 3, figsize=(14, 12))\naxes = axes.flatten()\n\nfor i, col in enumerate(numeric_cols):\n    df.boxplot(column=col, by='Cluster', ax=axes[i])\n    axes[i].set_title(col)\n    axes[i].set_xlabel('Cluster')\n\nplt.suptitle('Feature-Verteilungen pro Cluster', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Analysiere die Boxplots:</p> <p>a) Bei welchen Features sind die Unterschiede zwischen den Clustern am gr\u00f6\u00dften?</p> <p>b) Gibt es Features, bei denen sich die Cluster kaum unterscheiden? Was bedeutet das?</p> <p>c) Welches Cluster hat die gr\u00f6\u00dfte Streuung (Variabilit\u00e4t) innerhalb der Gruppe?</p> <pre><code># Mittelwerte normalisieren f\u00fcr Vergleichbarkeit\ncluster_means_norm = cluster_means.apply(lambda x: (x - x.mean()) / x.std())\n\ncluster_means_norm.T.plot(kind='bar', figsize=(12, 6))\nplt.xlabel('Feature')\nplt.ylabel('Normalisierter Mittelwert (Z-Score)')\nplt.title('Cluster-Profile (normalisiert)')\nplt.legend(title='Cluster')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"arbeitsblaetter/ul-05-interpretation/#aufgabe-4-heatmap-der-cluster-profile","title":"Aufgabe 4: Heatmap der Cluster-Profile","text":"<p>Eine Heatmap zeigt auf einen Blick, welche Features pro Cluster hoch oder niedrig sind:</p> <pre><code># Heatmap der normalisierten Mittelwerte\nplt.figure(figsize=(12, 8))\nsns.heatmap(cluster_means_norm.T, \n            annot=True, \n            cmap='RdYlGn', \n            center=0,\n            fmt='.2f',\n            linewidths=0.5)\nplt.title('Cluster-Profile (Z-Score normalisiert)')\nplt.xlabel('Cluster')\nplt.ylabel('Feature')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Interpretiere die Heatmap:</p> <p>a) Welches Cluster zeigt durchgehend gr\u00fcne (positive) Werte? Was bedeutet das inhaltlich?</p> <p>b) Welches Cluster zeigt durchgehend rote (negative) Werte? </p> <p>c) Gibt es ein Cluster mit gemischten Werten (teils gr\u00fcn, teils rot)? Was k\u00f6nnte das bedeuten?</p> <ul> <li>Gr\u00fcn/Positive Werte: \u00dcberdurchschnittlich hoch</li> <li>Rot/Negative Werte: Unterdurchschnittlich niedrig</li> <li>Wei\u00df/Null: Durchschnittlich</li> </ul>"},{"location":"arbeitsblaetter/ul-05-interpretation/#aufgabe-5-lander-den-clustern-zuordnen","title":"Aufgabe 5: L\u00e4nder den Clustern zuordnen","text":"<p>Welche L\u00e4nder sind in welchem Cluster?</p> <pre><code># L\u00e4nder pro Cluster\nfor cluster_id in sorted(df['Cluster'].unique()):\n    countries = df[df['Cluster'] == cluster_id]['country'].tolist()\n    print(f\"\\n=== Cluster {cluster_id} ({len(countries)} L\u00e4nder) ===\")\n    print(', '.join(countries[:10]))  # Erste 10\n    if len(countries) &gt; 10:\n        print(f\"... und {len(countries)-10} weitere\")\n</code></pre> <p>Analyse:</p> <p>a) Erkennst du ein Muster? Welche Art von L\u00e4ndern ist in welchem Cluster?</p> <p>b) Gibt es \u00dcberraschungen? L\u00e4nder, die du in einem anderen Cluster erwartet h\u00e4ttest?</p>"},{"location":"arbeitsblaetter/ul-05-interpretation/#aufgabe-6-cluster-benennen","title":"Aufgabe 6: Cluster benennen","text":"<p>Wichtig f\u00fcr die Praxis</p> <p>Cluster-Nummern (0, 1, 2) sind bedeutungslos! Gib den Clustern aussagekr\u00e4ftige Namen.</p> <p>Basierend auf deiner Analyse, wie w\u00fcrdest du die Cluster benennen?</p> <pre><code># Beispiel-Mapping (anpassen nach deiner Analyse!)\ncluster_names = {\n    0: '___________',  # z.B. \"Industriel\u00e4nder\"\n    1: '___________',  # z.B. \"Entwicklungsl\u00e4nder\"  \n    2: '___________'   # z.B. \"Schwellenl\u00e4nder\"\n}\n\ndf['Cluster_Name'] = df['Cluster'].map(cluster_names)\nprint(df[['country', 'Cluster', 'Cluster_Name']].head(20))\n</code></pre> <p>Tipps f\u00fcr gute Namen:</p> <ul> <li>Beschreibe die Hauptmerkmale</li> <li>Verwende verst\u00e4ndliche Begriffe (keine Fachsprache)</li> <li>Vermeide wertende Begriffe wenn m\u00f6glich</li> </ul>"},{"location":"arbeitsblaetter/ul-05-interpretation/#aufgabe-7-ergebnisse-zusammenfassen","title":"Aufgabe 7: Ergebnisse zusammenfassen","text":"<p>Erstelle eine Zusammenfassung f\u00fcr einen Nicht-Techniker:</p> <pre><code># Executive Summary erstellen\nprint(\"=\"*60)\nprint(\"CLUSTER-ANALYSE: L\u00c4NDERGRUPPEN\")\nprint(\"=\"*60)\n\nfor cluster_id in sorted(df['Cluster'].unique()):\n    cluster_df = df[df['Cluster'] == cluster_id]\n    name = cluster_names.get(cluster_id, f'Cluster {cluster_id}')\n\n    print(f\"\\n{name.upper()} ({len(cluster_df)} L\u00e4nder)\")\n    print(\"-\"*40)\n    print(f\"Typische Merkmale:\")\n    print(f\"  - Einkommen: ${cluster_df['income'].mean():,.0f}\")\n    print(f\"  - Lebenserwartung: {cluster_df['life_expec'].mean():.1f} Jahre\")\n    print(f\"  - Kindersterblichkeit: {cluster_df['child_mort'].mean():.1f} pro 1000\")\n    print(f\"\\nBeispiell\u00e4nder: {', '.join(cluster_df['country'].head(5).tolist())}\")\n</code></pre>"},{"location":"arbeitsblaetter/ul-05-interpretation/#aufgabe-8-reflexion","title":"Aufgabe 8: Reflexion","text":"<p>Reflexionsfragen</p> <ol> <li> <p>Wie w\u00fcrdest du die gefundenen Cluster einem Manager erkl\u00e4ren (ohne technische Begriffe)?</p> </li> <li> <p>Welche Handlungsempfehlungen k\u00f6nnte man aus den Clustern ableiten?    (z.B. f\u00fcr eine Hilfsorganisation: \"Fokussiere Ressourcen auf Cluster X wegen...\")</p> </li> <li> <p>Sind die Cluster \"wahr\"? Oder k\u00f6nnten andere Clusteranzahlen genauso sinnvoll sein?</p> </li> <li> <p>Welche zus\u00e4tzlichen Daten w\u00fcrden die Analyse verbessern?</p> </li> </ol>"},{"location":"arbeitsblaetter/ul-05-interpretation/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"<p>Typische Probleme und L\u00f6sungen</p> <p>\u274c Mittelwerte auf skalierten Daten berechnet \u2192 Die skalierten Werte sind nicht interpretierbar! Nutze Original-Daten: <pre><code># Falsch: Mittelwerte der X_scaled\n# Richtig: Mittelwerte der Original-Daten\ndf.groupby('Cluster')[numeric_cols].mean()\n</code></pre></p> <p>\u274c Cluster-Labels stimmen nicht \u00fcberein \u2192 Die Label-Nummern sind willk\u00fcrlich! Cluster 0 heute kann morgen Cluster 2 sein.</p> <p>\u274c Bar-Chart unleserlich \u2192 Normalisiere auf Z-Scores oder nutze Heatmap: <pre><code>sns.heatmap(cluster_means_norm.T, ...)\n</code></pre></p> <p>\u274c Keine sinnvolle Interpretation \u2192 Schau dir extreme Werte an: Welches Feature unterscheidet die Cluster am st\u00e4rksten? <pre><code># Varianz zwischen Clustern\ncluster_means.var()\n</code></pre></p>"},{"location":"arbeitsblaetter/ul-05-interpretation/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das hast du gelernt</p> <ul> <li>Cluster-Profile: Mittelwerte der Features pro Cluster</li> <li>Heatmaps: Auf einen Blick sehen, was Cluster unterscheidet</li> <li>Normalisierung: Z-Scores f\u00fcr Vergleichbarkeit</li> <li>Benennung: Cluster brauchen aussagekr\u00e4ftige Namen!</li> <li>Die inhaltliche Interpretation ist der wichtigste Schritt f\u00fcr Business Value</li> </ul>"},{"location":"arbeitsblaetter/ul-05-interpretation/#nachste-schritte","title":"N\u00e4chste Schritte","text":"<p>K-Means erfordert, dass du k vorher festlegst. Im n\u00e4chsten Arbeitsblatt lernst du hierarchisches Clustering kennen \u2013 dort entscheidest du erst nachtr\u00e4glich!</p> <p>\u27a1\ufe0f Weiter zu UL-06: Hierarchisches Clustering</p>"},{"location":"arbeitsblaetter/ul-06-hierarchisch/","title":"UL-06: Hierarchisches Clustering","text":"<p>Advance Organizer</p> <p>K-Means zwingt dich, die Clusteranzahl vorher festzulegen. Hierarchisches Clustering zeigt dir stattdessen eine Baumstruktur (Dendrogramm), aus der du verschiedene Ebenen ablesen kannst. Das ist besonders n\u00fctzlich, wenn du die \"nat\u00fcrliche\" Struktur der Daten entdecken m\u00f6chtest.</p> <p>Dein Ziel: Du verstehst den Unterschied zwischen K-Means und hierarchischem Clustering und kannst f\u00fcr einen Use Case entscheiden, welcher Algorithmus besser passt.</p>"},{"location":"arbeitsblaetter/ul-06-hierarchisch/#lernziele","title":"Lernziele","text":"<p>Nach Bearbeitung dieses Arbeitsblatts kannst du:</p> <ul> <li> Hierarchisches Clustering verstehen und erkl\u00e4ren</li> <li> Dendrogramme erstellen und interpretieren</li> <li> Verschiedene Linkage-Methoden vergleichen</li> </ul>"},{"location":"arbeitsblaetter/ul-06-hierarchisch/#aufgabe-1-daten-vorbereiten","title":"Aufgabe 1: Daten vorbereiten","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Daten laden und skalieren\ndf = pd.read_csv('Country-data.csv')\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n\nX = df[numeric_cols]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\ncountries = df['country'].values\n</code></pre> <p>Pr\u00fcfe dein Setup:</p> <p>a) Wie viele L\u00e4nder und wie viele Features hat der Datensatz?</p> <p>b) Warum ist Skalierung auch f\u00fcr hierarchisches Clustering wichtig?</p>"},{"location":"arbeitsblaetter/ul-06-hierarchisch/#aufgabe-2-linkage-matrix-berechnen","title":"Aufgabe 2: Linkage-Matrix berechnen","text":"<p>Hierarchisches Clustering baut einen Baum auf, in dem \u00e4hnliche Punkte schrittweise zusammengef\u00fcgt werden:</p> <pre><code># Linkage-Matrix berechnen (Ward-Methode)\nZ = linkage(X_scaled, method='ward')\n\n# Struktur der Linkage-Matrix\nprint(\"Linkage-Matrix Shape:\", Z.shape)\nprint(\"\\nBeispiel der ersten Merge-Schritte:\")\nprint(\"Cluster 1 | Cluster 2 | Distanz | Anzahl Punkte\")\nprint(Z[:5].round(2))\n</code></pre> <p>Analysiere die Linkage-Matrix:</p> <p>a) Wie viele Zeilen hat die Matrix? (Tipp: Bei n Datenpunkten gibt es n-1 Merge-Schritte)</p> <p>b) Schau dir die ersten Merge-Schritte an: Welche Distanzen haben die ersten Zusammenf\u00fchrungen? Sind das \u00e4hnliche oder un\u00e4hnliche Punkte?</p> <p>c) Wie ver\u00e4ndert sich die Distanz in Spalte 2 \u00fcber die Zeilen hinweg?</p> <p>Die Linkage-Matrix erkl\u00e4rt:</p> Spalte Bedeutung 0 Index des ersten zusammengef\u00fchrten Clusters 1 Index des zweiten zusammengef\u00fchrten Clusters 2 Distanz zwischen den Clustern 3 Anzahl Punkte im neuen Cluster"},{"location":"arbeitsblaetter/ul-06-hierarchisch/#aufgabe-3-dendrogramm-erstellen","title":"Aufgabe 3: Dendrogramm erstellen","text":"<p>Das Dendrogramm visualisiert die hierarchische Struktur:</p> <pre><code>plt.figure(figsize=(16, 8))\n\n# Dendrogramm mit L\u00e4ndernamen\ndendrogram(Z, \n           labels=countries,\n           leaf_rotation=90,\n           leaf_font_size=8)\n\nplt.title('Dendrogramm der L\u00e4nder (Ward-Methode)')\nplt.xlabel('L\u00e4nder')\nplt.ylabel('Distanz')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Analysiere das Dendrogramm:</p> <p>a) Bei welcher Distanz (y-Achse) w\u00fcrde das Dendrogramm in 2 Cluster zerfallen? Bei welcher in 3?</p> <p>b) Gibt es L\u00e4nder, die besonders fr\u00fch (unten) zusammengef\u00fchrt werden? Welche sind das?</p> <p>c) Gibt es L\u00e4nder, die erst sehr sp\u00e4t (oben) zusammengef\u00fchrt werden? Was bedeutet das?</p> <p>Wie liest man ein Dendrogramm?</p> <ul> <li>Horizontale Linien: Zusammenf\u00fchrung zweier Cluster</li> <li>H\u00f6he der Linien: Distanz (je h\u00f6her, desto un\u00e4hnlicher)</li> <li>Vertikale \u00c4ste: Einzelne Datenpunkte oder Cluster</li> </ul>"},{"location":"arbeitsblaetter/ul-06-hierarchisch/#aufgabe-4-schnitt-auf-gewunschter-hohe","title":"Aufgabe 4: Schnitt auf gew\u00fcnschter H\u00f6he","text":"<p>Du kannst das Dendrogramm auf verschiedenen H\u00f6hen \"schneiden\":</p> <pre><code>fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Dendrogramm mit Schnittlinien\nfor ax, n_clusters, color in [(axes[0], 3, 'red'), (axes[1], 5, 'green')]:\n    dendrogram(Z, labels=countries, leaf_rotation=90, leaf_font_size=8, ax=ax)\n\n    # Schwellenwert f\u00fcr n Cluster finden\n    labels_temp = fcluster(Z, n_clusters, criterion='maxclust')\n    threshold = Z[-(n_clusters-1), 2]\n\n    ax.axhline(y=threshold, color=color, linestyle='--', linewidth=2, \n               label=f'Schnitt f\u00fcr {n_clusters} Cluster')\n    ax.legend()\n    ax.set_title(f'{n_clusters} Cluster')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Vergleiche die beiden Schnitte:</p> <p>a) Bei welcher H\u00f6he liegt die rote Linie (3 Cluster)? Bei welcher die gr\u00fcne (5 Cluster)?</p> <p>b) Welcher Schnitt erscheint dir \"nat\u00fcrlicher\" \u2013 wo gibt es gr\u00f6\u00dfere L\u00fccken zwischen den Clustern?</p> <p>c) Was ist der Vorteil davon, dass du die Clusteranzahl nachtr\u00e4glich festlegen kannst?</p> <p>Extrahiere die Cluster-Labels f\u00fcr eine bestimmte Anzahl:</p> <pre><code># 3 Cluster extrahieren\nn_clusters = 3\nlabels_hc = fcluster(Z, n_clusters, criterion='maxclust')\n\n# Achtung: fcluster gibt Labels ab 1 (nicht 0!)\nlabels_hc = labels_hc - 1  # Anpassen auf 0-indexiert\n\ndf['Cluster_HC'] = labels_hc\nprint(df['Cluster_HC'].value_counts().sort_index())\n</code></pre> <p>Alternative mit AgglomerativeClustering:</p> <pre><code>agg = AgglomerativeClustering(n_clusters=3, linkage='ward')\nlabels_agg = agg.fit_predict(X_scaled)\n\ndf['Cluster_Agg'] = labels_agg\nprint(df['Cluster_Agg'].value_counts().sort_index())\n</code></pre> <p>Vergleiche die beiden Methoden:</p> <p>a) Liefern <code>fcluster()</code> und <code>AgglomerativeClustering</code> die gleichen Ergebnisse bei gleicher Clusteranzahl?</p> <p>b) Wann w\u00fcrdest du welche Methode verwenden? (Tipp: <code>fcluster</code> braucht die Linkage-Matrix, <code>AgglomerativeClustering</code> nicht)</p>"},{"location":"arbeitsblaetter/ul-06-hierarchisch/#aufgabe-6-vergleich-k-means-vs-hierarchisches-clustering","title":"Aufgabe 6: Vergleich K-Means vs. Hierarchisches Clustering","text":"<p>Vergleiche die Ergebnisse beider Methoden:</p> <pre><code>from sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n# K-Means\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\nlabels_km = kmeans.fit_predict(X_scaled)\n\n# Hierarchisch (Ward)\nagg = AgglomerativeClustering(n_clusters=3, linkage='ward')\nlabels_hc = agg.fit_predict(X_scaled)\n\n# \u00dcbereinstimmung\nari = adjusted_rand_score(labels_km, labels_hc)\nprint(f\"Adjusted Rand Index: {ari:.3f}\")\n\n# Crosstab f\u00fcr detaillierten Vergleich\nprint(\"\\nVergleich der Zuordnungen:\")\nprint(pd.crosstab(labels_km, labels_hc, \n                  rownames=['K-Means'], \n                  colnames=['Hierarchisch']))\n</code></pre> <p>Fragen:</p> <p>a) Wie stark stimmen die beiden Methoden \u00fcberein?</p> <p>b) Welche L\u00e4nder werden unterschiedlich zugeordnet?</p> <pre><code># Unterschiedlich zugeordnete L\u00e4nder finden\ndf_temp = df.copy()\ndf_temp['KM'] = labels_km\ndf_temp['HC'] = labels_hc\ndisagree = df_temp[df_temp['KM'] != df_temp['HC']]\nprint(f\"\\n{len(disagree)} L\u00e4nder mit unterschiedlicher Zuordnung:\")\nprint(disagree[['country', 'KM', 'HC']])\n</code></pre>"},{"location":"arbeitsblaetter/ul-06-hierarchisch/#aufgabe-7-verschiedene-linkage-methoden-vergleichen","title":"Aufgabe 7: Verschiedene Linkage-Methoden vergleichen","text":"<p>Es gibt mehrere Methoden, die Distanz zwischen Clustern zu berechnen:</p> <pre><code>methods = ['single', 'complete', 'average', 'ward']\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\nfor ax, method in zip(axes.flatten(), methods):\n    Z_method = linkage(X_scaled, method=method)\n    dendrogram(Z_method, \n               labels=countries,\n               leaf_rotation=90,\n               leaf_font_size=6,\n               ax=ax,\n               truncate_mode='lastp',\n               p=30)  # Nur letzte 30 Merges zeigen\n    ax.set_title(f'Linkage-Methode: {method}')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Vergleiche die Linkage-Methoden:</p> <p>a) Welche Methode erzeugt die \"kettenf\u00f6rmigsten\" Cluster (ein langes, d\u00fcnnes Cluster)? Warum?</p> <p>b) Welche Methode erzeugt die \"ausgewogensten\" Cluster (\u00e4hnliche Gr\u00f6\u00dfe)?</p> <p>c) F\u00fcr welche Anwendungsf\u00e4lle k\u00f6nnte <code>single</code> trotzdem sinnvoll sein?</p> <p>Linkage-Methoden erkl\u00e4rt:</p> Methode Distanz zwischen Clustern single K\u00fcrzeste Distanz zwischen Punkten complete L\u00e4ngste Distanz zwischen Punkten average Durchschnittliche Distanz ward Minimiert Varianz im Cluster <p>Frage: Welche Methode liefert die \"ausgewogensten\" Cluster?</p>"},{"location":"arbeitsblaetter/ul-06-hierarchisch/#aufgabe-8-vor-und-nachteile-diskutieren","title":"Aufgabe 8: Vor- und Nachteile diskutieren","text":"<p>F\u00fclle die Tabelle basierend auf deinen Erfahrungen:</p> Aspekt K-Means Hierarchisches Clustering Muss k vorher festgelegt werden? ___ ___ Laufzeit bei gro\u00dfen Daten ___ ___ Cluster-Form ___ ___ Deterministisch? ___ ___ Zus\u00e4tzliche Einblicke ___ ___ <p>Wann welchen Algorithmus?</p> <ul> <li>K-Means: Wenn du schnelle Ergebnisse brauchst und die ungef\u00e4hre Clusteranzahl kennst</li> <li>Hierarchisch: Wenn du die Struktur der Daten erkunden willst oder unsicher \u00fcber k bist</li> </ul>"},{"location":"arbeitsblaetter/ul-06-hierarchisch/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"<p>Typische Probleme und L\u00f6sungen</p> <p>\u274c Dendrogramm zu un\u00fcbersichtlich \u2192 Nutze <code>truncate_mode='lastp'</code>: <pre><code>dendrogram(Z, truncate_mode='lastp', p=30)\n</code></pre></p> <p>\u274c Linkage-Objekt nicht verstanden \u2192 <code>linkage()</code> gibt eine Matrix zur\u00fcck, die du an <code>dendrogram()</code> oder <code>fcluster()</code> \u00fcbergibst.</p> <p>\u274c <code>fcluster</code> liefert falsche Anzahl \u2192 Pr\u00fcfe den Parameter <code>t</code> (Schwellenwert) oder nutze <code>criterion='maxclust'</code>: <pre><code># Nach Distanz-Schwelle:\nfcluster(Z, t=10, criterion='distance')\n\n# Nach Anzahl Cluster:\nfcluster(Z, n_clusters, criterion='maxclust')\n</code></pre></p> <p>\u274c Vergleich mit K-Means nicht sinnvoll \u2192 Nutze dieselbe Clusteranzahl f\u00fcr beide Methoden!</p>"},{"location":"arbeitsblaetter/ul-06-hierarchisch/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das hast du gelernt</p> <ul> <li>Hierarchisches Clustering baut eine Baumstruktur auf</li> <li>Dendrogramm: Visualisiert die hierarchische Struktur</li> <li>Linkage-Methoden: single, complete, average, ward</li> <li>Du kannst nachtr\u00e4glich entscheiden, wie viele Cluster du haben m\u00f6chtest</li> <li>Ward-Linkage liefert oft die \"ausgewogensten\" Cluster</li> </ul>"},{"location":"arbeitsblaetter/ul-06-hierarchisch/#nachste-schritte","title":"N\u00e4chste Schritte","text":"<p>Du hast jetzt zwei wichtige Clustering-Algorithmen kennengelernt. Im n\u00e4chsten Arbeitsblatt wendest du alles Gelernte eigenst\u00e4ndig auf einen neuen Datensatz an!</p> <p>\u27a1\ufe0f Weiter zu UL-07: Kundensegmentierung</p>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/","title":"UL-07: Kundensegmentierung","text":"<p>Advance Organizer</p> <p>Jetzt wird es ernst: Du bekommst einen neuen Datensatz ohne Anleitung und musst selbst entscheiden, welche Schritte n\u00f6tig sind. Das entspricht einer echten Aufgabe als Datenanalyst. Kundensegmentierung ist einer der h\u00e4ufigsten Anwendungsf\u00e4lle f\u00fcr Clustering in der Wirtschaft!</p> <p>Dein Ziel: Du f\u00fchrst eine vollst\u00e4ndige Clustering-Analyse durch und leitest konkrete Marketing-Empfehlungen ab. Das ist deine Generalprobe f\u00fcr das Abschlussprojekt.</p>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#lernziele","title":"Lernziele","text":"<p>Nach Bearbeitung dieses Arbeitsblatts kannst du:</p> <ul> <li> Gelerntes eigenst\u00e4ndig auf einen neuen Datensatz anwenden</li> <li> Eine gesch\u00e4ftliche Fragestellung mit Clustering beantworten</li> <li> Marketing-Empfehlungen ableiten</li> </ul>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#das-szenario","title":"Das Szenario","text":"<p>Ausgangssituation</p> <p>Ein Einkaufszentrum m\u00f6chte seine Kunden besser verstehen, um gezieltes Marketing zu betreiben. Die Marketing-Abteilung hat dir die Aufgabe gegeben, verschiedene Kundengruppen zu identifizieren.</p> <p>Deine Aufgabe: Analysiere das Kundenverhalten und identifiziere verschiedene Kundengruppen. Leite daraus konkrete Marketing-Ma\u00dfnahmen ab.</p>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#der-datensatz","title":"Der Datensatz","text":"<p>Der Mall Customers Datensatz enth\u00e4lt Informationen \u00fcber 200 Kunden:</p> Feature Beschreibung CustomerID Eindeutige Kundennummer Gender Geschlecht (Male/Female) Age Alter in Jahren Annual Income (k$) Jahreseinkommen in Tausend Dollar Spending Score (1-100) Bewertung des Kaufverhaltens (1=wenig, 100=viel)"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#aufgabe-1-daten-laden-und-explorieren","title":"Aufgabe 1: Daten laden und explorieren","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Daten laden\ndf = pd.read_csv('Mall_Customers.csv')\n\n# Deine Aufgaben:\n# a) Zeige die ersten Zeilen\n# b) Pr\u00fcfe die Datentypen\n# c) Gibt es fehlende Werte?\n# d) Erstelle deskriptive Statistik\n</code></pre>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#aufgabe-2-features-auswahlen","title":"Aufgabe 2: Features ausw\u00e4hlen","text":"<p>Wichtige Entscheidung</p> <p>Nicht alle Features sind f\u00fcr Clustering geeignet!</p> <p>\u00dcberlege:</p> <p>a) Sollte <code>CustomerID</code> als Feature verwendet werden? Warum (nicht)?</p> <p>b) Wie gehst du mit <code>Gender</code> um? (Optionen: ignorieren, One-Hot-Encoding, separate Analyse)</p> <p>c) Welche Features sind f\u00fcr Marketing-Segmentierung am relevantesten?</p> <pre><code># Deine Feature-Auswahl\nfeatures = ['___', '___', '___']  # Trage deine Auswahl ein\n\nX = df[features]\nprint(X.describe())\n</code></pre>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#aufgabe-3-daten-visualisieren","title":"Aufgabe 3: Daten visualisieren","text":"<p>Erstelle Visualisierungen, um die Datenstruktur zu verstehen:</p> <pre><code># Pairplot oder Scatterplot erstellen\n# Tipp: Bei nur 2-3 Features geht auch ein einfacher 2D-Scatterplot\n</code></pre> <p>Fragen:</p> <p>a) Erkennst du bereits Gruppen in den Daten?</p> <p>b) Wie viele Cluster vermutest du?</p>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#aufgabe-4-daten-skalieren","title":"Aufgabe 4: Daten skalieren","text":"<pre><code># Daten skalieren\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Pr\u00fcfe das Ergebnis\n</code></pre>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#aufgabe-5-optimale-clusteranzahl-bestimmen","title":"Aufgabe 5: Optimale Clusteranzahl bestimmen","text":"<p>Wende die Elbow-Methode und/oder Silhouette-Analyse an:</p> <pre><code># Elbow-Methode\n\n\n\n# Silhouette-Scores\n\n\n\n# Deine Entscheidung: k = ___\n</code></pre>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#aufgabe-6-k-means-clustering-durchfuhren","title":"Aufgabe 6: K-Means Clustering durchf\u00fchren","text":"<pre><code># K-Means mit optimalem k\n\n\n\n# Cluster dem DataFrame hinzuf\u00fcgen\ndf['Cluster'] = ___\n\n# \u00dcbersicht\nprint(df['Cluster'].value_counts())\n</code></pre>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#aufgabe-7-cluster-visualisieren","title":"Aufgabe 7: Cluster visualisieren","text":"<p>Erstelle aussagekr\u00e4ftige Visualisierungen:</p> <pre><code># 2D-Scatterplot mit Cluster-Farben\n\n\n\n# Boxplots der Features pro Cluster\n\n\n\n# Heatmap der Cluster-Mittelwerte\n</code></pre>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#aufgabe-8-cluster-interpretieren-und-benennen","title":"Aufgabe 8: Cluster interpretieren und benennen","text":"<p>Analysiere die Cluster und gib ihnen aussagekr\u00e4ftige Namen:</p> <pre><code># Cluster-Profile berechnen\ncluster_profiles = df.groupby('Cluster')[features].mean()\nprint(cluster_profiles)\n\n# Cluster benennen\ncluster_names = {\n    0: '___________',\n    1: '___________',\n    2: '___________',\n    # ...\n}\n\ndf['Segment'] = df['Cluster'].map(cluster_names)\n</code></pre> <p>Typische Kundengruppen k\u00f6nnten sein:</p> <ul> <li>\"Sparsame Senioren\"</li> <li>\"Junge High-Spender\"  </li> <li>\"Durchschnitts-Shopper\"</li> <li>\"Premium-Kunden\"</li> <li>etc.</li> </ul>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#aufgabe-9-marketing-empfehlungen-ableiten","title":"Aufgabe 9: Marketing-Empfehlungen ableiten","text":"<p>Das wichtigste Ergebnis!</p> <p>Jetzt kommt der Business Value: Was soll das Marketing-Team mit deinen Erkenntnissen tun?</p> <p>F\u00fcr jede Kundengruppe:</p> Segment Charakteristik Marketing-Empfehlung ___ ___ ___ ___ ___ ___ ___ ___ ___ <p>Beispiele f\u00fcr Marketing-Ma\u00dfnahmen:</p> <ul> <li>Newsletter-Inhalte anpassen</li> <li>Spezielle Rabattaktionen</li> <li>Premium-Services anbieten</li> <li>Loyalty-Programme</li> <li>Events im Einkaufszentrum</li> <li>Personalisierte Werbung</li> </ul>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#zusatzaufgabe-fur-schnelle","title":"Zusatzaufgabe f\u00fcr Schnelle","text":"<p>Vergleiche das K-Means Ergebnis mit hierarchischem Clustering:</p> <pre><code>from sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Hierarchisches Clustering\n\n\n\n# Vergleich mit K-Means\n</code></pre>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"<p>Typische Probleme und L\u00f6sungen</p> <p>\u274c CustomerID als Feature verwendet \u2192 IDs sind keine sinnvollen Features! Sie enthalten keine Information \u00fcber Kundenverhalten.</p> <p>\u274c Geschlecht nicht ber\u00fccksichtigt \u2192 Optionen: - One-Hot-Encoding: <code>pd.get_dummies(df, columns=['Gender'])</code> - Separate Analyse f\u00fcr M\u00e4nner und Frauen - Ignorieren und sp\u00e4ter bei Interpretation ber\u00fccksichtigen</p> <p>\u274c Nur 2D-Visualisierung \u2192 Mit nur 3-4 Features geht auch 3D: <pre><code>from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X[:,0], X[:,1], X[:,2], c=labels)\n</code></pre></p> <p>\u274c Marketing-Empfehlungen zu vage \u2192 Sei konkret!  <pre><code># Statt: \"Marketing f\u00fcr Cluster 2 anpassen\"\n# Besser: \"Cluster 2 sind junge High-Spender (20-35 Jahre, hohes Einkommen, \n#          Spending Score &gt;70) \u2192 Premium-Events und exklusive Angebote\"\n</code></pre></p>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#checkliste-fur-deine-analyse","title":"Checkliste f\u00fcr deine Analyse","text":"<ul> <li> Daten geladen und exploriert</li> <li> Features sinnvoll ausgew\u00e4hlt</li> <li> Daten skaliert</li> <li> Optimale Clusteranzahl bestimmt (begr\u00fcndet!)</li> <li> Clustering durchgef\u00fchrt</li> <li> Visualisierungen erstellt</li> <li> Cluster inhaltlich interpretiert</li> <li> Aussagekr\u00e4ftige Namen vergeben</li> <li> Marketing-Empfehlungen abgeleitet</li> </ul>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das hast du gelernt</p> <ul> <li>Eigenst\u00e4ndige Anwendung des kompletten Clustering-Workflows</li> <li>Feature-Auswahl f\u00fcr Business-Fragestellungen</li> <li>Von technischen Ergebnissen zu Business-Empfehlungen</li> <li>Kundensegmentierung als wichtiger Use Case</li> </ul>"},{"location":"arbeitsblaetter/ul-07-kundensegmentierung/#nachste-schritte","title":"N\u00e4chste Schritte","text":"<p>Sehr gut! Du hast eine eigenst\u00e4ndige Analyse durchgef\u00fchrt. Im n\u00e4chsten Arbeitsblatt arbeitest du mit mehr Features und vergleichst verschiedene Algorithmen systematisch.</p> <p>\u27a1\ufe0f Weiter zu UL-08: Weinqualit\u00e4t-Analyse</p>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/","title":"UL-08: Weinqualit\u00e4t-Analyse","text":"<p>Advance Organizer</p> <p>Mehr Features bedeuten mehr Komplexit\u00e4t: Welche Features sind wichtig? Welche sind redundant? Hier lernst du, mit h\u00f6herdimensionalen Daten umzugehen und verschiedene Algorithmen systematisch zu vergleichen. Au\u00dferdem begegnest du zum ersten Mal dem Silhouette Score als objektive Bewertungsmetrik.</p> <p>Dein Ziel: Du kannst Feature-Korrelationen analysieren, Algorithmen vergleichen und deine Entscheidungen mit Metriken begr\u00fcnden.</p>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#lernziele","title":"Lernziele","text":"<p>Nach Bearbeitung dieses Arbeitsblatts kannst du:</p> <ul> <li> Mit mehr Features arbeiten (11 statt 5)</li> <li> Feature-Korrelationen verstehen und nutzen</li> <li> Verschiedene Algorithmen systematisch vergleichen</li> </ul>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#das-szenario","title":"Das Szenario","text":"<p>Ausgangssituation</p> <p>Ein Weinh\u00e4ndler m\u00f6chte sein Sortiment besser strukturieren. Statt nach Herkunftsregion oder Preis will er Weine nach ihren chemischen Eigenschaften gruppieren. So kann er \u00e4hnliche Weine zusammen vermarkten und Kunden passende Alternativen empfehlen.</p> <p>Deine Aufgabe: Finde Gruppen von Weinen mit \u00e4hnlichen Eigenschaften.</p>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#der-datensatz","title":"Der Datensatz","text":"<p>Der Wine Quality Datensatz enth\u00e4lt chemische Analysen von portugiesischem Wein (rot und wei\u00df):</p> Feature Beschreibung fixed acidity Feste S\u00e4uren (g/l) volatile acidity Fl\u00fcchtige S\u00e4uren (g/l) citric acid Zitronens\u00e4ure (g/l) residual sugar Restzucker (g/l) chlorides Chloride (g/l) free sulfur dioxide Freies Schwefeldioxid (mg/l) total sulfur dioxide Gesamtes Schwefeldioxid (mg/l) density Dichte (g/ml) pH pH-Wert sulphates Sulfate (g/l) alcohol Alkoholgehalt (%) quality Qualit\u00e4tsbewertung (0-10)"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#aufgabe-1-daten-laden-und-vorbereiten","title":"Aufgabe 1: Daten laden und vorbereiten","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n# Wine Quality Datens\u00e4tze laden\n# Option 1: Nur Rotwein\ndf_red = pd.read_csv('winequality-red.csv', sep=';')\ndf_red['type'] = 'red'\n\n# Option 2: Nur Wei\u00dfwein\ndf_white = pd.read_csv('winequality-white.csv', sep=';')\ndf_white['type'] = 'white'\n\n# Option 3: Beide zusammen\ndf = pd.concat([df_red, df_white], ignore_index=True)\n\nprint(f\"Rotwein: {len(df_red)} Proben\")\nprint(f\"Wei\u00dfwein: {len(df_white)} Proben\")\nprint(f\"Gesamt: {len(df)} Proben\")\n</code></pre> <p>Deine ersten Beobachtungen:</p> <p>a) Wie viele Rot- und wie viele Wei\u00dfweine gibt es? Ist das Verh\u00e4ltnis ausgeglichen?</p> <p>b) Pr\u00fcfe mit <code>df.info()</code> und <code>df.describe()</code>: Gibt es fehlende Werte? Gibt es Auff\u00e4lligkeiten in den Wertebereichen?</p> <p>Entscheidung: Analysierst du Rot- und Wei\u00dfwein zusammen oder getrennt? Begr\u00fcnde!</p> <p>Tipp</p> <p>F\u00fcr den Anfang empfehle ich, mit Rotwein zu starten (weniger Daten, schnelleres Experimentieren).</p>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#aufgabe-2-feature-korrelationen-analysieren","title":"Aufgabe 2: Feature-Korrelationen analysieren","text":"<p>Bei 11 Features ist es wichtig, Redundanzen zu erkennen:</p> <pre><code># Korrelationsmatrix\nfeatures = df.select_dtypes(include=[np.number]).columns.drop('quality')\ncorr = df[features].corr()\n\n# Heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr, annot=True, cmap='coolwarm', center=0, fmt='.2f')\nplt.title('Feature-Korrelationen')\nplt.tight_layout()\nplt.show()\n\n# Stark korrelierte Feature-Paare finden (|r| &gt; 0.7)\nhigh_corr = []\nfor i in range(len(corr)):\n    for j in range(i+1, len(corr)):\n        if abs(corr.iloc[i,j]) &gt; 0.7:\n            high_corr.append((corr.index[i], corr.columns[j], corr.iloc[i,j]))\n\nprint(\"\\nStark korrelierte Features:\")\nfor f1, f2, r in high_corr:\n    print(f\"  {f1} &lt;-&gt; {f2}: r = {r:.2f}\")\n</code></pre> <p>Fragen:</p> <p>a) Welche Features sind stark korreliert?</p> <p>b) Sollte man beide behalten oder eines entfernen? Begr\u00fcnde!</p>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#aufgabe-3-feature-auswahl-treffen","title":"Aufgabe 3: Feature-Auswahl treffen","text":"<p>Entscheide, welche Features du f\u00fcr das Clustering verwendest:</p> <pre><code># Welche Option w\u00e4hlst du? Begr\u00fcnde deine Entscheidung!\n\n# Option A: Alle Features\n# Option B: Hoch korrelierte entfernen \n# Option C: PCA zur Reduktion nutzen\n\n# Trage deine Auswahl ein (Beispiel - passe nach deiner Analyse an!):\nselected_features = ['fixed acidity', 'volatile acidity', 'citric acid', \n                     'residual sugar', 'chlorides', 'free sulfur dioxide',\n                     'density', 'pH', 'sulphates', 'alcohol']\n\nX = df[selected_features]\nprint(f\"Verwendete Features: {len(selected_features)}\")\nprint(selected_features)\n\n# Skalieren\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n</code></pre> <p>Begr\u00fcndung deiner Auswahl: (schreibe 2-3 S\u00e4tze)</p>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#aufgabe-4-clustering-mit-k-means","title":"Aufgabe 4: Clustering mit K-Means","text":"<p>F\u00fchre die Elbow- und Silhouette-Analyse selbstst\u00e4ndig durch:</p> <pre><code>from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Erstelle Listen f\u00fcr die Ergebnisse\nresults = []\nK_range = range(2, 10)\n\n# Implementiere die Schleife selbst!\n# F\u00fcr jedes k:\n#   1. Erstelle KMeans-Modell\n#   2. Trainiere und hole Labels\n#   3. Berechne Silhouette Score\n#   4. Speichere Ergebnisse\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X_scaled)\n    sil = silhouette_score(X_scaled, labels)\n    results.append({\n        'k': k,\n        'inertia': kmeans.inertia_,\n        'silhouette': sil\n    })\n\n# Visualisierung (diesen Teil kannst du so \u00fcbernehmen)\nresults_df = pd.DataFrame(results)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].plot(results_df['k'], results_df['inertia'], 'bo-')\naxes[0].set_xlabel('Anzahl Cluster')\naxes[0].set_ylabel('Inertia')\naxes[0].set_title('Elbow-Methode')\n\naxes[1].bar(results_df['k'], results_df['silhouette'], color='steelblue')\naxes[1].set_xlabel('Anzahl Cluster')\naxes[1].set_ylabel('Silhouette Score')\naxes[1].set_title('Silhouette-Analyse')\n\nplt.tight_layout()\nplt.show()\n\nprint(results_df)\n</code></pre> <p>Analysiere die Ergebnisse:</p> <p>a) Bei welchem k liegt der \"Ellbogen\"?</p> <p>b) Bei welchem k ist der Silhouette Score am h\u00f6chsten?</p> <p>c) Welches k w\u00e4hlst du? Begr\u00fcnde!</p>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#aufgabe-5-hierarchisches-clustering","title":"Aufgabe 5: Hierarchisches Clustering","text":"<p>Vergleiche mit hierarchischem Clustering:</p> <pre><code>from sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Linkage berechnen\nZ = linkage(X_scaled, method='ward')\n\n# Dendrogramm (bei vielen Daten truncated)\nplt.figure(figsize=(14, 6))\ndendrogram(Z, truncate_mode='lastp', p=50)\nplt.title('Dendrogramm (Ward-Linkage)')\nplt.xlabel('Cluster')\nplt.ylabel('Distanz')\nplt.show()\n\n# AgglomerativeClustering\nagg = AgglomerativeClustering(n_clusters=4, linkage='ward')  # Passe k an!\nlabels_hc = agg.fit_predict(X_scaled)\n\nprint(f\"\\nHierarchisches Clustering:\")\nprint(pd.Series(labels_hc).value_counts().sort_index())\n</code></pre> <p>Analysiere das Dendrogramm:</p> <p>a) Bei welcher Distanz w\u00fcrde das Dendrogramm in 3, 4 oder 5 Cluster zerfallen?</p> <p>b) Gibt es einen \"nat\u00fcrlichen\" Schnitt, wo die Distanz pl\u00f6tzlich stark ansteigt?</p> <p>c) Stimmt deine Wahl von k mit dem Dendrogramm \u00fcberein?</p>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#aufgabe-6-algorithmen-vergleichen","title":"Aufgabe 6: Algorithmen vergleichen","text":"<p>Vergleiche K-Means mit hierarchischem Clustering:</p> <pre><code># Dein gew\u00e4hltes k aus der Analyse:\nk_optimal = 4  # Passe nach deiner Analyse an!\n\n# K-Means\nkmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\nlabels_km = kmeans.fit_predict(X_scaled)\n\n# Hierarchisches Clustering\nfrom sklearn.cluster import AgglomerativeClustering\n\nagg = AgglomerativeClustering(n_clusters=k_optimal, linkage='ward')\nlabels_hc = agg.fit_predict(X_scaled)\n\n# Silhouette Scores vergleichen\nsil_km = silhouette_score(X_scaled, labels_km)\nsil_hc = silhouette_score(X_scaled, labels_hc)\n\nprint(f\"Silhouette Score:\")\nprint(f\"  K-Means:      {sil_km:.3f}\")\nprint(f\"  Hierarchisch: {sil_hc:.3f}\")\n</code></pre> <p>Fragen:</p> <p>a) Welcher Algorithmus liefert den h\u00f6heren Silhouette Score?</p> <p>b) Wie stark stimmen die Ergebnisse \u00fcberein? (Nutze <code>adjusted_rand_score</code>)</p>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#aufgabe-7-cluster-profile-erstellen","title":"Aufgabe 7: Cluster-Profile erstellen","text":"<p>Analysiere, was die Cluster charakterisiert:</p> <pre><code># K-Means Labels verwenden\ndf['Cluster'] = labels_km\n\n# Cluster-Profile\ncluster_means = df.groupby('Cluster')[selected_features].mean()\n\n# Heatmap\nplt.figure(figsize=(14, 8))\n# Normalisieren f\u00fcr bessere Visualisierung\ncluster_means_norm = (cluster_means - cluster_means.mean()) / cluster_means.std()\nsns.heatmap(cluster_means_norm.T, annot=True, cmap='RdYlGn', center=0, fmt='.2f')\nplt.title('Cluster-Profile (Z-Score normalisiert)')\nplt.xlabel('Cluster')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Analyse:</p> <p>Welche Eigenschaften zeichnen die verschiedenen Cluster aus?</p> Cluster Charakteristik 0 ___ 1 ___ 2 ___ 3 ___"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#aufgabe-8-cluster-mit-qualitat-vergleichen","title":"Aufgabe 8: Cluster mit Qualit\u00e4t vergleichen","text":"<p>Die Variable <code>quality</code> ist ein Label, das wir nicht f\u00fcrs Clustering genutzt haben. Pr\u00fcfe, ob es einen Zusammenhang gibt:</p> <pre><code># Qualit\u00e4tsverteilung pro Cluster\nplt.figure(figsize=(10, 6))\ndf.boxplot(column='quality', by='Cluster')\nplt.title('Weinqualit\u00e4t pro Cluster')\nplt.suptitle('')\nplt.xlabel('Cluster')\nplt.ylabel('Qualit\u00e4t')\nplt.show()\n\n# Durchschnittliche Qualit\u00e4t pro Cluster\nprint(df.groupby('Cluster')['quality'].agg(['mean', 'std', 'count']))\n</code></pre> <p>Fragen:</p> <p>a) Gibt es Cluster mit h\u00f6herer durchschnittlicher Qualit\u00e4t?</p> <p>b) Kann man das Clustering f\u00fcr Qualit\u00e4tsvorhersage nutzen?</p>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#zusatzaufgabe-dbscan-ausprobieren","title":"Zusatzaufgabe: DBSCAN ausprobieren","text":"<p>Findet DBSCAN Ausrei\u00dfer (z.B. besonders gute oder schlechte Weine)?</p> <pre><code>from sklearn.cluster import DBSCAN\n\n# DBSCAN \u2013 Parameter m\u00fcssen angepasst werden!\ndbscan = DBSCAN(eps=2.0, min_samples=10)\nlabels_db = dbscan.fit_predict(X_scaled)\n\nn_clusters = len(set(labels_db)) - (1 if -1 in labels_db else 0)\nn_outliers = (labels_db == -1).sum()\n\nprint(f\"Gefundene Cluster: {n_clusters}\")\nprint(f\"Ausrei\u00dfer: {n_outliers}\")\n\n# Ausrei\u00dfer analysieren\nif n_outliers &gt; 0:\n    outliers = df[labels_db == -1]\n    print(f\"\\nAusrei\u00dfer-Qualit\u00e4t: {outliers['quality'].mean():.2f} (vs. {df['quality'].mean():.2f} Durchschnitt)\")\n</code></pre> <p>DBSCAN-Parameter</p> <ul> <li><code>eps</code> zu klein \u2192 zu viele Ausrei\u00dfer</li> <li><code>eps</code> zu gro\u00df \u2192 alles ein Cluster</li> <li>Experimentiere mit verschiedenen Werten!</li> </ul>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"<p>Typische Probleme und L\u00f6sungen</p> <p>\u274c Rot- und Wei\u00dfwein gemischt ohne Kennzeichnung \u2192 F\u00fcge eine <code>type</code>-Spalte hinzu oder analysiere getrennt</p> <p>\u274c Zu viele korrelierte Features \u2192 Entferne hoch korrelierte Features (&gt; 0.8) oder nutze PCA: <pre><code>from sklearn.decomposition import PCA\npca = PCA(n_components=0.95)  # 95% Varianz erhalten\nX_reduced = pca.fit_transform(X_scaled)\n</code></pre></p> <p>\u274c Silhouette Score falsch interpretiert \u2192 H\u00f6her ist besser! Werte: - nahe 1 = sehr gute Trennung - nahe 0 = \u00fcberlappende Cluster - negativ = fehlerhafte Zuordnung</p> <p>\u274c DBSCAN findet nur 1 Cluster oder nur Ausrei\u00dfer \u2192 Parameter anpassen. Nutze k-distance Plot: <pre><code>from sklearn.neighbors import NearestNeighbors\nnn = NearestNeighbors(n_neighbors=10)\nnn.fit(X_scaled)\ndistances, _ = nn.kneighbors(X_scaled)\nplt.plot(sorted(distances[:,-1]))\n</code></pre></p>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das hast du gelernt</p> <ul> <li>Feature-Korrelationen analysieren und redundante Features identifizieren</li> <li>Systematischer Vergleich mehrerer Algorithmen</li> <li>Silhouette Score als objektive Metrik</li> <li>DBSCAN zur Ausrei\u00dfer-Erkennung</li> <li>Clustering-Ergebnisse mit externen Labels validieren</li> </ul>"},{"location":"arbeitsblaetter/ul-08-weinqualitaet/#nachste-schritte","title":"N\u00e4chste Schritte","text":"<p>Sehr gut! Du hast jetzt Erfahrung mit verschiedenen Algorithmen und Datens\u00e4tzen. Im n\u00e4chsten Arbeitsblatt bereitest du dich auf das Abschlussprojekt vor \u2013 mit einem noch gr\u00f6\u00dferen Datensatz.</p> <p>\u27a1\ufe0f Weiter zu UL-09: Musik-Clustering</p>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/","title":"UL-09: Musik-Clustering \u2013 Projektvorbereitung","text":"<p>Advance Organizer</p> <p>Das Abschlussprojekt steht vor der T\u00fcr! In diesem Arbeitsblatt erkundest du den Spotify-Datensatz und entwickelst deine Strategie. Mit ~100.000 Tracks ist das der gr\u00f6\u00dfte Datensatz bisher \u2013 du musst also auch \u00fcber Effizienz nachdenken.</p> <p>Dein Ziel: Du hast den Datensatz verstanden, eine Fragestellung formuliert und erste Clustering-Ergebnisse erzielt. Damit bist du f\u00fcr das Abschlussprojekt vorbereitet.</p>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#lernziele","title":"Lernziele","text":"<p>Nach Bearbeitung dieses Arbeitsblatts kannst du:</p> <ul> <li> Einen gr\u00f6\u00dferen Datensatz eigenst\u00e4ndig analysieren</li> <li> Feature-Auswahl und -Engineering durchf\u00fchren</li> <li> Eine sinnvolle Clustering-Strategie entwickeln</li> </ul>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#der-datensatz","title":"Der Datensatz","text":"<p>Der Spotify Tracks Datensatz enth\u00e4lt Audio-Features von Songs:</p> Feature Beschreibung Wertebereich danceability Tanzbarkeit 0.0 - 1.0 energy Energie/Intensit\u00e4t 0.0 - 1.0 key Tonart 0-11 (C, C#, D, ...) loudness Lautst\u00e4rke -60 bis 0 dB mode Dur (1) oder Moll (0) 0, 1 speechiness Sprachanteil 0.0 - 1.0 acousticness Akustische Eigenschaften 0.0 - 1.0 instrumentalness Instrumental (keine Vocals) 0.0 - 1.0 liveness Live-Aufnahme wahrscheinlich 0.0 - 1.0 valence Stimmung (fr\u00f6hlich/traurig) 0.0 - 1.0 tempo Geschwindigkeit 0 - 250 BPM duration_ms L\u00e4nge in Millisekunden variabel"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#aufgabe-1-daten-laden-und-uberblick-verschaffen","title":"Aufgabe 1: Daten laden und \u00dcberblick verschaffen","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Spotify-Datensatz laden\ndf = pd.read_csv('spotify_tracks.csv')\n\nprint(f\"Anzahl Tracks: {len(df):,}\")\nprint(f\"Anzahl Features: {len(df.columns)}\")\nprint(f\"\\nSpeicherverbrauch: {df.memory_usage().sum() / 1e6:.1f} MB\")\n\n# Spalten\u00fcbersicht\nprint(\"\\nSpalten:\")\nprint(df.columns.tolist())\n\n# Erste Zeilen\ndf.head()\n</code></pre> <p>Verschaffe dir einen \u00dcberblick:</p> <p>a) Wie viele Tracks und wie viele Features hat der Datensatz?</p> <p>b) Welche Spalten sind numerisch, welche kategorial (z.B. Artist, Genre)?</p> <p>c) Gibt es fehlende Werte? Pr\u00fcfe mit <code>df.isnull().sum()</code></p>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#aufgabe-2-audio-features-verstehen","title":"Aufgabe 2: Audio-Features verstehen","text":"<p>Visualisiere die Verteilungen der Audio-Features:</p> <pre><code>audio_features = ['danceability', 'energy', 'loudness', 'speechiness', \n                  'acousticness', 'instrumentalness', 'liveness', \n                  'valence', 'tempo']\n\nfig, axes = plt.subplots(3, 3, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, feature in enumerate(audio_features):\n    df[feature].hist(bins=50, ax=axes[i], edgecolor='black', alpha=0.7)\n    axes[i].set_title(feature)\n    axes[i].set_xlabel(feature)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Analysiere:</p> <p>a) Welche Features sind normalverteilt?</p> <p>b) Welche Features haben eine schiefe Verteilung?</p> <p>c) Gibt es Features mit vielen Nullwerten (z.B. instrumentalness)?</p>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#aufgabe-3-sampling-strategie-entwickeln","title":"Aufgabe 3: Sampling-Strategie entwickeln","text":"<p>Bei ~100.000 Tracks wird Clustering langsam. Entwickle eine Sampling-Strategie:</p> <pre><code># Zuf\u00e4lliges Sample f\u00fcr Experimente\nnp.random.seed(42)\nsample_size = 20000  # Starte mit 20k f\u00fcr schnelles Experimentieren\n\ndf_sample = df.sample(n=sample_size, random_state=42)\nprint(f\"Sample-Gr\u00f6\u00dfe: {len(df_sample):,} Tracks\")\n\n# Pr\u00fcfe: Ist das Sample repr\u00e4sentativ?\n# Vergleiche Mittelwerte\ncomparison = pd.DataFrame({\n    'Gesamt': df[audio_features].mean(),\n    'Sample': df_sample[audio_features].mean()\n})\ncomparison['Differenz %'] = abs(comparison['Gesamt'] - comparison['Sample']) / comparison['Gesamt'] * 100\nprint(comparison.round(3))\n</code></pre> <p>Sample-Gr\u00f6\u00dfe</p> <ul> <li>Exploration: 10.000 - 20.000 (schnell)</li> <li>Finale Analyse: 50.000+ (genauer)</li> <li>Vollst\u00e4ndig: Nur wenn n\u00f6tig (langsam)</li> </ul>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#aufgabe-4-feature-korrelationen-analysieren","title":"Aufgabe 4: Feature-Korrelationen analysieren","text":"<pre><code># Korrelationsmatrix\ncorr = df_sample[audio_features].corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap='coolwarm', center=0, fmt='.2f')\nplt.title('Feature-Korrelationen')\nplt.tight_layout()\nplt.show()\n\n# Stark korrelierte Features\nprint(\"Stark korrelierte Features (|r| &gt; 0.5):\")\nfor i in range(len(corr)):\n    for j in range(i+1, len(corr)):\n        if abs(corr.iloc[i,j]) &gt; 0.5:\n            print(f\"  {corr.index[i]} &lt;-&gt; {corr.columns[j]}: {corr.iloc[i,j]:.2f}\")\n</code></pre> <p>Analysiere die Korrelationen:</p> <p>a) Welche Feature-Paare sind stark korreliert? K\u00f6nnten diese redundant sein?</p> <p>b) Gibt es Features, die mit fast keinem anderen Feature korrelieren? Was k\u00f6nnte das bedeuten?</p> <p>c) Welche Features w\u00fcrdest du basierend auf der Korrelationsanalyse ausschlie\u00dfen?</p>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#aufgabe-5-feature-auswahl-treffen","title":"Aufgabe 5: Feature-Auswahl treffen","text":"<p>W\u00e4hle die Features f\u00fcr dein Clustering:</p> <pre><code># Option 1: Alle Audio-Features\nselected_features = audio_features.copy()\n\n# Option 2: Subset ohne stark korrelierte\n# selected_features = ['danceability', 'energy', 'speechiness', \n#                      'acousticness', 'instrumentalness', 'valence', 'tempo']\n\n# Option 3: Nur die \"musikalischen\" Features\n# selected_features = ['danceability', 'energy', 'valence', 'acousticness']\n\nprint(f\"Ausgew\u00e4hlte Features: {selected_features}\")\n</code></pre> <p>Deine Entscheidung:</p> <p>a) Welche Option w\u00e4hlst du und warum?</p> <p>b) Wie viele Features bleiben \u00fcbrig?</p> <p>c) Welche Features hast du bewusst ausgeschlossen und warum?</p>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#aufgabe-6-erste-clustering-versuche","title":"Aufgabe 6: Erste Clustering-Versuche","text":"<pre><code>from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Daten vorbereiten\nX = df_sample[selected_features].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Elbow + Silhouette\nresults = []\nfor k in range(2, 12):\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X_scaled)\n    results.append({\n        'k': k,\n        'inertia': kmeans.inertia_,\n        'silhouette': silhouette_score(X_scaled, labels)\n    })\n\nresults_df = pd.DataFrame(results)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].plot(results_df['k'], results_df['inertia'], 'bo-')\naxes[0].set_xlabel('k')\naxes[0].set_ylabel('Inertia')\naxes[0].set_title('Elbow-Methode')\n\naxes[1].bar(results_df['k'], results_df['silhouette'])\naxes[1].set_xlabel('k')\naxes[1].set_ylabel('Silhouette')\naxes[1].set_title('Silhouette Score')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Analysiere die Plots:</p> <p>a) Wo liegt der \"Ellbogen\" in der Elbow-Kurve?</p> <p>b) Bei welchem k ist der Silhouette Score am h\u00f6chsten?</p> <p>c) Stimmen beide Methoden \u00fcberein? Wenn nicht, welchem Ergebnis vertraust du mehr?</p> <p>Nutze PCA f\u00fcr die Visualisierung:</p> <pre><code>from sklearn.decomposition import PCA\n\n# K-Means mit gew\u00e4hltem k\nk = 5  # Anpassen nach deiner Analyse!\nkmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(X_scaled)\n\n# PCA f\u00fcr 2D-Visualisierung\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nplt.figure(figsize=(12, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.5)\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\nplt.title('Spotify Tracks Clustering')\nplt.colorbar(scatter, label='Cluster')\nplt.show()\n\nprint(f\"Erkl\u00e4rte Varianz (2 Komponenten): {pca.explained_variance_ratio_.sum():.1%}\")\n</code></pre> <p>Betrachte die Visualisierung:</p> <p>a) Sind die Cluster visuell klar getrennt oder \u00fcberlappen sie stark?</p> <p>b) Wie viel Varianz erkl\u00e4ren die 2 PCA-Komponenten? Ist das ausreichend f\u00fcr eine zuverl\u00e4ssige 2D-Darstellung?</p> <p>c) Gibt es Ausrei\u00dfer, die zu keinem Cluster zu geh\u00f6ren scheinen?</p>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#aufgabe-8-cluster-interpretieren","title":"Aufgabe 8: Cluster interpretieren","text":"<p>Was charakterisiert die verschiedenen Cluster?</p> <pre><code># Cluster-Profile\ndf_sample_clustered = df_sample.copy()\ndf_sample_clustered['Cluster'] = labels\n\ncluster_profiles = df_sample_clustered.groupby('Cluster')[selected_features].mean()\n\n# Heatmap\nplt.figure(figsize=(14, 8))\ncluster_profiles_norm = (cluster_profiles - cluster_profiles.mean()) / cluster_profiles.std()\nsns.heatmap(cluster_profiles_norm.T, annot=True, cmap='RdYlGn', center=0, fmt='.2f')\nplt.title('Cluster-Profile (normalisiert)')\nplt.xlabel('Cluster')\nplt.tight_layout()\nplt.show()\n\n# Beschreibungen notieren\nprint(\"\\nCluster-Beschreibungen:\")\nfor c in range(k):\n    profile = cluster_profiles.loc[c]\n    print(f\"\\nCluster {c}:\")\n    print(f\"  - Danceability: {profile['danceability']:.2f}\")\n    print(f\"  - Energy: {profile['energy']:.2f}\")\n    print(f\"  - Valence: {profile['valence']:.2f}\")\n</code></pre> <p>Interpretiere die Cluster:</p> <p>a) Welches Cluster k\u00f6nnte \"Party-Musik\" sein? (hohe Energy, hohe Danceability)</p> <p>b) Welches Cluster k\u00f6nnte \"Entspannungsmusik\" sein? (niedrige Energy, hohe Acousticness)</p> <p>c) Gibt es ein Cluster, das schwer zu interpretieren ist? Warum?</p> <p>d) Gib jedem Cluster einen aussagekr\u00e4ftigen Namen:</p>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#aufgabe-9-fragestellung-fur-abschlussprojekt-formulieren","title":"Aufgabe 9: Fragestellung f\u00fcr Abschlussprojekt formulieren","text":"<p>M\u00f6gliche Fragestellungen</p> <p>W\u00e4hle eine Fragestellung f\u00fcr dein Abschlussprojekt:</p> <ol> <li> <p>Musik-Typen: Welche \"Musik-Typen\" gibt es? (z.B. Party, Entspannung, Workout)</p> </li> <li> <p>Genre-Entdeckung: Kann man Genres durch Clustering \"entdecken\"?    (Vergleiche mit dem <code>genre</code>-Feld, falls vorhanden)</p> </li> <li> <p>Playlist-Empfehlungen: Wie k\u00f6nnte Spotify diese Cluster f\u00fcr Empfehlungen nutzen?</p> </li> <li> <p>Zeitliche Entwicklung: Unterscheiden sich Cluster f\u00fcr verschiedene Jahrzehnte?    (Falls <code>year</code> oder <code>release_date</code> vorhanden)</p> </li> <li> <p>Eigene Idee: _________</p> </li> </ol> <p>Deine gew\u00e4hlte Fragestellung:</p>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#aufgabe-10-nachste-schritte-planen","title":"Aufgabe 10: N\u00e4chste Schritte planen","text":"<p>Plane dein Abschlussprojekt:</p> <ul> <li> Sample-Gr\u00f6\u00dfe f\u00fcr finale Analyse festlegen</li> <li> Feature-Auswahl finalisieren</li> <li> Optimales k bestimmen</li> <li> Algorithmus(en) w\u00e4hlen</li> <li> Visualisierungen planen</li> <li> Interpretation/Story vorbereiten</li> </ul>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"<p>Typische Probleme und L\u00f6sungen</p> <p>\u274c <code>MemoryError</code> bei 100k Tracks \u2192 Arbeite mit Sample: <pre><code>df_sample = df.sample(n=20000, random_state=42)\n</code></pre></p> <p>\u274c Kategoriale Features (Genre, Artist) ignoriert \u2192 Diese k\u00f6nnen sp\u00e4ter zur Validierung dienen! <pre><code># Pr\u00fcfe Genre-Verteilung pro Cluster\ndf_sample_clustered.groupby('Cluster')['genre'].value_counts()\n</code></pre></p> <p>\u274c Tempo extrem unterschiedlich (60-200 BPM) \u2192 Skalierung ist hier besonders wichtig! Pr\u00fcfe die Verteilung.</p> <p>\u274c Keine klare Fragestellung \u2192 Formuliere eine konkrete Frage, bevor du loslegst!</p> <p>\u274c Zu viele Cluster gew\u00e4hlt \u2192 5-8 Cluster sind meist interpretierbar, mehr wird un\u00fcbersichtlich.</p>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#checkliste-fur-die-vorbereitung","title":"Checkliste f\u00fcr die Vorbereitung","text":"<ul> <li> Datensatz geladen und verstanden</li> <li> Audio-Features exploriert</li> <li> Sampling-Strategie entwickelt</li> <li> Feature-Korrelationen analysiert</li> <li> Feature-Auswahl getroffen</li> <li> Erste Clustering-Versuche durchgef\u00fchrt</li> <li> Cluster visualisiert</li> <li> Erste Interpretationen notiert</li> <li> Fragestellung formuliert</li> </ul>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das hast du gelernt</p> <ul> <li>Umgang mit gr\u00f6\u00dferen Datens\u00e4tzen durch Sampling</li> <li>Systematische Feature-Analyse f\u00fcr Audio-Daten</li> <li>Entwicklung einer eigenen Analysestrategie</li> <li>Vorbereitung eines eigenst\u00e4ndigen Projekts</li> </ul>"},{"location":"arbeitsblaetter/ul-09-musik-clustering/#nachste-schritte","title":"N\u00e4chste Schritte","text":"<p>Du bist jetzt bereit f\u00fcr das Abschlussprojekt! Dort f\u00fchrst du eine vollst\u00e4ndige Analyse durch und pr\u00e4sentierst deine Ergebnisse.</p> <p>\u27a1\ufe0f Weiter zu UL-10: Abschlussprojekt</p>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/","title":"UL-10: Abschlussprojekt \u2013 Musik-Clustering","text":"<p>Advance Organizer</p> <p>Dies ist deine Gelegenheit, alles Gelernte zusammenzuf\u00fchren: Exploration, Vorverarbeitung, Clustering, Visualisierung und Interpretation. Du arbeitest eigenst\u00e4ndig und pr\u00e4sentierst deine Ergebnisse \u2013 genau wie im echten Berufsleben als Datenanalyst.</p> <p>Dein Ziel: Eine vollst\u00e4ndige, dokumentierte Analyse mit konkreten Handlungsempfehlungen. Qualit\u00e4t geht vor Quantit\u00e4t!</p>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#lernziele","title":"Lernziele","text":"<p>Nach Bearbeitung dieses Projekts kannst du:</p> <ul> <li> Eine vollst\u00e4ndige Clustering-Analyse eigenst\u00e4ndig durchf\u00fchren</li> <li> Ergebnisse professionell pr\u00e4sentieren und dokumentieren</li> <li> Gesch\u00e4ftliche Empfehlungen aus technischen Analysen ableiten</li> </ul>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#aufgabenstellung","title":"Aufgabenstellung","text":"<p>Projektauftrag</p> <p>Szenario: Du arbeitest als Data Analyst f\u00fcr einen Musik-Streaming-Dienst. Das Produktteam m\u00f6chte verstehen, welche \"Musik-Typen\" es gibt, um bessere Playlists und Empfehlungen zu entwickeln.</p> <p>Dein Auftrag: Analysiere den Spotify Tracks Datensatz und finde Muster in der Musik.</p>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#anforderungen","title":"Anforderungen","text":"<p>Deine Analyse muss folgende Elemente enthalten:</p>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#1-datenexploration-und-vorverarbeitung","title":"1. Datenexploration und Vorverarbeitung","text":"<ul> <li> Daten laden und verstehen</li> <li> Fehlende Werte behandeln</li> <li> Feature-Auswahl begr\u00fcnden</li> <li> Skalierung durchf\u00fchren</li> </ul>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#2-clustering-analyse","title":"2. Clustering-Analyse","text":"<ul> <li> Optimale Clusteranzahl bestimmen (mit Begr\u00fcndung!)</li> <li> Mindestens zwei verschiedene Algorithmen anwenden</li> <li> Ergebnisse vergleichen</li> </ul>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#3-visualisierung","title":"3. Visualisierung","text":"<ul> <li> PCA f\u00fcr 2D-Darstellung</li> <li> Cluster-Profile (Heatmap oder Balkendiagramm)</li> <li> Mindestens 3 aussagekr\u00e4ftige Grafiken</li> </ul>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#4-interpretation","title":"4. Interpretation","text":"<ul> <li> Was bedeuten die gefundenen Cluster?</li> <li> Aussagekr\u00e4ftige Namen f\u00fcr jedes Cluster</li> <li> Beispiel-Songs pro Cluster (falls verf\u00fcgbar)</li> </ul>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#5-anwendungsvorschlag","title":"5. Anwendungsvorschlag","text":"<ul> <li> Wie k\u00f6nnte Spotify diese Erkenntnisse nutzen?</li> <li> Konkrete Vorschl\u00e4ge (z.B. Playlist-Ideen)</li> </ul>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#abgabeleistungen","title":"Abgabeleistungen","text":"Leistung Beschreibung Jupyter Notebook Dokumentierte Analyse mit Code und Erkl\u00e4rungen Pr\u00e4sentation 5-10 Minuten, wichtigste Erkenntnisse Handout 1-2 Seiten Zusammenfassung"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#bewertungskriterien","title":"Bewertungskriterien","text":"Kriterium Gewichtung Beschreibung Methoden 30% Korrekte Anwendung von Skalierung, Clustering, PCA Visualisierungen 20% Aussagekr\u00e4ftige, beschriftete Grafiken Interpretation 30% Inhaltliche Bedeutung der Cluster, Business-Relevanz Dokumentation 20% Struktur, Lesbarkeit, Nachvollziehbarkeit"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#hinweise-zur-durchfuhrung","title":"Hinweise zur Durchf\u00fchrung","text":""},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#struktur-deines-notebooks","title":"Struktur deines Notebooks","text":"<pre><code># Musik-Clustering mit Spotify-Daten\n\n## 1. Einleitung\n- Fragestellung\n- Datensatz-Beschreibung\n\n## 2. Datenexploration\n- Laden und \u00dcberblick\n- Deskriptive Statistik\n- Visualisierungen\n\n## 3. Datenvorverarbeitung\n- Feature-Auswahl (mit Begr\u00fcndung!)\n- Skalierung\n\n## 4. Clustering\n- Optimale Clusteranzahl\n- Algorithmus 1: K-Means\n- Algorithmus 2: [deine Wahl]\n- Vergleich\n\n## 5. Visualisierung\n- PCA\n- Cluster-Profile\n\n## 6. Interpretation\n- Was bedeuten die Cluster?\n- Cluster-Namen\n- Beispiel-Songs\n\n## 7. Fazit und Empfehlungen\n- Zusammenfassung\n- Vorschl\u00e4ge f\u00fcr Spotify\n</code></pre>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#tipps-fur-die-prasentation","title":"Tipps f\u00fcr die Pr\u00e4sentation","text":"<p>Pr\u00e4sentations-Tipps</p> <p>DO: - Starte mit der Fragestellung - Zeige die wichtigsten 3-4 Grafiken - Erkl\u00e4re die Cluster in verst\u00e4ndlicher Sprache - Ende mit konkreten Empfehlungen</p> <p>DON'T: - Notebook vorlesen - Jeden Code-Block erkl\u00e4ren - Zu technisch werden - Mehr als 10 Minuten</p>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#beispiel-cluster-zur-inspiration","title":"Beispiel-Cluster (zur Inspiration)","text":"<p>Typische Musik-Cluster k\u00f6nnten sein:</p> Cluster Merkmale M\u00f6glicher Name A Hohe Energy, hohe Danceability, mittlere Valence \"Party Mix\" B Niedrige Energy, hohe Acousticness, geringe Instrumentalness \"Akustik &amp; Singer-Songwriter\" C Hohe Instrumentalness, niedrige Speechiness \"Instrumentale Musik\" D Hohe Valence, mittlere Energy \"Feel-Good Hits\" E Niedrige Valence, niedrige Energy \"Melancholische Balladen\""},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"<p>Vermeide diese Fehler!</p> <p>\u274c Zu viele Cluster gew\u00e4hlt \u2192 5-8 Cluster sind meist interpretierbar, mehr wird un\u00fcbersichtlich</p> <p>\u274c Notebook schlecht dokumentiert \u2192 Markdown-Zellen nutzen! Jeder Schritt braucht eine Erkl\u00e4rung</p> <p>\u274c Visualisierungen ohne Titel/Beschriftung \u2192 Jedes Diagramm braucht: - Aussagekr\u00e4ftigen Titel - Achsenbeschriftungen - Legende (falls n\u00f6tig)</p> <p>\u274c Interpretation zu oberfl\u00e4chlich \u2192 Was bedeutet \"hohe Energy\"? Nenne konkrete Songs/Genres als Beispiele!</p> <p>\u274c Pr\u00e4sentation = Notebook vorlesen \u2192 Extrahiere die Kernaussagen, zeige nur die wichtigsten Grafiken</p> <p>\u274c Keine Business-Empfehlungen \u2192 Ohne Handlungsempfehlungen hat die Analyse keinen Wert f\u00fcr das Unternehmen</p>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#zeitplan-vorschlag","title":"Zeitplan-Vorschlag","text":"Phase Dauer Aktivit\u00e4ten Exploration 45 min Daten laden, verstehen, visualisieren Clustering 60 min k w\u00e4hlen, Algorithmen anwenden, vergleichen Interpretation 30 min Cluster analysieren, Namen vergeben Dokumentation 30 min Notebook aufr\u00e4umen, Texte schreiben Pr\u00e4sentation 15 min Folien erstellen, Handout schreiben"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#ressourcen","title":"Ressourcen","text":""},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#infoblatter-zur-wiederholung","title":"Infobl\u00e4tter zur Wiederholung","text":"<ul> <li>Einf\u00fchrung Unsupervised Learning</li> <li>K-Means Clustering</li> <li>PCA Dimensionsreduktion</li> <li>Cluster-Evaluation</li> </ul>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#code-snippets","title":"Code-Snippets","text":"<pre><code># Quick-Reference: Die wichtigsten Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\n# Daten laden\ndf = pd.read_csv('spotify_tracks.csv')\n\n# Sample f\u00fcr schnelles Arbeiten\ndf_sample = df.sample(n=20000, random_state=42)\n\n# Skalieren\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# K-Means\nkmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(X_scaled)\n\n# Silhouette Score\nscore = silhouette_score(X_scaled, labels)\n\n# PCA f\u00fcr Visualisierung\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n</code></pre>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#viel-erfolg","title":"Viel Erfolg! \ud83c\udfb5","text":"<p>Du schaffst das!</p> <p>Du hast alle Werkzeuge gelernt. Jetzt bring sie zusammen und zeig, was du kannst!</p> <p>Denk daran: - Qualit\u00e4t vor Quantit\u00e4t - Interpretation ist wichtiger als fancy Grafiken - Ein gutes Cluster-Modell erz\u00e4hlt eine Geschichte</p>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#nach-dem-projekt","title":"Nach dem Projekt","text":"<p>Nach Abgabe und Pr\u00e4sentation:</p> <ul> <li> Feedback einholen</li> <li> Was hat gut funktioniert?</li> <li> Was w\u00fcrdest du beim n\u00e4chsten Mal anders machen?</li> </ul>"},{"location":"arbeitsblaetter/ul-10-abschlussprojekt/#optionale-erweiterungen","title":"Optionale Erweiterungen","text":"<p>Falls du fr\u00fcher fertig bist oder das Projekt vertiefen m\u00f6chtest:</p> <ol> <li>Hierarchisches Clustering als dritten Algorithmus</li> <li>DBSCAN zur Ausrei\u00dfer-Erkennung (Songs, die zu keinem Cluster passen)</li> <li>Zeitliche Analyse: Unterscheiden sich Cluster nach Jahrzehnten?</li> <li>Genre-Validierung: Stimmen deine Cluster mit echten Genres \u00fcberein?</li> </ol> <p>\u27a1\ufe0f Optionale Arbeitsbl\u00e4tter:  - UL-OPT-01: Kreditkartenanalyse - UL-OPT-02: Big Data &amp; Clustering</p>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/","title":"UL-OPT-01: Kreditkartenanalyse (Optional)","text":"<p>Referenz-Material</p> <p>Dieses Arbeitsblatt enth\u00e4lt vollst\u00e4ndigen Code als Referenz und Nachschlagewerk.  Der Fokus liegt auf dem Verst\u00e4ndnis fortgeschrittener Techniken (DBSCAN, GMM), nicht auf der eigenst\u00e4ndigen Implementierung.</p> <p>Empfehlung: Arbeite den Code durch, experimentiere mit Parametern, und nutze ihn als Vorlage f\u00fcr eigene Projekte.</p> <p>Advance Organizer</p> <p>Dieser Datensatz ist der komplexeste bisher: 17 Features, viele Korrelationen, und die Frage nach Risikokunden erfordert besondere Aufmerksamkeit f\u00fcr Ausrei\u00dfer. Hier lernst du DBSCAN und GMM praktisch anzuwenden \u2013 Algorithmen, die bei K-Means nicht gut funktionierenden Datenstrukturen gl\u00e4nzen k\u00f6nnen.</p> <p>Dein Ziel: Du beherrschst den Umgang mit hochdimensionalen Daten und kannst verschiedene Algorithmen gezielt einsetzen.</p>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#lernziele","title":"Lernziele","text":"<p>Nach Bearbeitung dieses Arbeitsblatts kannst du:</p> <ul> <li> Komplexere Datens\u00e4tze mit vielen Features analysieren</li> <li> Feature-Engineering und -Selection anwenden</li> <li> DBSCAN und Gaussian Mixture Models praktisch nutzen</li> </ul>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#das-szenario","title":"Das Szenario","text":"<p>Ausgangssituation</p> <p>Eine Bank m\u00f6chte ihre Kreditkartenkunden segmentieren, um:</p> <ol> <li>Personalisierte Angebote zu entwickeln</li> <li>Risikokunden zu identifizieren (hohe Balance, wenig Zahlungen)</li> <li>Cross-Selling-Potenziale zu erkennen</li> </ol> <p>Deine Aufgabe: Analysiere das Kundenverhalten und erstelle eine Segmentierung.</p>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#der-datensatz","title":"Der Datensatz","text":"<p>Der CustomerData Datensatz enth\u00e4lt Kreditkartenverhalten von ~900 Kunden:</p> Feature Beschreibung CUST_ID Kundennummer BALANCE Kontostand BALANCE_FREQUENCY H\u00e4ufigkeit der Kontostand-Updates PURCHASES Gesamtk\u00e4ufe ONEOFF_PURCHASES Einmalk\u00e4ufe INSTALLMENTS_PURCHASES Ratenk\u00e4ufe CASH_ADVANCE Bargeldabhebungen PURCHASES_FREQUENCY Kaufh\u00e4ufigkeit ONEOFF_PURCHASES_FREQUENCY Einmalkauf-H\u00e4ufigkeit PURCHASES_INSTALLMENTS_FREQUENCY Ratenkauf-H\u00e4ufigkeit CASH_ADVANCE_FREQUENCY Bargeldabhebungs-H\u00e4ufigkeit CASH_ADVANCE_TRX Anzahl Bargeldabhebungen PURCHASES_TRX Anzahl K\u00e4ufe CREDIT_LIMIT Kreditlimit PAYMENTS Zahlungen MINIMUM_PAYMENTS Mindestzahlungen PRC_FULL_PAYMENT Anteil vollst\u00e4ndiger Zahlungen TENURE Kundenalter (Monate)"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#aufgabe-1-daten-laden-und-explorieren","title":"Aufgabe 1: Daten laden und explorieren","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n# Daten laden\ndf = pd.read_csv('CustomerData.csv')\n\nprint(f\"Anzahl Kunden: {len(df)}\")\nprint(f\"Anzahl Features: {len(df.columns)}\")\nprint(f\"\\nFehlende Werte:\")\nprint(df.isnull().sum()[df.isnull().sum() &gt; 0])\n\n# Deskriptive Statistik\ndf.describe()\n</code></pre> <p>Aufgaben:</p> <p>a) Wie viele fehlende Werte gibt es? Welche Strategie w\u00e4hlst du?</p> <p>b) Welche Features haben besonders hohe Varianz?</p>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#aufgabe-2-feature-korrelationen-analysieren","title":"Aufgabe 2: Feature-Korrelationen analysieren","text":"<p>Bei 17 Features sind Korrelationen wahrscheinlich:</p> <pre><code># ID entfernen, numerische Features\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.drop('CUST_ID', errors='ignore')\n\n# Korrelationsmatrix\ncorr = df[numeric_cols].corr()\n\nplt.figure(figsize=(14, 12))\nsns.heatmap(corr, annot=True, cmap='coolwarm', center=0, fmt='.1f', annot_kws={'size': 8})\nplt.title('Feature-Korrelationen')\nplt.tight_layout()\nplt.show()\n\n# Stark korrelierte Feature-Paare (&gt; 0.8)\nhigh_corr = []\nfor i in range(len(corr)):\n    for j in range(i+1, len(corr)):\n        if abs(corr.iloc[i,j]) &gt; 0.8:\n            high_corr.append((corr.index[i], corr.columns[j], corr.iloc[i,j]))\n\nprint(\"\\nStark korrelierte Features (|r| &gt; 0.8):\")\nfor f1, f2, r in sorted(high_corr, key=lambda x: abs(x[2]), reverse=True):\n    print(f\"  {f1} &lt;-&gt; {f2}: {r:.2f}\")\n</code></pre> <p>Frage: Welche Features w\u00fcrdest du entfernen oder zusammenfassen?</p>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#aufgabe-3-dimensionsreduktion-vor-dem-clustering","title":"Aufgabe 3: Dimensionsreduktion vor dem Clustering","text":"<p>Bei vielen Features ist PCA sinnvoll:</p> <pre><code>from sklearn.decomposition import PCA\n\n# Daten vorbereiten\nX = df[numeric_cols].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# PCA mit allen Komponenten\npca_full = PCA()\npca_full.fit(X_scaled)\n\n# Scree Plot\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].bar(range(1, len(pca_full.explained_variance_ratio_)+1), \n            pca_full.explained_variance_ratio_)\naxes[0].set_xlabel('Komponente')\naxes[0].set_ylabel('Erkl\u00e4rte Varianz')\naxes[0].set_title('Scree Plot')\n\ncumvar = np.cumsum(pca_full.explained_variance_ratio_)\naxes[1].plot(range(1, len(cumvar)+1), cumvar, 'bo-')\naxes[1].axhline(y=0.9, color='r', linestyle='--', label='90%')\naxes[1].set_xlabel('Anzahl Komponenten')\naxes[1].set_ylabel('Kumulierte Varianz')\naxes[1].set_title('Kumulierte erkl\u00e4rte Varianz')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Wie viele Komponenten f\u00fcr 90%?\nn_components = np.argmax(cumvar &gt;= 0.9) + 1\nprint(f\"\\n{n_components} Komponenten erkl\u00e4ren 90% der Varianz\")\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#aufgabe-4-dbscan-anwenden","title":"Aufgabe 4: DBSCAN anwenden","text":"<p>DBSCAN kann Ausrei\u00dfer (potenzielle Risikokunden) identifizieren:</p> <pre><code>from sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import NearestNeighbors\n\n# PCA mit gew\u00e4hlter Komponentenzahl\npca = PCA(n_components=5)  # Anpassen!\nX_pca = pca.fit_transform(X_scaled)\n\n# k-distance Plot f\u00fcr eps-Bestimmung\nnn = NearestNeighbors(n_neighbors=10)\nnn.fit(X_pca)\ndistances, _ = nn.kneighbors(X_pca)\nk_distances = np.sort(distances[:, -1])\n\nplt.figure(figsize=(10, 5))\nplt.plot(k_distances)\nplt.xlabel('Punkte (sortiert)')\nplt.ylabel('Distanz zum 10. Nachbarn')\nplt.title('k-Distance Plot f\u00fcr eps-Bestimmung')\nplt.grid(True)\nplt.show()\n</code></pre> <pre><code># DBSCAN mit gew\u00e4hlten Parametern\neps_value = 2.0  # Aus k-distance Plot ablesen!\ndbscan = DBSCAN(eps=eps_value, min_samples=10)\nlabels_db = dbscan.fit_predict(X_pca)\n\nn_clusters = len(set(labels_db)) - (1 if -1 in labels_db else 0)\nn_outliers = (labels_db == -1).sum()\n\nprint(f\"Gefundene Cluster: {n_clusters}\")\nprint(f\"Ausrei\u00dfer: {n_outliers} ({n_outliers/len(labels_db)*100:.1f}%)\")\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#aufgabe-5-ausreier-analysieren","title":"Aufgabe 5: Ausrei\u00dfer analysieren","text":"<p>Sind die Ausrei\u00dfer wirklich \"Risikokunden\"?</p> <pre><code># Ausrei\u00dfer vs. Normale\ndf_analysis = df.copy()\ndf_analysis['is_outlier'] = labels_db == -1\n\n# Vergleich\ncomparison = df_analysis.groupby('is_outlier')[numeric_cols].mean().T\ncomparison.columns = ['Normale', 'Ausrei\u00dfer']\ncomparison['Differenz %'] = (comparison['Ausrei\u00dfer'] - comparison['Normale']) / comparison['Normale'] * 100\n\nprint(\"Vergleich Ausrei\u00dfer vs. Normale Kunden:\")\nprint(comparison.sort_values('Differenz %', ascending=False).round(1))\n</code></pre> <p>Analyse:</p> <p>a) Was zeichnet die Ausrei\u00dfer aus?</p> <p>b) Sind das wirklich Risikokunden?</p>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#aufgabe-6-gaussian-mixture-models","title":"Aufgabe 6: Gaussian Mixture Models","text":"<p>GMM kann \"weiche\" Cluster finden (Wahrscheinlichkeiten statt harte Zuordnung):</p> <pre><code>from sklearn.mixture import GaussianMixture\n\n# Optimale Anzahl Komponenten mit BIC\nbics = []\nfor n in range(2, 10):\n    gmm = GaussianMixture(n_components=n, random_state=42)\n    gmm.fit(X_pca)\n    bics.append((n, gmm.bic(X_pca)))\n\nbic_df = pd.DataFrame(bics, columns=['n_components', 'BIC'])\nplt.figure(figsize=(8, 4))\nplt.plot(bic_df['n_components'], bic_df['BIC'], 'bo-')\nplt.xlabel('Anzahl Komponenten')\nplt.ylabel('BIC (niedriger = besser)')\nplt.title('GMM: Modellauswahl mit BIC')\nplt.show()\n</code></pre> <pre><code># GMM mit optimalem n\nn_opt = 4  # Anpassen basierend auf BIC!\ngmm = GaussianMixture(n_components=n_opt, random_state=42)\nlabels_gmm = gmm.fit_predict(X_pca)\n\n# Wahrscheinlichkeiten\nprobs = gmm.predict_proba(X_pca)\nprint(f\"\\nWahrscheinlichkeits-Matrix Shape: {probs.shape}\")\nprint(\"\\nBeispiel - Erste 5 Kunden:\")\nprint(pd.DataFrame(probs[:5], columns=[f'Cluster_{i}' for i in range(n_opt)]).round(3))\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#aufgabe-7-algorithmen-vergleichen","title":"Aufgabe 7: Algorithmen vergleichen","text":"<pre><code>from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# K-Means zum Vergleich\nkmeans = KMeans(n_clusters=n_opt, random_state=42, n_init=10)\nlabels_km = kmeans.fit_predict(X_pca)\n\n# Silhouette Scores\nprint(\"Silhouette Scores:\")\nprint(f\"  K-Means: {silhouette_score(X_pca, labels_km):.3f}\")\nprint(f\"  GMM:     {silhouette_score(X_pca, labels_gmm):.3f}\")\n\n# DBSCAN nur f\u00fcr Nicht-Ausrei\u00dfer\nif n_outliers &lt; len(labels_db):\n    mask = labels_db != -1\n    print(f\"  DBSCAN:  {silhouette_score(X_pca[mask], labels_db[mask]):.3f} (ohne Ausrei\u00dfer)\")\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#aufgabe-8-kundenprofile-erstellen","title":"Aufgabe 8: Kundenprofile erstellen","text":"<p>Erstelle aussagekr\u00e4ftige Kundenprofile:</p> <pre><code># K-Means oder GMM Labels verwenden\ndf_final = df.copy()\ndf_final['Cluster'] = labels_km\n\n# Cluster-Profile\nprofiles = df_final.groupby('Cluster')[numeric_cols].mean()\n\n# Heatmap\nplt.figure(figsize=(14, 10))\nprofiles_norm = (profiles - profiles.mean()) / profiles.std()\nsns.heatmap(profiles_norm.T, annot=True, cmap='RdYlGn', center=0, fmt='.2f')\nplt.title('Kundenprofile (Z-Score normalisiert)')\nplt.xlabel('Cluster')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#aufgabe-9-cluster-interpretieren-und-benennen","title":"Aufgabe 9: Cluster interpretieren und benennen","text":"<p>Gib den Clustern aussagekr\u00e4ftige Namen:</p> <pre><code># Beispiel-Interpretation (anpassen!)\ncluster_names = {\n    0: '___________',  # z.B. \"Premium-Kunden\"\n    1: '___________',  # z.B. \"Gelegenheitsk\u00e4ufer\"\n    2: '___________',  # z.B. \"Bargeld-Nutzer\"\n    3: '___________'   # z.B. \"Risikokunden\"\n}\n\ndf_final['Segment'] = df_final['Cluster'].map(cluster_names)\n\n# Zusammenfassung\nprint(\"Kundensegmente:\")\nfor cluster_id, name in cluster_names.items():\n    count = (df_final['Cluster'] == cluster_id).sum()\n    print(f\"\\n{name} ({count} Kunden):\")\n    profile = profiles.loc[cluster_id]\n    print(f\"  - Balance: ${profile['BALANCE']:,.0f}\")\n    print(f\"  - K\u00e4ufe: ${profile['PURCHASES']:,.0f}\")\n    print(f\"  - Zahlungen: ${profile['PAYMENTS']:,.0f}\")\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"<p>Typische Probleme und L\u00f6sungen</p> <p>\u274c 17 Features auf einmal clustern \u2192 Starte mit PCA oder Feature-Selection (5-8 wichtigste Features)</p> <p>\u274c DBSCAN-Parameter blind gew\u00e4hlt \u2192 Nutze k-distance Plot zur eps-Bestimmung: <pre><code>from sklearn.neighbors import NearestNeighbors\nnn = NearestNeighbors(n_neighbors=10)\ndistances, _ = nn.kneighbors(X)\nplt.plot(sorted(distances[:,-1]))\n</code></pre></p> <p>\u274c GMM ohne Kovarianz-Typ getestet \u2192 Probiere verschiedene: <pre><code>GaussianMixture(covariance_type='full')   # Flexibelste\nGaussianMixture(covariance_type='diag')   # Schneller\nGaussianMixture(covariance_type='spherical')  # Wie K-Means\n</code></pre></p> <p>\u274c Ausrei\u00dfer = Risikokunden? \u2192 Nicht automatisch! Pr\u00fcfe, ob Ausrei\u00dfer wirklich problematisch sind.</p>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das hast du gelernt</p> <ul> <li>Umgang mit hochdimensionalen Daten (17 Features)</li> <li>PCA zur Dimensionsreduktion vor Clustering</li> <li>DBSCAN f\u00fcr Ausrei\u00dfer-Erkennung</li> <li>GMM f\u00fcr \"weiche\" Cluster mit Wahrscheinlichkeiten</li> <li>Systematischer Algorithmen-Vergleich</li> </ul>"},{"location":"arbeitsblaetter/ul-opt-01-kreditkarten/#weiter-zu","title":"Weiter zu","text":"<p>\u27a1\ufe0f UL-OPT-02: Big Data &amp; Clustering</p>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/","title":"UL-OPT-02: Big Data &amp; Clustering (Optional)","text":"<p>Referenz-Material</p> <p>Dieses Arbeitsblatt enth\u00e4lt vollst\u00e4ndigen Code als Referenz und Nachschlagewerk.  Die Techniken (Sampling, Chunked Processing, Mini-Batch K-Means) sind f\u00fcr reale Big-Data-Szenarien wichtig.</p> <p>Empfehlung: Verstehe die Konzepte, probiere den Code aus, und adaptiere ihn f\u00fcr eigene gro\u00dfe Datens\u00e4tze.</p> <p>Advance Organizer</p> <p>In der echten Arbeitswelt sind Datens\u00e4tze oft zu gro\u00df, um sie komplett in den Speicher zu laden. Mit 7,7 Millionen Unf\u00e4llen lernst du hier Techniken, die du sp\u00e4ter im Beruf brauchen wirst: Sampling, Chunked Processing und speichereffiziente Algorithmen.</p> <p>Dein Ziel: Du verstehst die Grenzen von Standard-Clustering und kannst L\u00f6sungsstrategien f\u00fcr Big-Data-Szenarien entwickeln.</p>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#lernziele","title":"Lernziele","text":"<p>Nach Bearbeitung dieses Arbeitsblatts kannst du:</p> <ul> <li> Herausforderungen bei gro\u00dfen Datens\u00e4tzen verstehen</li> <li> Sampling-Strategien anwenden und bewerten</li> <li> Mini-Batch K-Means f\u00fcr effizientes Clustering nutzen</li> <li> Daten in Chunks verarbeiten</li> </ul>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#das-szenario","title":"Das Szenario","text":"<p>Ausgangssituation</p> <p>Ein Verkehrsministerium m\u00f6chte Unfallmuster analysieren, um:</p> <ol> <li>Unfallschwerpunkte zu identifizieren</li> <li>Zeitliche Muster zu erkennen (Tageszeit, Wochentag, Monat)</li> <li>Wetterbezogene Zusammenh\u00e4nge zu finden</li> </ol> <p>Problem: Der Datensatz hat 7,7 Millionen Zeilen \u2013 zu gro\u00df f\u00fcr Standard-Methoden!</p> <p>Deine Aufgabe: Entwickle eine Strategie f\u00fcr die Analyse!</p>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#der-datensatz","title":"Der Datensatz","text":"<p>Der US-Accidents Datensatz enth\u00e4lt Verkehrsunf\u00e4lle in den USA von 2016-2023:</p> Feature Beschreibung Start_Time Zeitpunkt des Unfalls End_Time Ende der Behinderung Start_Lat, Start_Lng GPS-Koordinaten Severity Schweregrad (1-4) Temperature Temperatur (\u00b0F) Humidity Luftfeuchtigkeit (%) Visibility Sichtweite (Meilen) Weather_Condition Wetterbedingung Sunrise_Sunset Tag/Nacht State, City Ort"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#aufgabe-1-datensatz-groe-verstehen","title":"Aufgabe 1: Datensatz-Gr\u00f6\u00dfe verstehen","text":"<p>Bevor du l\u00e4dst, pr\u00fcfe die Gr\u00f6\u00dfe:</p> <pre><code>import pandas as pd\nimport numpy as np\nimport os\n\n# Dateigr\u00f6\u00dfe pr\u00fcfen (ohne zu laden!)\nfile_path = 'US_Accidents_March23.csv'\nfile_size_mb = os.path.getsize(file_path) / 1e6\nprint(f\"Dateigr\u00f6\u00dfe: {file_size_mb:.0f} MB\")\n\n# Erste Zeilen lesen (ohne alles zu laden)\ndf_peek = pd.read_csv(file_path, nrows=5)\nprint(f\"\\nSpalten: {len(df_peek.columns)}\")\nprint(df_peek.columns.tolist())\n</code></pre> <p>Fragen:</p> <p>a) Wie gro\u00df ist die Datei?</p> <p>b) Wie viel RAM w\u00fcrde sie ben\u00f6tigen (Faustregel: 2-3x Dateigr\u00f6\u00dfe)?</p>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#aufgabe-2-chunked-loading","title":"Aufgabe 2: Chunked Loading","text":"<p>Lade die Daten in Teilen:</p> <pre><code>import time\n\n# Variante 1: Nur Anzahl Zeilen z\u00e4hlen\nstart = time.time()\nn_rows = 0\nfor chunk in pd.read_csv(file_path, chunksize=100000, usecols=['ID']):\n    n_rows += len(chunk)\nprint(f\"Anzahl Zeilen: {n_rows:,} (in {time.time()-start:.1f}s gez\u00e4hlt)\")\n\n# Variante 2: Nur relevante Spalten laden\nrelevant_cols = ['Severity', 'Start_Time', 'Start_Lat', 'Start_Lng',\n                 'Temperature(F)', 'Humidity(%)', 'Visibility(mi)', \n                 'Weather_Condition', 'State']\n\ndf_sample = pd.read_csv(file_path, nrows=100000, usecols=relevant_cols)\nprint(f\"\\nSample-Shape: {df_sample.shape}\")\nprint(f\"Speicher: {df_sample.memory_usage().sum() / 1e6:.1f} MB\")\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#aufgabe-3-sampling-strategien-vergleichen","title":"Aufgabe 3: Sampling-Strategien vergleichen","text":"<p>Vergleiche verschiedene Sampling-Methoden:</p> <pre><code># Strategie 1: Zuf\u00e4lliges Sampling\nnp.random.seed(42)\nsample_random = pd.read_csv(file_path, usecols=relevant_cols,\n                             skiprows=lambda x: x &gt; 0 and np.random.random() &gt; 0.01)\nprint(f\"Zuf\u00e4lliges Sample: {len(sample_random):,} Zeilen\")\n\n# Strategie 2: Systematisches Sampling (jede 100. Zeile)\nsample_systematic = pd.read_csv(file_path, usecols=relevant_cols,\n                                 skiprows=lambda x: x &gt; 0 and x % 100 != 0)\nprint(f\"Systematisches Sample: {len(sample_systematic):,} Zeilen\")\n\n# Strategie 3: Stratifiziertes Sampling (aus jeder Chunk)\nsample_parts = []\nfor i, chunk in enumerate(pd.read_csv(file_path, usecols=relevant_cols, chunksize=500000)):\n    sample_parts.append(chunk.sample(frac=0.02, random_state=42))\n    if i &gt;= 4:  # Nur erste 5 Chunks\n        break\n\nsample_stratified = pd.concat(sample_parts, ignore_index=True)\nprint(f\"Stratifiziertes Sample: {len(sample_stratified):,} Zeilen\")\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#aufgabe-4-sample-reprasentativitat-prufen","title":"Aufgabe 4: Sample-Repr\u00e4sentativit\u00e4t pr\u00fcfen","text":"<p>Ist das Sample repr\u00e4sentativ?</p> <pre><code># Vergleiche Verteilungen\ndef compare_distributions(sample, col_name):\n    \"\"\"Vergleicht Sample mit theoretischer Gesamtverteilung.\"\"\"\n    print(f\"\\n{col_name}:\")\n    print(sample[col_name].value_counts(normalize=True).head(5).round(3))\n\ncompare_distributions(sample_stratified, 'Severity')\ncompare_distributions(sample_stratified, 'State')\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#aufgabe-5-feature-engineering","title":"Aufgabe 5: Feature Engineering","text":"<p>Extrahiere n\u00fctzliche Features aus dem Zeitstempel:</p> <pre><code># Zeitliche Features\nsample_stratified['Start_Time'] = pd.to_datetime(sample_stratified['Start_Time'])\n\nsample_stratified['Hour'] = sample_stratified['Start_Time'].dt.hour\nsample_stratified['DayOfWeek'] = sample_stratified['Start_Time'].dt.dayofweek\nsample_stratified['Month'] = sample_stratified['Start_Time'].dt.month\nsample_stratified['Year'] = sample_stratified['Start_Time'].dt.year\n\n# Visualisierung\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\nsample_stratified['Hour'].value_counts().sort_index().plot(kind='bar', ax=axes[0])\naxes[0].set_title('Unf\u00e4lle nach Stunde')\naxes[0].set_xlabel('Stunde')\n\nsample_stratified['DayOfWeek'].value_counts().sort_index().plot(kind='bar', ax=axes[1])\naxes[1].set_title('Unf\u00e4lle nach Wochentag')\naxes[1].set_xlabel('Tag (0=Mo)')\n\nsample_stratified['Month'].value_counts().sort_index().plot(kind='bar', ax=axes[2])\naxes[2].set_title('Unf\u00e4lle nach Monat')\naxes[2].set_xlabel('Monat')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#aufgabe-6-features-fur-clustering-auswahlen","title":"Aufgabe 6: Features f\u00fcr Clustering ausw\u00e4hlen","text":"<pre><code># Numerische Features f\u00fcr Clustering\nclustering_features = ['Hour', 'DayOfWeek', 'Temperature(F)', 'Humidity(%)', 'Visibility(mi)']\n\n# Fehlende Werte behandeln\nX = sample_stratified[clustering_features].dropna()\nprint(f\"Datenpunkte f\u00fcr Clustering: {len(X):,}\")\n\n# Skalieren\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#aufgabe-7-mini-batch-k-means","title":"Aufgabe 7: Mini-Batch K-Means","text":"<p>Mini-Batch K-Means ist optimiert f\u00fcr gro\u00dfe Datenmengen:</p> <pre><code>from sklearn.cluster import KMeans, MiniBatchKMeans\nfrom sklearn.metrics import silhouette_score\nimport time\n\n# Vergleich: Standard vs. Mini-Batch\nprint(\"Vergleich K-Means vs. Mini-Batch K-Means:\")\nprint(\"=\" * 50)\n\nfor k in [4, 6, 8]:\n    # Standard K-Means\n    start = time.time()\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels_km = kmeans.fit_predict(X_scaled)\n    time_km = time.time() - start\n    sil_km = silhouette_score(X_scaled, labels_km)\n\n    # Mini-Batch K-Means\n    start = time.time()\n    mbkmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=1024)\n    labels_mb = mbkmeans.fit_predict(X_scaled)\n    time_mb = time.time() - start\n    sil_mb = silhouette_score(X_scaled, labels_mb)\n\n    print(f\"\\nk={k}:\")\n    print(f\"  K-Means:       {time_km:.2f}s, Silhouette={sil_km:.3f}\")\n    print(f\"  Mini-Batch:    {time_mb:.2f}s, Silhouette={sil_mb:.3f}\")\n    print(f\"  Speedup:       {time_km/time_mb:.1f}x schneller\")\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#aufgabe-8-partial-fit-fur-sehr-groe-daten","title":"Aufgabe 8: Partial Fit f\u00fcr sehr gro\u00dfe Daten","text":"<p>Bei Daten, die nicht in den Speicher passen:</p> <pre><code>from sklearn.cluster import MiniBatchKMeans\n\n# Mini-Batch K-Means mit Partial Fit\nmbk = MiniBatchKMeans(n_clusters=5, random_state=42, batch_size=1024)\n\n# Trainiere auf Chunks\nprint(\"Training auf Chunks:\")\nchunk_count = 0\nfor chunk in pd.read_csv(file_path, usecols=relevant_cols, chunksize=100000):\n    # Feature Engineering\n    chunk['Start_Time'] = pd.to_datetime(chunk['Start_Time'])\n    chunk['Hour'] = chunk['Start_Time'].dt.hour\n    chunk['DayOfWeek'] = chunk['Start_Time'].dt.dayofweek\n\n    # Features extrahieren und skalieren\n    X_chunk = chunk[clustering_features].dropna()\n    if len(X_chunk) &gt; 0:\n        X_chunk_scaled = scaler.transform(X_chunk)  # Nutze bereits gefitteten Scaler!\n        mbk.partial_fit(X_chunk_scaled)\n        chunk_count += 1\n        print(f\"  Chunk {chunk_count} verarbeitet ({len(X_chunk):,} Zeilen)\")\n\n    if chunk_count &gt;= 5:  # Stoppe nach 5 Chunks f\u00fcr Demo\n        break\n\nprint(f\"\\nTraining abgeschlossen mit {chunk_count} Chunks\")\nprint(f\"Cluster-Zentren Shape: {mbk.cluster_centers_.shape}\")\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#aufgabe-9-ergebnisse-interpretieren","title":"Aufgabe 9: Ergebnisse interpretieren","text":"<p>Analysiere die gefundenen Cluster:</p> <pre><code># Labels f\u00fcr Sample vorhersagen\nlabels = mbkmeans.predict(X_scaled)\nsample_clustered = sample_stratified.copy()\nsample_clustered = sample_clustered.loc[X.index]  # Nur Zeilen ohne NaN\nsample_clustered['Cluster'] = labels\n\n# Cluster-Profile\ncluster_profiles = sample_clustered.groupby('Cluster')[clustering_features].mean()\nprint(\"Cluster-Profile:\")\nprint(cluster_profiles.round(2))\n\n# Visualisierung\nimport seaborn as sns\nplt.figure(figsize=(12, 6))\ncluster_profiles_norm = (cluster_profiles - cluster_profiles.mean()) / cluster_profiles.std()\nsns.heatmap(cluster_profiles_norm.T, annot=True, cmap='RdYlGn', center=0, fmt='.2f')\nplt.title('Unfall-Cluster Profile')\nplt.xlabel('Cluster')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#aufgabe-10-speicheroptimierung","title":"Aufgabe 10: Speicheroptimierung","text":"<pre><code>def optimize_dtypes(df):\n    \"\"\"Reduziere Speicherverbrauch durch optimierte Datentypen.\"\"\"\n    before = df.memory_usage().sum() / 1e6\n\n    for col in df.select_dtypes(include=['float64']).columns:\n        df[col] = df[col].astype('float32')\n\n    for col in df.select_dtypes(include=['int64']).columns:\n        if df[col].min() &gt;= 0 and df[col].max() &lt; 255:\n            df[col] = df[col].astype('uint8')\n        elif df[col].min() &gt;= 0 and df[col].max() &lt; 65535:\n            df[col] = df[col].astype('uint16')\n\n    after = df.memory_usage().sum() / 1e6\n    print(f\"Speicher: {before:.1f} MB \u2192 {after:.1f} MB ({(1-after/before)*100:.0f}% gespart)\")\n    return df\n\nsample_optimized = optimize_dtypes(sample_stratified.copy())\n</code></pre>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#aufgabe-11-reflexion","title":"Aufgabe 11: Reflexion","text":"<p>Reflexionsfragen</p> <ol> <li> <p>Ab welcher Datengr\u00f6\u00dfe werden diese Techniken notwendig?</p> </li> <li> <p>Wann ist Sampling vertretbar? Wann nicht?</p> </li> <li> <p>Was sind die Trade-offs bei Mini-Batch K-Means?</p> </li> <li> <p>Wie w\u00fcrdest du die Analyse in der echten Welt fortsetzen?</p> </li> </ol>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"<p>Typische Probleme und L\u00f6sungen</p> <p>\u274c <code>MemoryError</code> beim Laden \u2192 Nutze <code>usecols</code> um nur ben\u00f6tigte Spalten zu laden: <pre><code>pd.read_csv(file, usecols=['col1', 'col2'])\n</code></pre></p> <p>\u274c Sample nicht repr\u00e4sentativ \u2192 Stratifiziertes Sampling nach wichtigen Kategorien: <pre><code>from sklearn.model_selection import train_test_split\nsample, _ = train_test_split(df, stratify=df['State'], train_size=0.01)\n</code></pre></p> <p>\u274c Mini-Batch K-Means mit zu kleiner <code>batch_size</code> \u2192 Minimum ~1000, sonst instabile Ergebnisse</p> <p>\u274c Datetime nicht geparst \u2192 <pre><code>pd.read_csv(..., parse_dates=['Start_Time'])\n</code></pre></p> <p>\u274c Clustering ohne Feature-Selection \u2192 Der Datensatz hat ~47 Spalten! W\u00e4hle 5-10 relevante Features aus</p>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das hast du gelernt</p> <ul> <li>Sampling: Zuf\u00e4llig, systematisch, stratifiziert</li> <li>Chunked Processing: <code>read_csv(chunksize=...)</code> + <code>partial_fit()</code></li> <li>Mini-Batch K-Means: 10-100x schneller als Standard</li> <li>Speicheroptimierung: float64 \u2192 float32, int64 \u2192 uint8</li> <li>\"Mehr Daten\" \u2260 \"bessere Ergebnisse\"</li> </ul>"},{"location":"arbeitsblaetter/ul-opt-02-big-data/#weiterfuhrende-ressourcen","title":"Weiterf\u00fchrende Ressourcen","text":"<ul> <li>scikit-learn: Scaling Strategies</li> <li>Pandas: Enhancing Performance</li> <li>Dask f\u00fcr Out-of-Core Computing</li> </ul>"},{"location":"infoblaetter/cluster-evaluation/","title":"Cluster-Evaluation &amp; Interpretation","text":""},{"location":"infoblaetter/cluster-evaluation/#das-evaluationsproblem","title":"Das Evaluationsproblem","text":"<p>Bei Unsupervised Learning gibt es keine \"richtigen\" Labels. Wie bewertet man also, ob das Clustering gut ist?</p> <p></p> <p>Zwei Ans\u00e4tze: 1. Interne Metriken: Basieren nur auf den Daten 2. Externe Metriken: Vergleichen mit bekannten Labels (wenn vorhanden)</p>"},{"location":"infoblaetter/cluster-evaluation/#interne-metriken","title":"Interne Metriken","text":""},{"location":"infoblaetter/cluster-evaluation/#silhouette-score","title":"Silhouette Score","text":"<p>Misst, wie gut Punkte zu ihrem Cluster passen vs. zu anderen Clustern.</p> <p>$$s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$$</p> <ul> <li>a(i): Mittlere Distanz zu Punkten im eigenen Cluster</li> <li>b(i): Mittlere Distanz zum n\u00e4chsten fremden Cluster</li> </ul> <pre><code>from sklearn.metrics import silhouette_score, silhouette_samples\n\n# Gesamt-Score\nscore = silhouette_score(X_scaled, labels)\nprint(f\"Silhouette Score: {score:.3f}\")\n\n# Score pro Datenpunkt\nsample_scores = silhouette_samples(X_scaled, labels)\n</code></pre> Wert Interpretation 0.7 - 1.0 Starke Cluster-Struktur 0.5 - 0.7 Vern\u00fcnftige Struktur 0.25 - 0.5 Schwache Struktur, m\u00f6glicherweise \u00fcberlappend &lt; 0.25 Keine klare Struktur"},{"location":"infoblaetter/cluster-evaluation/#silhouette-plot","title":"Silhouette-Plot","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\ny_lower = 10\nfor i in range(n_clusters):\n    cluster_silhouette = sample_scores[labels == i]\n    cluster_silhouette.sort()\n\n    y_upper = y_lower + len(cluster_silhouette)\n    ax.fill_betweenx(np.arange(y_lower, y_upper),\n                      0, cluster_silhouette, alpha=0.7)\n    ax.text(-0.05, y_lower + 0.5 * len(cluster_silhouette), str(i))\n    y_lower = y_upper + 10\n\nax.axvline(x=score, color='red', linestyle='--', label=f'Durchschnitt: {score:.2f}')\nax.set_xlabel('Silhouette Score')\nax.set_ylabel('Cluster')\nax.legend()\nplt.show()\n</code></pre>"},{"location":"infoblaetter/cluster-evaluation/#davies-bouldin-index","title":"Davies-Bouldin Index","text":"<p>Misst die \u00c4hnlichkeit zwischen Clustern. Niedriger ist besser!</p> <pre><code>from sklearn.metrics import davies_bouldin_score\n\ndb_score = davies_bouldin_score(X_scaled, labels)\nprint(f\"Davies-Bouldin Index: {db_score:.3f}\")\n</code></pre> Wert Interpretation &lt; 1.0 Gute Trennung 1.0 - 2.0 Akzeptabel &gt; 2.0 Cluster \u00fcberlappen stark"},{"location":"infoblaetter/cluster-evaluation/#calinski-harabasz-index","title":"Calinski-Harabasz Index","text":"<p>Verh\u00e4ltnis von Zwischen-Cluster-Streuung zu Innerhalb-Cluster-Streuung. H\u00f6her ist besser!</p> <pre><code>from sklearn.metrics import calinski_harabasz_score\n\nch_score = calinski_harabasz_score(X_scaled, labels)\nprint(f\"Calinski-Harabasz Index: {ch_score:.1f}\")\n</code></pre>"},{"location":"infoblaetter/cluster-evaluation/#metriken-vergleich","title":"Metriken-Vergleich","text":"Metrik Richtung Vorteile Nachteile Silhouette H\u00f6her = besser Intuitiv, -1 bis 1 Langsam bei gro\u00dfen Daten Davies-Bouldin Niedriger = besser Schnell Bevorzugt konvexe Cluster Calinski-Harabasz H\u00f6her = besser Sehr schnell Skaliert mit Datenanzahl <pre><code># Alle Metriken f\u00fcr verschiedene k berechnen\nfrom sklearn.cluster import KMeans\n\nresults = []\nfor k in range(2, 10):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(X_scaled)\n\n    results.append({\n        'k': k,\n        'silhouette': silhouette_score(X_scaled, labels),\n        'davies_bouldin': davies_bouldin_score(X_scaled, labels),\n        'calinski_harabasz': calinski_harabasz_score(X_scaled, labels),\n        'inertia': kmeans.inertia_\n    })\n\ndf_results = pd.DataFrame(results)\nprint(df_results)\n</code></pre>"},{"location":"infoblaetter/cluster-evaluation/#cluster-interpretation","title":"Cluster-Interpretation","text":""},{"location":"infoblaetter/cluster-evaluation/#cluster-profile-erstellen","title":"Cluster-Profile erstellen","text":"<pre><code># Mittelwerte pro Cluster\ncluster_profiles = df.groupby('Cluster').mean()\nprint(cluster_profiles)\n\n# Als Heatmap visualisieren\nimport seaborn as sns\nplt.figure(figsize=(12, 8))\nsns.heatmap(cluster_profiles.T, annot=True, cmap='coolwarm', center=0)\nplt.title('Cluster-Profile')\nplt.show()\n</code></pre>"},{"location":"infoblaetter/cluster-evaluation/#normalisierte-profile","title":"Normalisierte Profile","text":"<pre><code># F\u00fcr besseren Vergleich: Z-Score pro Feature\nfrom scipy import stats\n\nprofiles_normalized = cluster_profiles.apply(stats.zscore, axis=0)\nplt.figure(figsize=(12, 8))\nsns.heatmap(profiles_normalized.T, annot=True, cmap='RdYlGn', center=0)\nplt.title('Cluster-Profile (Z-Score normalisiert)')\nplt.show()\n</code></pre>"},{"location":"infoblaetter/cluster-evaluation/#radar-chart-spider-plot","title":"Radar-Chart (Spider-Plot)","text":"<pre><code>from math import pi\n\ndef radar_chart(df, cluster_col='Cluster'):\n    categories = df.drop(cluster_col, axis=1).columns\n    N = len(categories)\n\n    angles = [n / float(N) * 2 * pi for n in range(N)]\n    angles += angles[:1]\n\n    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n\n    for cluster in df[cluster_col].unique():\n        values = df[df[cluster_col] == cluster].drop(cluster_col, axis=1).values.flatten().tolist()\n        values += values[:1]\n        ax.plot(angles, values, 'o-', linewidth=2, label=f'Cluster {cluster}')\n        ax.fill(angles, values, alpha=0.25)\n\n    ax.set_xticks(angles[:-1])\n    ax.set_xticklabels(categories)\n    ax.legend(loc='upper right')\n    plt.show()\n</code></pre>"},{"location":"infoblaetter/cluster-evaluation/#cluster-benennen","title":"Cluster benennen","text":"<p>Nach der Analyse solltest du Clustern aussagekr\u00e4ftige Namen geben:</p> <pre><code># Beispiel: L\u00e4nder-Clustering\ncluster_names = {\n    0: 'Entwicklungsl\u00e4nder',\n    1: 'Schwellenl\u00e4nder',\n    2: 'Industriel\u00e4nder'\n}\n\ndf['Cluster_Name'] = df['Cluster'].map(cluster_names)\n</code></pre> <p>Gute Cluster-Namen</p> <ul> <li>Beschreiben die Hauptmerkmale</li> <li>Sind verst\u00e4ndlich f\u00fcr Nicht-Techniker</li> <li>Keine Zahlen (Cluster 0, 1, 2) in Berichten verwenden!</li> </ul>"},{"location":"infoblaetter/cluster-evaluation/#wann-clustering-nicht-nutzen","title":"Wann Clustering NICHT nutzen","text":"<p>Clustering ist problematisch wenn...</p> <p>1. Keine nat\u00fcrliche Gruppenstruktur <pre><code>Silhouette Score &lt; 0.25 \u2192 Keine klaren Cluster\n</code></pre></p> <p>2. Du exakte Vorhersagen brauchst <pre><code>Clustering = Exploration, nicht Pr\u00e4diktion\n</code></pre></p> <p>3. Du bereits Labels hast <pre><code>\u2192 Supervised Learning nutzen!\n</code></pre></p> <p>4. Die Features unkorreliert sind <pre><code>Random Features \u2192 Random Cluster\n</code></pre></p>"},{"location":"infoblaetter/cluster-evaluation/#cluster-validieren","title":"Cluster validieren","text":"<pre><code># Sind die Cluster \"echt\"?\ndef cluster_stability(X, n_runs=10, k=3):\n    \"\"\"F\u00fchre Clustering mehrmals aus und vergleiche Ergebnisse.\"\"\"\n    from sklearn.metrics import adjusted_rand_score\n\n    results = []\n    for _ in range(n_runs):\n        labels = KMeans(n_clusters=k).fit_predict(X)\n        results.append(labels)\n\n    # Paarweise \u00dcbereinstimmung\n    scores = []\n    for i in range(n_runs):\n        for j in range(i+1, n_runs):\n            scores.append(adjusted_rand_score(results[i], results[j]))\n\n    return np.mean(scores)\n\nstability = cluster_stability(X_scaled)\nprint(f\"Cluster-Stabilit\u00e4t: {stability:.2f}\")  # &gt; 0.8 ist gut\n</code></pre>"},{"location":"infoblaetter/cluster-evaluation/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das Wichtigste</p> <ul> <li>Silhouette Score: Intuitiv, zwischen -1 und 1, h\u00f6her ist besser</li> <li>Davies-Bouldin: Niedriger ist besser, misst Cluster-\u00dcberlappung</li> <li>Cluster-Profile: Mittelwerte + Heatmap f\u00fcr Interpretation</li> <li>Metriken allein reichen nicht \u2013 inhaltliche Interpretation ist entscheidend!</li> </ul> Selbstkontrolle <ol> <li>Was bedeutet ein Silhouette Score von 0.3?</li> <li>Wie interpretierst du einen Davies-Bouldin Index von 0.8?</li> <li>Wie erstellst du ein Cluster-Profil?</li> <li>Wann solltest du auf Clustering verzichten?</li> </ol> Antworten <ol> <li>Schwache Cluster-Struktur, Cluster \u00fcberlappen m\u00f6glicherweise</li> <li>Gute Trennung der Cluster (&lt; 1.0 ist gut)</li> <li>Mittelwerte der Features pro Cluster berechnen und z.B. als Heatmap visualisieren</li> <li>Wenn keine nat\u00fcrliche Gruppenstruktur existiert, Labels vorhanden sind, oder exakte Vorhersagen ben\u00f6tigt werden</li> </ol>"},{"location":"infoblaetter/datenvorverarbeitung/","title":"Datenvorverarbeitung f\u00fcr Clustering","text":""},{"location":"infoblaetter/datenvorverarbeitung/#warum-vorverarbeitung","title":"Warum Vorverarbeitung?","text":"<p>Clustering-Algorithmen berechnen Abst\u00e4nde zwischen Datenpunkten. Ohne Vorverarbeitung k\u00f6nnen folgende Probleme auftreten:</p> <ul> <li>Features mit gro\u00dfen Werten dominieren</li> <li>Kategoriale Daten k\u00f6nnen nicht direkt verwendet werden</li> <li>Redundante Features verf\u00e4lschen Ergebnisse</li> </ul>"},{"location":"infoblaetter/datenvorverarbeitung/#skalierung","title":"Skalierung","text":""},{"location":"infoblaetter/datenvorverarbeitung/#das-problem-ohne-skalierung","title":"Das Problem ohne Skalierung","text":"<pre><code># Beispiel: Gehalt (\u20ac) vs. Alter (Jahre)\ngehalt = [50000, 75000, 40000]  # Werte: 40.000 - 75.000\nalter = [25, 45, 30]            # Werte: 25 - 45\n\n# Ohne Skalierung: Gehalt dominiert die Distanz!\n</code></pre> <p>Wichtig</p> <p>K-Means und hierarchisches Clustering sind distanzbasiert. Ein Feature mit Werten von 0-100.000 hat mehr Einfluss als eines mit Werten von 0-10.</p>"},{"location":"infoblaetter/datenvorverarbeitung/#standardscaler-z-score-normalisierung","title":"StandardScaler (Z-Score Normalisierung)","text":"<p>Transformiert Daten zu Mittelwert = 0 und Standardabweichung = 1.</p> <p>$$z = \\frac{x - \\mu}{\\sigma}$$</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Zur\u00fcck-transformieren (falls n\u00f6tig)\nX_original = scaler.inverse_transform(X_scaled)\n</code></pre> <p>Wann verwenden: Standard f\u00fcr die meisten Clustering-Aufgaben.</p>"},{"location":"infoblaetter/datenvorverarbeitung/#minmaxscaler","title":"MinMaxScaler","text":"<p>Skaliert Daten auf einen festen Bereich (meist 0-1).</p> <p>$$x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()  # Default: 0-1\nX_scaled = scaler.fit_transform(X)\n\n# Anderer Bereich\nscaler = MinMaxScaler(feature_range=(-1, 1))\n</code></pre> <p>Wann verwenden: Wenn Werte in bestimmtem Bereich sein m\u00fcssen (z.B. Neuronale Netze).</p>"},{"location":"infoblaetter/datenvorverarbeitung/#vergleich-der-scaler","title":"Vergleich der Scaler","text":"Scaler Mittelwert Bereich Ausrei\u00dfer-Empfindlich StandardScaler 0 unbegrenzt Ja MinMaxScaler variabel 0-1 Sehr RobustScaler Median unbegrenzt Nein <pre><code>from sklearn.preprocessing import RobustScaler\n\n# F\u00fcr Daten mit Ausrei\u00dfern\nscaler = RobustScaler()  # Nutzt Median und IQR\nX_scaled = scaler.fit_transform(X)\n</code></pre>"},{"location":"infoblaetter/datenvorverarbeitung/#umgang-mit-kategorialen-daten","title":"Umgang mit kategorialen Daten","text":"<p>Clustering-Algorithmen brauchen numerische Eingaben.</p>"},{"location":"infoblaetter/datenvorverarbeitung/#one-hot-encoding","title":"One-Hot Encoding","text":"<p>Wandelt kategoriale in bin\u00e4re Spalten um:</p> <pre><code>import pandas as pd\n\ndf = pd.DataFrame({'Farbe': ['rot', 'blau', 'gr\u00fcn', 'rot']})\n\n# Mit pandas\ndf_encoded = pd.get_dummies(df, columns=['Farbe'])\nprint(df_encoded)\n#    Farbe_blau  Farbe_gr\u00fcn  Farbe_rot\n# 0           0           0          1\n# 1           1           0          0\n# 2           0           1          0\n# 3           0           0          1\n</code></pre> <pre><code>from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse_output=False)\nX_encoded = encoder.fit_transform(df[['Farbe']])\n</code></pre> <p>Wann One-Hot Encoding?</p> <ul> <li>Wenige Kategorien (&lt; 10)</li> <li>Keine Ordnung zwischen Kategorien</li> <li>Achtung: Viele Kategorien \u2192 viele neue Spalten!</li> </ul>"},{"location":"infoblaetter/datenvorverarbeitung/#label-encoding","title":"Label Encoding","text":"<p>F\u00fcr ordinale Daten (mit Reihenfolge):</p> <pre><code>from sklearn.preprocessing import LabelEncoder\n\n# F\u00fcr ordinale Daten\ngroesse = ['klein', 'mittel', 'gro\u00df', 'klein']\nencoder = LabelEncoder()\ngroesse_encoded = encoder.fit_transform(groesse)\n# [0, 2, 1, 0]  # Achtung: Alphabetisch!\n\n# Besser: Manuelle Zuordnung\nmapping = {'klein': 0, 'mittel': 1, 'gro\u00df': 2}\ndf['groesse_num'] = df['groesse'].map(mapping)\n</code></pre>"},{"location":"infoblaetter/datenvorverarbeitung/#feature-selection","title":"Feature Selection","text":""},{"location":"infoblaetter/datenvorverarbeitung/#warum-features-reduzieren","title":"Warum Features reduzieren?","text":"<ul> <li>Fluch der Dimensionalit\u00e4t: Bei vielen Features werden Abst\u00e4nde bedeutungslos</li> <li>Rechenzeit: Weniger Features = schnellere Berechnung</li> <li>Interpretierbarkeit: Cluster mit 50 Features schwer zu verstehen</li> </ul>"},{"location":"infoblaetter/datenvorverarbeitung/#korrelationsanalyse","title":"Korrelationsanalyse","text":"<pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Korrelationsmatrix\ncorr = df.corr()\n\n# Heatmap visualisieren\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\nplt.title('Korrelationsmatrix')\nplt.tight_layout()\nplt.show()\n\n# Hoch korrelierte Features finden (&gt;0.8)\nhigh_corr = np.where(np.abs(corr) &gt; 0.8)\nfor i, j in zip(*high_corr):\n    if i != j:\n        print(f\"{corr.columns[i]} &lt;-&gt; {corr.columns[j]}: {corr.iloc[i, j]:.2f}\")\n</code></pre> <p>Faustregel</p> <p>Bei Korrelation &gt; 0.8: Eines der Features entfernen oder PCA nutzen.</p>"},{"location":"infoblaetter/datenvorverarbeitung/#varianz-basierte-selektion","title":"Varianz-basierte Selektion","text":"<p>Features mit sehr geringer Varianz enthalten wenig Information:</p> <pre><code>from sklearn.feature_selection import VarianceThreshold\n\n# Features mit Varianz &lt; 0.1 entfernen\nselector = VarianceThreshold(threshold=0.1)\nX_selected = selector.fit_transform(X_scaled)\n\nprint(f\"Vorher: {X_scaled.shape[1]} Features\")\nprint(f\"Nachher: {X_selected.shape[1]} Features\")\n</code></pre>"},{"location":"infoblaetter/datenvorverarbeitung/#der-komplette-vorverarbeitungs-workflow","title":"Der komplette Vorverarbeitungs-Workflow","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. Daten laden\ndf = pd.read_csv('data.csv')\n\n# 2. Fehlende Werte pr\u00fcfen\nprint(df.isnull().sum())\n\n# 3. Nur numerische Features (oder encodieren)\nX = df.select_dtypes(include=[np.number])\n\n# 4. Korrelationen pr\u00fcfen (optional)\ncorr_matrix = X.corr()\n\n# 5. Skalieren\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Jetzt bereit f\u00fcr Clustering!\n</code></pre>"},{"location":"infoblaetter/datenvorverarbeitung/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"Fehler Problem L\u00f6sung Nicht-numerische Spalten <code>ValueError: could not convert string</code> <code>select_dtypes(include=[np.number])</code> Fehlende Werte <code>NaN</code> besch\u00e4digt Ergebnisse <code>df.dropna()</code> oder <code>df.fillna()</code> ID-Spalten mit skaliert IDs verf\u00e4lschen Clustering IDs vorher entfernen Fit auf Testdaten Data Leakage Nur auf Trainingsdaten fitten"},{"location":"infoblaetter/datenvorverarbeitung/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das Wichtigste</p> <ul> <li>Immer skalieren bei distanzbasierten Methoden</li> <li>StandardScaler ist der Standard f\u00fcr Clustering</li> <li>Kategoriale Daten mit One-Hot Encoding umwandeln</li> <li>Hoch korrelierte Features pr\u00fcfen und ggf. entfernen</li> <li>Fehlende Werte VOR dem Skalieren behandeln</li> </ul> Selbstkontrolle <ol> <li>Warum ist Skalierung f\u00fcr K-Means wichtig?</li> <li>Was ist der Unterschied zwischen StandardScaler und MinMaxScaler?</li> <li>Wie behandelst du kategoriale Daten f\u00fcr Clustering?</li> <li>Was tust du bei hoch korrelierten Features?</li> </ol> Antworten <ol> <li>K-Means ist distanzbasiert \u2013 ohne Skalierung dominieren Features mit gro\u00dfen Werten</li> <li>StandardScaler: Mittelwert=0, Std=1; MinMaxScaler: Werte in festem Bereich (0-1)</li> <li>One-Hot Encoding oder Label Encoding (bei ordinalen Daten)</li> <li>Eines der Features entfernen oder PCA zur Dimensionsreduktion nutzen</li> </ol>"},{"location":"infoblaetter/einfuehrung-unsupervised/","title":"Einf\u00fchrung Unsupervised Learning","text":""},{"location":"infoblaetter/einfuehrung-unsupervised/#was-ist-unsupervised-learning","title":"Was ist Unsupervised Learning?","text":"<p>Unsupervised Learning (un\u00fcberwachtes Lernen) ist ein Bereich des maschinellen Lernens, bei dem Algorithmen Muster in Daten finden \u2013 ohne vorgegebene Labels oder \"richtige Antworten\".</p> <pre><code>flowchart LR\n    subgraph Supervised[\"Supervised Learning\"]\n        A[Eingabe X] --&gt; B[Modell]\n        C[Label Y] --&gt; B\n        B --&gt; D[Vorhersage]\n    end\n\n    subgraph Unsupervised[\"Unsupervised Learning\"]\n        E[Eingabe X] --&gt; F[Modell]\n        F --&gt; G[Muster/Struktur]\n    end</code></pre>"},{"location":"infoblaetter/einfuehrung-unsupervised/#supervised-vs-unsupervised","title":"Supervised vs. Unsupervised","text":"Aspekt Supervised Learning Unsupervised Learning Labels Ja, Daten sind gelabelt Nein, keine Labels Ziel Vorhersage Strukturentdeckung Beispiele Klassifikation, Regression Clustering, Dimensionsreduktion Evaluation Accuracy, RMSE, etc. Silhouette, Elbow Anwendung Spam-Filter, Preisvorhersage Kundensegmentierung, Anomalien <p>Analogie</p> <p>Supervised: Ein Lehrer zeigt dir Bilder von Katzen und Hunden und sagt dir, was was ist. Du lernst, sie zu unterscheiden.</p> <p>Unsupervised: Du bekommst viele Tierbilder ohne Beschriftung und sollst selbst Gruppen finden. Du entdeckst vielleicht: \"Diese Tiere haben Fell, diese Federn, diese Schuppen.\"</p>"},{"location":"infoblaetter/einfuehrung-unsupervised/#hauptaufgaben-im-unsupervised-learning","title":"Hauptaufgaben im Unsupervised Learning","text":""},{"location":"infoblaetter/einfuehrung-unsupervised/#1-clustering","title":"1. Clustering","text":"<p>Ziel: \u00c4hnliche Datenpunkte in Gruppen (Cluster) zusammenfassen.</p> <pre><code>from sklearn.cluster import KMeans\n\n# 3 Cluster finden\nkmeans = KMeans(n_clusters=3, random_state=42)\nlabels = kmeans.fit_predict(X)\n</code></pre> <p>Anwendungsf\u00e4lle: - Kundensegmentierung (Marketing) - Dokumenten-Gruppierung (NLP) - Bildsegmentierung (Computer Vision) - Anomalie-Erkennung (Fraud Detection)</p>"},{"location":"infoblaetter/einfuehrung-unsupervised/#2-dimensionsreduktion","title":"2. Dimensionsreduktion","text":"<p>Ziel: Anzahl der Features reduzieren, ohne wichtige Information zu verlieren.</p> <pre><code>from sklearn.decomposition import PCA\n\n# Auf 2 Dimensionen reduzieren\npca = PCA(n_components=2)\nX_reduced = pca.fit_transform(X)\n</code></pre> <p>Anwendungsf\u00e4lle: - Visualisierung hochdimensionaler Daten - Rauschunterdr\u00fcckung - Feature-Engineering - Beschleunigung anderer Algorithmen</p>"},{"location":"infoblaetter/einfuehrung-unsupervised/#3-assoziationsanalyse","title":"3. Assoziationsanalyse","text":"<p>Ziel: Regeln und Zusammenh\u00e4nge in Transaktionsdaten finden.</p> <p>Beispiel: \"Kunden, die Windeln kaufen, kaufen oft auch Bier.\"</p>"},{"location":"infoblaetter/einfuehrung-unsupervised/#wichtige-algorithmen","title":"Wichtige Algorithmen","text":"Algorithmus Typ Kurzbeschreibung K-Means Clustering Teilt in k Cluster basierend auf Zentroiden Hierarchisches Clustering Clustering Baut Baumstruktur (Dendrogramm) auf DBSCAN Clustering Dichtebasiert, findet Ausrei\u00dfer GMM Clustering Probabilistisch, weiche Zuordnung PCA Dimensionsreduktion Lineare Projektion auf Hauptkomponenten t-SNE / UMAP Dimensionsreduktion Nicht-linear, f\u00fcr Visualisierung"},{"location":"infoblaetter/einfuehrung-unsupervised/#der-typische-workflow","title":"Der typische Workflow","text":"<pre><code>flowchart TD\n    A[1. Daten laden] --&gt; B[2. Exploration &amp; Bereinigung]\n    B --&gt; C[3. Feature-Auswahl]\n    C --&gt; D[4. Skalierung]\n    D --&gt; E[5. Clustering anwenden]\n    E --&gt; F[6. Optimale Clusteranzahl finden]\n    F --&gt; G[7. Ergebnisse visualisieren]\n    G --&gt; H[8. Cluster interpretieren]\n    H --&gt; I[9. Gesch\u00e4ftliche Empfehlungen]</code></pre>"},{"location":"infoblaetter/einfuehrung-unsupervised/#herausforderungen","title":"Herausforderungen","text":"<p>Typische Schwierigkeiten</p> <ol> <li>Keine \"richtige\" L\u00f6sung \u2013 Anders als bei Supervised Learning gibt es keine Labels zur Validierung</li> <li>Anzahl der Cluster w\u00e4hlen \u2013 Oft unklar, wie viele Gruppen sinnvoll sind</li> <li>Interpretation \u2013 Cluster zu finden ist einfacher als sie zu verstehen</li> <li>Skalierung wichtig \u2013 Ohne Normalisierung dominieren Features mit gro\u00dfen Werten</li> <li>Fluch der Dimensionalit\u00e4t \u2013 Bei vielen Features werden Abst\u00e4nde bedeutungslos</li> </ol>"},{"location":"infoblaetter/einfuehrung-unsupervised/#wann-clustering-nutzen","title":"Wann Clustering nutzen?","text":"<p>Gute Anwendungsf\u00e4lle</p> <ul> <li>Es gibt keine Labels, aber du vermutest Gruppen</li> <li>Du m\u00f6chtest Daten explorativ verstehen</li> <li>Segmentierung f\u00fcr personalisiertes Marketing</li> <li>Anomalie-/Ausrei\u00dfer-Erkennung</li> <li>Komprimierung/Vereinfachung von Daten</li> </ul> <p>Schlechte Anwendungsf\u00e4lle</p> <ul> <li>Du hast bereits Labels \u2192 Supervised Learning nutzen</li> <li>Die Daten haben keine nat\u00fcrliche Gruppenstruktur</li> <li>Du brauchst exakte Vorhersagen (z.B. Preise)</li> <li>Die Features sind komplett unkorreliert</li> </ul>"},{"location":"infoblaetter/einfuehrung-unsupervised/#python-bibliotheken","title":"Python-Bibliotheken","text":"<pre><code># Hauptbibliothek f\u00fcr ML\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\n\n# F\u00fcr Dendrogramme\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Visualisierung\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</code></pre>"},{"location":"infoblaetter/einfuehrung-unsupervised/#praxisbeispiel-erste-schritte","title":"Praxisbeispiel: Erste Schritte","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Daten laden\ndf = pd.read_csv('Country-data.csv')\n\n# Nur numerische Features\nX = df.select_dtypes(include=[np.number])\n\n# Skalieren (wichtig!)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# K-Means mit 3 Clustern\nkmeans = KMeans(n_clusters=3, random_state=42)\nlabels = kmeans.fit_predict(X_scaled)\n\n# Ergebnis\ndf['Cluster'] = labels\nprint(df.groupby('Cluster').mean())\n</code></pre>"},{"location":"infoblaetter/einfuehrung-unsupervised/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das Wichtigste</p> <ul> <li>Unsupervised Learning findet Muster ohne Labels</li> <li>Clustering gruppiert \u00e4hnliche Datenpunkte</li> <li>Dimensionsreduktion komprimiert Features</li> <li>Skalierung ist essentiell f\u00fcr distanzbasierte Methoden</li> <li>Die Interpretation ist oft die gr\u00f6\u00dfte Herausforderung</li> </ul> Selbstkontrolle <ol> <li>Was ist der Hauptunterschied zwischen Supervised und Unsupervised Learning?</li> <li>Nenne drei Anwendungsf\u00e4lle f\u00fcr Clustering.</li> <li>Warum ist Skalierung bei Clustering wichtig?</li> <li>Welche Herausforderung gibt es bei der Evaluation von Clustering?</li> </ol> Antworten <ol> <li>Supervised Learning nutzt Labels, Unsupervised Learning nicht</li> <li>z.B. Kundensegmentierung, Anomalie-Erkennung, Bildsegmentierung</li> <li>Ohne Skalierung dominieren Features mit gro\u00dfen Werten die Distanzberechnung</li> <li>Es gibt keine \"richtigen\" Labels zur Validierung \u2013 Evaluation ist subjektiver</li> </ol>"},{"location":"infoblaetter/grosse-datenmengen/","title":"Gro\u00dfe Datenmengen verarbeiten","text":""},{"location":"infoblaetter/grosse-datenmengen/#das-skalierungsproblem","title":"Das Skalierungsproblem","text":"<p>Standard-Algorithmen sto\u00dfen bei gro\u00dfen Datenmengen an Grenzen:</p> Datensatz K-Means Hierarchisches Clustering DBSCAN 1.000 \u2713 Schnell \u2713 Schnell \u2713 Schnell 100.000 \u2713 OK \u26a0 Langsam \u26a0 Langsam 1.000.000 \u26a0 Langsam \u2717 Unm\u00f6glich \u2717 Speicherprobleme 10.000.000 \u2717 Speicherprobleme \u2717 Unm\u00f6glich \u2717 Unm\u00f6glich"},{"location":"infoblaetter/grosse-datenmengen/#strategien-fur-groe-daten","title":"Strategien f\u00fcr gro\u00dfe Daten","text":""},{"location":"infoblaetter/grosse-datenmengen/#1-sampling","title":"1. Sampling","text":"<p>Arbeite mit einer repr\u00e4sentativen Stichprobe:</p> <pre><code># Zuf\u00e4llige Stichprobe\nsample = df.sample(n=10000, random_state=42)\n\n# Stratifiziertes Sampling (wenn Kategorien vorhanden)\nfrom sklearn.model_selection import train_test_split\nsample, _ = train_test_split(df, train_size=0.1, stratify=df['category'], random_state=42)\n</code></pre> <p>Sample-Gr\u00f6\u00dfe w\u00e4hlen</p> <ul> <li>Minimum: 10.000 Punkte oder 10x Anzahl Features</li> <li>Mehr Cluster = mehr Samples n\u00f6tig</li> <li>Pr\u00fcfe: Sind alle Cluster im Sample vertreten?</li> </ul>"},{"location":"infoblaetter/grosse-datenmengen/#2-mini-batch-k-means","title":"2. Mini-Batch K-Means","text":"<p>Verarbeitet Daten in kleinen Batches:</p> <pre><code>from sklearn.cluster import MiniBatchKMeans\n\nmbkmeans = MiniBatchKMeans(\n    n_clusters=5,\n    batch_size=1024,      # Punkte pro Batch\n    max_iter=100,\n    random_state=42\n)\n\nlabels = mbkmeans.fit_predict(X_scaled)\n</code></pre> <p>Vorteile: - 10-100x schneller als Standard K-Means - Konstanter Speicherverbrauch - F\u00fcr Streaming-Daten geeignet</p> <p>Nachteile: - Leicht schlechtere Ergebnisse - Ergebnisse weniger stabil</p>"},{"location":"infoblaetter/grosse-datenmengen/#3-daten-in-chunks-laden","title":"3. Daten in Chunks laden","text":"<pre><code>import pandas as pd\n\n# CSV in Chunks lesen\nchunks = pd.read_csv('large_file.csv', chunksize=100000)\n\n# Mini-Batch K-Means mit Partial Fit\nfrom sklearn.cluster import MiniBatchKMeans\nmbkmeans = MiniBatchKMeans(n_clusters=5, random_state=42)\n\nfor i, chunk in enumerate(chunks):\n    X_chunk = preprocess(chunk)  # Deine Vorverarbeitung\n    mbkmeans.partial_fit(X_chunk)\n    print(f\"Batch {i+1} verarbeitet\")\n\n# Danach: Labels f\u00fcr alle Daten vorhersagen\nchunks = pd.read_csv('large_file.csv', chunksize=100000)\nall_labels = []\nfor chunk in chunks:\n    X_chunk = preprocess(chunk)\n    labels = mbkmeans.predict(X_chunk)\n    all_labels.extend(labels)\n</code></pre>"},{"location":"infoblaetter/grosse-datenmengen/#4-dimensionsreduktion-zuerst","title":"4. Dimensionsreduktion zuerst","text":"<p>Reduziere Features vor dem Clustering:</p> <pre><code>from sklearn.decomposition import IncrementalPCA\n\n# Inkrementelle PCA f\u00fcr gro\u00dfe Daten\nipca = IncrementalPCA(n_components=10, batch_size=1000)\n\n# Fit in Batches\nfor chunk in pd.read_csv('large_file.csv', chunksize=100000):\n    X_chunk = preprocess(chunk)\n    ipca.partial_fit(X_chunk)\n\n# Transformiere und clustere\nX_reduced = ipca.transform(X_scaled)\n</code></pre>"},{"location":"infoblaetter/grosse-datenmengen/#optimierte-algorithmen","title":"Optimierte Algorithmen","text":""},{"location":"infoblaetter/grosse-datenmengen/#hdbscan-statt-dbscan","title":"HDBSCAN statt DBSCAN","text":"<pre><code>import hdbscan\n\nclusterer = hdbscan.HDBSCAN(\n    min_cluster_size=100,\n    min_samples=10,\n    core_dist_n_jobs=-1  # Nutze alle CPU-Kerne\n)\nlabels = clusterer.fit_predict(X_scaled)\n</code></pre> <p>Vorteile gegen\u00fcber DBSCAN: - Automatische Epsilon-Wahl - Hierarchisches Ergebnis - Besser f\u00fcr unterschiedliche Dichten</p>"},{"location":"infoblaetter/grosse-datenmengen/#balltree-fur-nachbarschaftssuchen","title":"BallTree f\u00fcr Nachbarschaftssuchen","text":"<pre><code>from sklearn.neighbors import BallTree\n\n# BallTree vorberechnen\ntree = BallTree(X_scaled, leaf_size=40)\n\n# Schnelle Nachbarn-Suche\ndistances, indices = tree.query(X_query, k=5)\n</code></pre>"},{"location":"infoblaetter/grosse-datenmengen/#speicheroptimierung","title":"Speicheroptimierung","text":""},{"location":"infoblaetter/grosse-datenmengen/#datentypen-reduzieren","title":"Datentypen reduzieren","text":"<pre><code>def optimize_dtypes(df):\n    \"\"\"Reduziere Speicherverbrauch durch optimierte Datentypen.\"\"\"\n    for col in df.select_dtypes(include=['float64']).columns:\n        df[col] = df[col].astype('float32')\n\n    for col in df.select_dtypes(include=['int64']).columns:\n        if df[col].min() &gt;= 0:\n            if df[col].max() &lt; 255:\n                df[col] = df[col].astype('uint8')\n            elif df[col].max() &lt; 65535:\n                df[col] = df[col].astype('uint16')\n        else:\n            if df[col].min() &gt; -128 and df[col].max() &lt; 128:\n                df[col] = df[col].astype('int8')\n\n    return df\n\nprint(f\"Vorher: {df.memory_usage().sum() / 1e6:.1f} MB\")\ndf = optimize_dtypes(df)\nprint(f\"Nachher: {df.memory_usage().sum() / 1e6:.1f} MB\")\n</code></pre>"},{"location":"infoblaetter/grosse-datenmengen/#sparse-matrices","title":"Sparse Matrices","text":"<p>F\u00fcr Daten mit vielen Nullwerten (z.B. One-Hot Encoding):</p> <pre><code>from scipy import sparse\n\n# Dense zu Sparse\nX_sparse = sparse.csr_matrix(X)\n\n# K-Means kann mit Sparse arbeiten\nkmeans = KMeans(n_clusters=5)\nkmeans.fit(X_sparse)\n</code></pre>"},{"location":"infoblaetter/grosse-datenmengen/#parallelisierung","title":"Parallelisierung","text":""},{"location":"infoblaetter/grosse-datenmengen/#scikit-learn-parallelisierung","title":"Scikit-learn Parallelisierung","text":"<pre><code>from sklearn.cluster import KMeans\n\n# n_jobs=-1 nutzt alle CPU-Kerne\nkmeans = KMeans(n_clusters=5, n_init=10, n_jobs=-1)\n</code></pre>"},{"location":"infoblaetter/grosse-datenmengen/#joblib-fur-eigene-parallelisierung","title":"Joblib f\u00fcr eigene Parallelisierung","text":"<pre><code>from joblib import Parallel, delayed\n\ndef fit_single_k(X, k):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X)\n    return k, kmeans.inertia_, silhouette_score(X, kmeans.labels_)\n\n# Parallele Suche nach optimalem k\nresults = Parallel(n_jobs=-1)(\n    delayed(fit_single_k)(X_scaled, k) \n    for k in range(2, 20)\n)\n</code></pre>"},{"location":"infoblaetter/grosse-datenmengen/#workflow-fur-groe-datenmengen","title":"Workflow f\u00fcr gro\u00dfe Datenmengen","text":"<pre><code>flowchart TD\n    A[Gro\u00dfe Daten] --&gt; B{&gt; 1 Mio Zeilen?}\n    B --&gt;|Ja| C[Sampling f\u00fcr Exploration]\n    B --&gt;|Nein| D[Dimensionsreduktion]\n    C --&gt; D\n    D --&gt; E{&gt; 100.000 Zeilen?}\n    E --&gt;|Ja| F[Mini-Batch K-Means]\n    E --&gt;|Nein| G[Standard Algorithmen]\n    F --&gt; H[Ergebnisse evaluieren]\n    G --&gt; H\n    H --&gt; I{Gut genug?}\n    I --&gt;|Nein| J[Parameter anpassen]\n    J --&gt; F\n    I --&gt;|Ja| K[Auf Gesamtdaten anwenden]</code></pre>"},{"location":"infoblaetter/grosse-datenmengen/#beispiel-us-accidents-datensatz","title":"Beispiel: US-Accidents Datensatz","text":"<pre><code>import pandas as pd\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. Sample zum Explorieren\nsample = pd.read_csv('us_accidents.csv', nrows=100000)\n\n# 2. Features ausw\u00e4hlen und skalieren\nfeatures = ['Temperature', 'Humidity', 'Visibility']\nX = sample[features].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 3. Optimales k mit Sample finden\nfrom sklearn.metrics import silhouette_score\n\nscores = []\nfor k in range(2, 10):\n    mbkmeans = MiniBatchKMeans(n_clusters=k, batch_size=1000, random_state=42)\n    labels = mbkmeans.fit_predict(X_scaled)\n    scores.append((k, silhouette_score(X_scaled, labels)))\n\nbest_k = max(scores, key=lambda x: x[1])[0]\nprint(f\"Bestes k: {best_k}\")\n\n# 4. Finales Modell auf Gesamtdaten trainieren\nfinal_model = MiniBatchKMeans(n_clusters=best_k, batch_size=1024, random_state=42)\n\nfor chunk in pd.read_csv('us_accidents.csv', chunksize=50000):\n    X_chunk = chunk[features].dropna()\n    X_chunk_scaled = scaler.transform(X_chunk)\n    final_model.partial_fit(X_chunk_scaled)\n</code></pre>"},{"location":"infoblaetter/grosse-datenmengen/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das Wichtigste</p> <ul> <li>Sampling: Nutze Stichproben f\u00fcr Exploration und Parametersuche</li> <li>Mini-Batch K-Means: 10-100x schneller, konstanter Speicher</li> <li>Chunk-Verarbeitung: <code>partial_fit()</code> f\u00fcr Daten gr\u00f6\u00dfer als RAM</li> <li>Dimensionsreduktion: Erst PCA, dann Clustering</li> <li>Datentypen optimieren: float64 \u2192 float32 spart 50% Speicher</li> </ul> Selbstkontrolle <ol> <li>Ab welcher Datengr\u00f6\u00dfe wird Mini-Batch K-Means empfohlen?</li> <li>Wie funktioniert Chunk-Verarbeitung in Python?</li> <li>Wie viel Speicher spart die Umstellung von float64 auf float32?</li> <li>Was ist der Vorteil von HDBSCAN gegen\u00fcber DBSCAN?</li> </ol> Antworten <ol> <li>Ab ca. 100.000 Datenpunkten empfohlen, ab 1 Million notwendig</li> <li><code>pd.read_csv(..., chunksize=N)</code> + <code>model.partial_fit()</code> f\u00fcr jeden Chunk</li> <li>50% (8 Bytes \u2192 4 Bytes pro Wert)</li> <li>Automatische Epsilon-Wahl, hierarchisches Ergebnis, besser f\u00fcr unterschiedliche Dichten</li> </ol>"},{"location":"infoblaetter/hierarchisches-clustering/","title":"Hierarchisches Clustering","text":""},{"location":"infoblaetter/hierarchisches-clustering/#grundidee","title":"Grundidee","text":"<p>Hierarchisches Clustering baut eine Baumstruktur (Dendrogramm) auf, die zeigt, wie Datenpunkte schrittweise zu Clustern zusammengefasst werden.</p> <p></p> <p>Vorteil gegen\u00fcber K-Means: Du musst die Clusteranzahl nicht vorher festlegen!</p>"},{"location":"infoblaetter/hierarchisches-clustering/#zwei-ansatze","title":"Zwei Ans\u00e4tze","text":""},{"location":"infoblaetter/hierarchisches-clustering/#agglomerativ-bottom-up","title":"Agglomerativ (Bottom-Up)","text":"<ol> <li>Jeder Punkt ist ein eigenes Cluster</li> <li>Verbinde die zwei \u00e4hnlichsten Cluster</li> <li>Wiederhole bis nur noch 1 Cluster \u00fcbrig</li> </ol> <pre><code>flowchart BT\n    A[A] --&gt; AB[A+B]\n    B[B] --&gt; AB\n    C[C] --&gt; CD[C+D]\n    D[D] --&gt; CD\n    AB --&gt; ABCD[A+B+C+D]\n    CD --&gt; ABCD</code></pre>"},{"location":"infoblaetter/hierarchisches-clustering/#divisiv-top-down","title":"Divisiv (Top-Down)","text":"<ol> <li>Alle Punkte in einem Cluster</li> <li>Teile rekursiv in kleinere Cluster</li> <li>Wiederhole bis gew\u00fcnschte Anzahl erreicht</li> </ol> <p>In der Praxis</p> <p>Agglomerativ ist viel h\u00e4ufiger \u2013 einfacher zu implementieren und zu berechnen.</p>"},{"location":"infoblaetter/hierarchisches-clustering/#linkage-methoden","title":"Linkage-Methoden","text":"<p>Die Linkage-Methode bestimmt, wie der Abstand zwischen Clustern gemessen wird.</p> Methode Beschreibung Eigenschaften <code>single</code> Minimaler Abstand Kann lange, d\u00fcnne Cluster bilden <code>complete</code> Maximaler Abstand Kompakte, \u00e4hnlich gro\u00dfe Cluster <code>average</code> Durchschnittlicher Abstand Ausgewogen <code>ward</code> Minimiert Varianz-Zunahme Empfohlen f\u00fcr die meisten F\u00e4lle <pre><code>from scipy.cluster.hierarchy import linkage\n\n# Verschiedene Linkage-Methoden\nZ_ward = linkage(X_scaled, method='ward')\nZ_complete = linkage(X_scaled, method='complete')\nZ_average = linkage(X_scaled, method='average')\nZ_single = linkage(X_scaled, method='single')\n</code></pre> <p>Best Practice</p> <p>Starte mit Ward-Linkage \u2013 sie liefert oft die besten Ergebnisse und ist am robustesten.</p>"},{"location":"infoblaetter/hierarchisches-clustering/#dendrogramm-erstellen","title":"Dendrogramm erstellen","text":"<pre><code>from scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Linkage berechnen\nZ = linkage(X_scaled, method='ward')\n\n# Dendrogramm plotten\nplt.figure(figsize=(12, 8))\ndendrogram(Z, \n           truncate_mode='lastp',  # Nur letzte p Cluster zeigen\n           p=30,                   # Anzahl der gezeigten Cluster\n           leaf_rotation=90,\n           leaf_font_size=10)\nplt.xlabel('Datenpunkte')\nplt.ylabel('Distanz')\nplt.title('Dendrogramm (Ward-Linkage)')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"infoblaetter/hierarchisches-clustering/#dendrogramm-interpretieren","title":"Dendrogramm interpretieren","text":"<ul> <li>Y-Achse: Abstand/Distanz bei der Verschmelzung</li> <li>Horizontale Linien: Zeigen, welche Cluster verschmolzen werden</li> <li>H\u00f6he der Verschmelzung: Je h\u00f6her, desto un\u00e4hnlicher die Cluster</li> </ul>"},{"location":"infoblaetter/hierarchisches-clustering/#cluster-aus-dendrogramm-ableiten","title":"Cluster aus Dendrogramm ableiten","text":""},{"location":"infoblaetter/hierarchisches-clustering/#methode-1-schnitt-bei-bestimmter-hohe","title":"Methode 1: Schnitt bei bestimmter H\u00f6he","text":"<pre><code>from scipy.cluster.hierarchy import fcluster\n\n# Schnitt bei H\u00f6he t\nlabels = fcluster(Z, t=10, criterion='distance')\nprint(f\"Anzahl Cluster: {len(set(labels))}\")\n</code></pre>"},{"location":"infoblaetter/hierarchisches-clustering/#methode-2-gewunschte-clusteranzahl","title":"Methode 2: Gew\u00fcnschte Clusteranzahl","text":"<pre><code># Genau 3 Cluster\nlabels = fcluster(Z, t=3, criterion='maxclust')\n</code></pre>"},{"location":"infoblaetter/hierarchisches-clustering/#visualisierung-mit-schnittlinie","title":"Visualisierung mit Schnittlinie","text":"<pre><code>plt.figure(figsize=(12, 8))\ndendrogram(Z, truncate_mode='lastp', p=30)\nplt.axhline(y=10, color='r', linestyle='--', label='Schnitt bei t=10')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"infoblaetter/hierarchisches-clustering/#mit-scikit-learn","title":"Mit scikit-learn","text":"<pre><code>from sklearn.cluster import AgglomerativeClustering\n\n# Agglomeratives Clustering\nagg = AgglomerativeClustering(\n    n_clusters=3,        # Anzahl Cluster\n    linkage='ward'       # Linkage-Methode\n)\nlabels = agg.fit_predict(X_scaled)\n\n# Ohne vorgegebene Clusteranzahl (mit distance_threshold)\nagg_auto = AgglomerativeClustering(\n    n_clusters=None,\n    distance_threshold=10,  # Schnitt bei dieser Distanz\n    linkage='ward'\n)\nlabels_auto = agg_auto.fit_predict(X_scaled)\n</code></pre>"},{"location":"infoblaetter/hierarchisches-clustering/#vergleich-hierarchisch-vs-k-means","title":"Vergleich: Hierarchisch vs. K-Means","text":"Aspekt K-Means Hierarchisch Clusteranzahl Muss vorher gew\u00e4hlt werden Kann nachtr\u00e4glich bestimmt werden Visualisierung Kein nat\u00fcrliches Diagramm Dendrogramm zeigt Struktur Geschwindigkeit Schnell O(n\u00b7k\u00b7i) Langsam O(n\u00b3) oder O(n\u00b2log n) Cluster-Form Nur kugelf\u00f6rmig Flexibler Gro\u00dfe Daten Gut geeignet Problematisch (Speicher/Zeit) Reproduzierbarkeit Abh\u00e4ngig von Initialisierung Deterministisch"},{"location":"infoblaetter/hierarchisches-clustering/#praktisches-beispiel","title":"Praktisches Beispiel","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nimport matplotlib.pyplot as plt\n\n# Daten laden und vorbereiten\ndf = pd.read_csv('Country-data.csv')\nX = df.select_dtypes(include=[np.number])\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Linkage berechnen\nZ = linkage(X_scaled, method='ward')\n\n# Dendrogramm mit L\u00e4ndernamen\nplt.figure(figsize=(15, 10))\ndendrogram(Z, \n           labels=df['country'].values,\n           leaf_rotation=90,\n           leaf_font_size=8)\nplt.ylabel('Distanz')\nplt.title('L\u00e4nder-Clustering (Hierarchisch)')\nplt.tight_layout()\nplt.show()\n\n# 4 Cluster extrahieren\ndf['Cluster'] = fcluster(Z, t=4, criterion='maxclust')\nprint(df.groupby('Cluster')['country'].apply(list))\n</code></pre>"},{"location":"infoblaetter/hierarchisches-clustering/#wann-hierarchisches-clustering-nutzen","title":"Wann Hierarchisches Clustering nutzen?","text":"<p>Gut geeignet f\u00fcr</p> <ul> <li>Explorative Analyse (Struktur entdecken)</li> <li>Kleine bis mittlere Datens\u00e4tze (&lt; 10.000)</li> <li>Wenn Clusteranzahl unklar ist</li> <li>Visualisierung der Cluster-Hierarchie ist wichtig</li> <li>Taxonomien erstellen (z.B. Biologie)</li> </ul> <p>Weniger geeignet f\u00fcr</p> <ul> <li>Sehr gro\u00dfe Datens\u00e4tze (Speicher/Zeit)</li> <li>Echtzeit-Anwendungen</li> <li>Wenn Effizienz kritisch ist</li> </ul>"},{"location":"infoblaetter/hierarchisches-clustering/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das Wichtigste</p> <ul> <li>Hierarchisches Clustering baut eine Baumstruktur (Dendrogramm) auf</li> <li>Agglomerativ (bottom-up) ist der Standard</li> <li>Ward-Linkage liefert meist die besten Ergebnisse</li> <li>Clusteranzahl kann nachtr\u00e4glich aus dem Dendrogramm abgelesen werden</li> <li>Langsamer als K-Means, aber flexibler</li> </ul> Selbstkontrolle <ol> <li>Was ist der Unterschied zwischen agglomerativem und divisivem Clustering?</li> <li>Welche Linkage-Methode ist f\u00fcr den Start empfohlen?</li> <li>Wie ermittelst du die Clusteranzahl aus einem Dendrogramm?</li> <li>Wann ist hierarchisches Clustering K-Means vorzuziehen?</li> </ol> Antworten <ol> <li>Agglomerativ: Bottom-up (Punkte \u2192 Cluster), Divisiv: Top-down (ein Cluster \u2192 viele)</li> <li>Ward-Linkage \u2013 minimiert die Varianz-Zunahme</li> <li>Horizontale Linie bei gew\u00fcnschter H\u00f6he ziehen, Anzahl der Schnittpunkte = Clusteranzahl</li> <li>Wenn Clusteranzahl unklar ist, Visualisierung wichtig ist, oder Datensatz klein ist</li> </ol>"},{"location":"infoblaetter/kmeans-clustering/","title":"K-Means Clustering","text":""},{"location":"infoblaetter/kmeans-clustering/#der-algorithmus","title":"Der Algorithmus","text":"<p>K-Means ist der Standard-Algorithmus f\u00fcr Clustering \u2013 einfach, schnell und oft ausreichend gut.</p>"},{"location":"infoblaetter/kmeans-clustering/#grundidee","title":"Grundidee","text":"<p>K-Means teilt n Datenpunkte in k Cluster, wobei jeder Punkt zum Cluster mit dem n\u00e4chsten Zentroid geh\u00f6rt.</p>"},{"location":"infoblaetter/kmeans-clustering/#schritt-fur-schritt","title":"Schritt f\u00fcr Schritt","text":"<pre><code>flowchart TD\n    A[1. W\u00e4hle k Startpunkte&lt;br/&gt;zuf\u00e4llig] --&gt; B[2. Weise jeden Punkt&lt;br/&gt;zum n\u00e4chsten Zentroid zu]\n    B --&gt; C[3. Berechne neue&lt;br/&gt;Zentroide als Mittelwert]\n    C --&gt; D{Zentroide&lt;br/&gt;ver\u00e4ndert?}\n    D --&gt;|Ja| B\n    D --&gt;|Nein| E[Fertig!]</code></pre> <ol> <li>Initialisierung: W\u00e4hle k zuf\u00e4llige Punkte als Startzentroids</li> <li>Zuweisung: Weise jeden Datenpunkt dem n\u00e4chsten Zentroid zu</li> <li>Update: Berechne neue Zentroids als Mittelwert der zugewiesenen Punkte</li> <li>Wiederhole Schritte 2-3 bis Konvergenz</li> </ol> <p>Interaktive Visualisierung</p> <p>Probiere den Algorithmus interaktiv aus: K-Means Visualisierung von Naftali Harris</p>"},{"location":"infoblaetter/kmeans-clustering/#code-beispiel","title":"Code-Beispiel","text":"<pre><code>from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Daten vorbereiten\ndf = pd.read_csv('Country-data.csv')\nX = df.select_dtypes(include=['number'])\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# K-Means mit 3 Clustern\nkmeans = KMeans(n_clusters=3, random_state=42)\nlabels = kmeans.fit_predict(X_scaled)\n\n# Labels dem DataFrame hinzuf\u00fcgen\ndf['Cluster'] = labels\n\n# Cluster-Zentroide\nprint(kmeans.cluster_centers_)\n\n# Inertia (Within-Cluster Sum of Squares)\nprint(f\"Inertia: {kmeans.inertia_:.2f}\")\n</code></pre>"},{"location":"infoblaetter/kmeans-clustering/#die-elbow-methode","title":"Die Elbow-Methode","text":"<p>Problem: Wie viele Cluster (k) sind optimal?</p> <p>L\u00f6sung: Die Elbow-Methode findet den Punkt, ab dem mehr Cluster kaum noch Verbesserung bringen.</p> <pre><code>import matplotlib.pyplot as plt\n\ninertias = []\nK_range = range(1, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n\n# Elbow-Plot\nplt.figure(figsize=(10, 6))\nplt.plot(K_range, inertias, 'bo-')\nplt.xlabel('Anzahl Cluster (k)')\nplt.ylabel('Inertia (WCSS)')\nplt.title('Elbow-Methode')\nplt.xticks(K_range)\nplt.grid(True)\nplt.show()\n</code></pre> <p>Den Ellbogen finden</p> <p>Der optimale k-Wert liegt dort, wo die Kurve einen Knick macht \u2013 wie ein Ellbogen. Danach flacht die Verbesserung ab.</p>"},{"location":"infoblaetter/kmeans-clustering/#silhouette-score","title":"Silhouette Score","text":"<p>Der Silhouette Score misst, wie gut Punkte zu ihrem eigenen Cluster passen vs. zu anderen Clustern.</p> <p>$$s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$$</p> <ul> <li>a(i): Mittlere Distanz zu Punkten im eigenen Cluster</li> <li>b(i): Mittlere Distanz zum n\u00e4chsten fremden Cluster</li> </ul> Wert Interpretation 1.0 Perfekt getrennt 0.5 - 1.0 Gute Trennung 0.25 - 0.5 Schwache Trennung &lt; 0.25 Cluster \u00fcberlappen &lt; 0 Falsche Clusterzuordnung <pre><code>from sklearn.metrics import silhouette_score\n\n# Silhouette Score f\u00fcr verschiedene k\nfor k in range(2, 8):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(X_scaled)\n    score = silhouette_score(X_scaled, labels)\n    print(f\"k={k}: Silhouette Score = {score:.3f}\")\n</code></pre>"},{"location":"infoblaetter/kmeans-clustering/#initialisierung-k-means","title":"Initialisierung: k-means++","text":"<p>Das Problem mit zuf\u00e4lliger Initialisierung: Schlechte Startpunkte f\u00fchren zu schlechten Ergebnissen.</p> <p>k-means++ w\u00e4hlt Startpunkte intelligent: 1. Erster Zentroid zuf\u00e4llig 2. N\u00e4chster Zentroid: weit weg von existierenden (gewichtete Wahrscheinlichkeit) 3. Wiederhole bis k Zentroids gew\u00e4hlt</p> <pre><code># k-means++ ist der Default in scikit-learn\nkmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n\n# Alternative: zuf\u00e4llig\nkmeans_random = KMeans(n_clusters=3, init='random', random_state=42)\n</code></pre> <p>Best Practice</p> <p>Immer <code>random_state</code> setzen f\u00fcr reproduzierbare Ergebnisse!</p>"},{"location":"infoblaetter/kmeans-clustering/#wichtige-parameter","title":"Wichtige Parameter","text":"Parameter Default Beschreibung <code>n_clusters</code> 8 Anzahl Cluster <code>init</code> 'k-means++' Initialisierungsmethode <code>n_init</code> 10 Anzahl Durchl\u00e4ufe mit verschiedenen Seeds <code>max_iter</code> 300 Maximale Iterationen <code>random_state</code> None Seed f\u00fcr Reproduzierbarkeit <pre><code>kmeans = KMeans(\n    n_clusters=5,\n    init='k-means++',\n    n_init=10,        # 10 Durchl\u00e4ufe, bestes Ergebnis nehmen\n    max_iter=300,\n    random_state=42\n)\n</code></pre>"},{"location":"infoblaetter/kmeans-clustering/#vor-und-nachteile","title":"Vor- und Nachteile","text":"Vorteile Nachteile \u2705 Einfach zu verstehen \u274c k muss vorher gew\u00e4hlt werden \u2705 Schnell (O(n\u00b7k\u00b7i)) \u274c Nur kugelf\u00f6rmige Cluster \u2705 Skaliert gut \u274c Empfindlich gegen Ausrei\u00dfer \u2705 Garantiert Konvergenz \u274c Findet nur lokales Optimum"},{"location":"infoblaetter/kmeans-clustering/#wann-k-means-nicht-verwenden","title":"Wann K-Means (nicht) verwenden","text":"<p>K-Means ist gut f\u00fcr</p> <ul> <li>Kugelf\u00f6rmige, \u00e4hnlich gro\u00dfe Cluster</li> <li>Erste Exploration der Daten</li> <li>Gro\u00dfe Datens\u00e4tze (schnell!)</li> <li>Klare Trennung zwischen Gruppen</li> </ul> <p>K-Means ist schlecht f\u00fcr</p> <ul> <li>Nicht-kugelf\u00f6rmige Cluster (Halbmonde, Ringe)</li> <li>Cluster sehr unterschiedlicher Gr\u00f6\u00dfe</li> <li>Daten mit vielen Ausrei\u00dfern</li> <li>Wenn k unbekannt und schwer zu sch\u00e4tzen</li> </ul>"},{"location":"infoblaetter/kmeans-clustering/#visualisierung-der-ergebnisse","title":"Visualisierung der Ergebnisse","text":"<pre><code>import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Bei mehr als 2 Features: PCA f\u00fcr Visualisierung\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Scatter-Plot mit Cluster-Farben\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n                      c=labels, cmap='viridis', alpha=0.6)\nplt.colorbar(scatter, label='Cluster')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('K-Means Clustering (PCA-Visualisierung)')\nplt.show()\n</code></pre>"},{"location":"infoblaetter/kmeans-clustering/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das Wichtigste</p> <ul> <li>K-Means teilt Daten in k Cluster basierend auf Zentroiden</li> <li>Elbow-Methode hilft bei der Wahl von k</li> <li>Silhouette Score bewertet die Clustering-Qualit\u00e4t</li> <li>Immer <code>random_state</code> setzen f\u00fcr Reproduzierbarkeit</li> <li>Daten m\u00fcssen vorher skaliert werden!</li> </ul> Selbstkontrolle <ol> <li>Was macht K-Means in jedem Iterationsschritt?</li> <li>Was zeigt die Elbow-Methode?</li> <li>Was bedeutet ein Silhouette Score von 0.7?</li> <li>Warum ist k-means++ besser als zuf\u00e4llige Initialisierung?</li> </ol> Antworten <ol> <li>Punkte dem n\u00e4chsten Zentroid zuweisen, dann Zentroide als Mittelwert neu berechnen</li> <li>Wie sich die Inertia (Kompaktheit) mit steigender Clusteranzahl ver\u00e4ndert</li> <li>Gute Trennung der Cluster \u2013 Punkte sind n\u00e4her am eigenen als an fremden Clustern</li> <li>Bessere Startpunkte f\u00fchren zu besseren und konsistenteren Ergebnissen</li> </ol>"},{"location":"infoblaetter/pca-dimensionsreduktion/","title":"PCA (Dimensionsreduktion)","text":""},{"location":"infoblaetter/pca-dimensionsreduktion/#was-ist-pca","title":"Was ist PCA?","text":"<p>PCA (Principal Component Analysis / Hauptkomponentenanalyse) reduziert die Anzahl der Features, w\u00e4hrend m\u00f6glichst viel Information erhalten bleibt.</p> <p>Hauptanwendungen: - Visualisierung hochdimensionaler Daten (z.B. 50D \u2192 2D) - Rauschunterdr\u00fcckung - Beschleunigung anderer Algorithmen - Feature-Engineering</p>"},{"location":"infoblaetter/pca-dimensionsreduktion/#die-grundidee","title":"Die Grundidee","text":"<p>PCA findet neue Achsen (Hauptkomponenten), die: 1. Orthogonal zueinander stehen 2. Die Varianz der Daten maximieren</p> <pre><code>flowchart LR\n    A[Originaldaten&lt;br/&gt;z.B. 9 Features] --&gt; B[PCA]\n    B --&gt; C[Reduzierte Daten&lt;br/&gt;z.B. 2 Features]\n    B --&gt; D[Erkl\u00e4rte Varianz&lt;br/&gt;z.B. 85%]</code></pre> <p>Analogie</p> <p>Stell dir vor, du fotografierst eine 3D-Skulptur. Der beste Winkel zeigt die meiste Information auf einem 2D-Bild. PCA findet automatisch diesen \"besten Winkel\".</p>"},{"location":"infoblaetter/pca-dimensionsreduktion/#code-beispiel","title":"Code-Beispiel","text":"<pre><code>from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# 1. Daten skalieren (wichtig f\u00fcr PCA!)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 2. PCA anwenden\npca = PCA(n_components=2)  # Auf 2 Dimensionen reduzieren\nX_pca = pca.fit_transform(X_scaled)\n\n# 3. Ergebnis visualisieren\nplt.figure(figsize=(10, 8))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Daten nach PCA (2D)')\nplt.show()\n</code></pre>"},{"location":"infoblaetter/pca-dimensionsreduktion/#erklarte-varianz","title":"Erkl\u00e4rte Varianz","text":"<p>Die erkl\u00e4rte Varianz zeigt, wie viel Information jede Komponente enth\u00e4lt.</p> <pre><code># PCA mit allen Komponenten\npca_full = PCA()\npca_full.fit(X_scaled)\n\n# Erkl\u00e4rte Varianz pro Komponente\nprint(\"Varianz pro Komponente:\")\nprint(pca_full.explained_variance_ratio_)\n\n# Kumulierte Varianz\ncumsum = np.cumsum(pca_full.explained_variance_ratio_)\nprint(f\"\\nKumuliert: {cumsum}\")\n</code></pre>"},{"location":"infoblaetter/pca-dimensionsreduktion/#scree-plot-varianz-visualisieren","title":"Scree-Plot (Varianz visualisieren)","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# Erkl\u00e4rte Varianz plotten\nplt.figure(figsize=(10, 6))\n\n# Balken f\u00fcr einzelne Komponenten\nplt.bar(range(1, len(pca_full.explained_variance_ratio_) + 1),\n        pca_full.explained_variance_ratio_, alpha=0.7, label='Einzeln')\n\n# Linie f\u00fcr kumulierte Varianz\nplt.plot(range(1, len(cumsum) + 1), cumsum, 'ro-', label='Kumuliert')\n\nplt.xlabel('Hauptkomponente')\nplt.ylabel('Erkl\u00e4rte Varianz')\nplt.title('Scree-Plot')\nplt.axhline(y=0.95, color='g', linestyle='--', label='95% Schwelle')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"infoblaetter/pca-dimensionsreduktion/#wahl-der-komponentenanzahl","title":"Wahl der Komponentenanzahl","text":""},{"location":"infoblaetter/pca-dimensionsreduktion/#methode-1-prozentsatz-der-varianz","title":"Methode 1: Prozentsatz der Varianz","text":"<pre><code># 95% der Varianz erhalten\npca = PCA(n_components=0.95)\nX_reduced = pca.fit_transform(X_scaled)\n\nprint(f\"Anzahl Komponenten: {pca.n_components_}\")\nprint(f\"Erkl\u00e4rte Varianz: {sum(pca.explained_variance_ratio_):.2%}\")\n</code></pre>"},{"location":"infoblaetter/pca-dimensionsreduktion/#methode-2-feste-anzahl","title":"Methode 2: Feste Anzahl","text":"<pre><code># Genau 2 Komponenten f\u00fcr Visualisierung\npca = PCA(n_components=2)\nX_2d = pca.fit_transform(X_scaled)\n</code></pre>"},{"location":"infoblaetter/pca-dimensionsreduktion/#faustregel","title":"Faustregel","text":"Erkl\u00e4rte Varianz Interpretation &gt; 95% Sehr gut, fast keine Information verloren 80-95% Gut f\u00fcr die meisten Anwendungen &lt; 80% M\u00f6glicherweise zu viel Information verloren"},{"location":"infoblaetter/pca-dimensionsreduktion/#features-interpretieren","title":"Features interpretieren","text":"<p>Welche Original-Features stecken in den Hauptkomponenten?</p> <pre><code># Komponenten-Matrix (Loadings)\nloadings = pd.DataFrame(\n    pca.components_.T,\n    columns=['PC1', 'PC2'],\n    index=feature_names\n)\nprint(loadings)\n\n# Visualisierung als Heatmap\nimport seaborn as sns\nplt.figure(figsize=(10, 8))\nsns.heatmap(loadings, annot=True, cmap='coolwarm', center=0)\nplt.title('Feature-Beitr\u00e4ge zu den Hauptkomponenten')\nplt.show()\n</code></pre> <p>Interpretation</p> <p>Hohe absolute Werte zeigen, dass ein Feature stark zur Komponente beitr\u00e4gt. Das Vorzeichen zeigt die Richtung.</p>"},{"location":"infoblaetter/pca-dimensionsreduktion/#pca-clustering-kombinieren","title":"PCA + Clustering kombinieren","text":"<pre><code>from sklearn.cluster import KMeans\n\n# 1. PCA f\u00fcr Dimensionsreduktion\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# 2. K-Means auf reduzierten Daten\nkmeans = KMeans(n_clusters=3, random_state=42)\nlabels = kmeans.fit_predict(X_pca)\n\n# 3. Visualisierung\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\nplt.colorbar(scatter, label='Cluster')\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\nplt.title('K-Means Clustering nach PCA')\nplt.show()\n</code></pre>"},{"location":"infoblaetter/pca-dimensionsreduktion/#alternativen-zu-pca","title":"Alternativen zu PCA","text":"Methode Typ Wann verwenden PCA Linear Standard, schnell, interpretierbar t-SNE Nicht-linear Visualisierung, lokale Strukturen UMAP Nicht-linear Schneller als t-SNE, globale + lokale Struktur"},{"location":"infoblaetter/pca-dimensionsreduktion/#kurz-t-sne","title":"Kurz: t-SNE","text":"<pre><code>from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nX_tsne = tsne.fit_transform(X_scaled)\n\n# Achtung: t-SNE ist langsam und nicht reproduzierbar ohne random_state!\n</code></pre> <p>t-SNE Einschr\u00e4nkungen</p> <ul> <li>Nur f\u00fcr Visualisierung (nicht f\u00fcr Clustering!)</li> <li>Abst\u00e4nde haben keine Bedeutung</li> <li>Sehr langsam bei gro\u00dfen Daten</li> <li>Parameter <code>perplexity</code> beeinflusst Ergebnis stark</li> </ul>"},{"location":"infoblaetter/pca-dimensionsreduktion/#haufige-fehler","title":"H\u00e4ufige Fehler","text":"Fehler Problem L\u00f6sung Nicht skaliert Features mit gro\u00dfen Werten dominieren StandardScaler vor PCA Zu wenig Varianz Information verloren Mehr Komponenten w\u00e4hlen Auf Testdaten fitten Data Leakage Nur auf Trainingsdaten fitten <pre><code># Richtig: fit auf Train, transform auf Test\npca.fit(X_train_scaled)\nX_train_pca = pca.transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n</code></pre>"},{"location":"infoblaetter/pca-dimensionsreduktion/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das Wichtigste</p> <ul> <li>PCA reduziert Dimensionen unter Erhalt der Varianz</li> <li>Immer skalieren vor PCA!</li> <li><code>explained_variance_ratio_</code> zeigt, wie viel Information erhalten bleibt</li> <li>2-3 Komponenten f\u00fcr Visualisierung, mehr f\u00fcr andere Anwendungen</li> <li>PCA + Clustering ist eine starke Kombination</li> </ul> Selbstkontrolle <ol> <li>Was maximiert PCA?</li> <li>Warum ist Skalierung vor PCA wichtig?</li> <li>Was bedeutet eine erkl\u00e4rte Varianz von 85%?</li> <li>Wann ist t-SNE besser als PCA?</li> </ol> Antworten <ol> <li>Die Varianz entlang der neuen Achsen (Hauptkomponenten)</li> <li>Ohne Skalierung dominieren Features mit gro\u00dfen Werten</li> <li>85% der Information aus den Originaldaten bleibt erhalten, 15% gehen verloren</li> <li>F\u00fcr Visualisierung nicht-linearer Strukturen (aber nicht f\u00fcr Clustering!)</li> </ol>"},{"location":"infoblaetter/weitere-algorithmen/","title":"Weitere Clustering-Algorithmen","text":""},{"location":"infoblaetter/weitere-algorithmen/#uberblick","title":"\u00dcberblick","text":"<p>Neben K-Means und hierarchischem Clustering gibt es weitere Algorithmen mit unterschiedlichen St\u00e4rken.</p> Algorithmus Typ Hauptvorteil DBSCAN Dichtebasiert Findet Ausrei\u00dfer, beliebige Formen GMM Probabilistisch Weiche Zuordnung, elliptische Cluster Mean Shift Dichtebasiert Findet k automatisch"},{"location":"infoblaetter/weitere-algorithmen/#dbscan","title":"DBSCAN","text":"<p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise) gruppiert Punkte basierend auf Dichte.</p>"},{"location":"infoblaetter/weitere-algorithmen/#grundkonzept","title":"Grundkonzept","text":"<ul> <li>Core Points: Punkte mit mindestens <code>min_samples</code> Nachbarn im Radius <code>eps</code></li> <li>Border Points: Im Radius eines Core Points, aber selbst kein Core Point</li> <li>Noise: Weder Core noch Border \u2192 Ausrei\u00dfer!</li> </ul> <pre><code>flowchart LR\n    A[Core Point&lt;br/&gt;\u2265 min_samples Nachbarn] --&gt; B[Cluster]\n    C[Border Point&lt;br/&gt;Nachbar von Core] --&gt; B\n    D[Noise Point&lt;br/&gt;Kein Cluster] --&gt; E[Ausrei\u00dfer&lt;br/&gt;Label = -1]</code></pre>"},{"location":"infoblaetter/weitere-algorithmen/#code-beispiel","title":"Code-Beispiel","text":"<pre><code>from sklearn.cluster import DBSCAN\nimport matplotlib.pyplot as plt\n\n# DBSCAN anwenden\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nlabels = dbscan.fit_predict(X_scaled)\n\n# Anzahl Cluster (ohne Noise)\nn_clusters = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise = list(labels).count(-1)\n\nprint(f\"Cluster: {n_clusters}\")\nprint(f\"Ausrei\u00dfer: {n_noise}\")\n</code></pre>"},{"location":"infoblaetter/weitere-algorithmen/#parameter-tunen","title":"Parameter tunen","text":"<p>eps finden mit k-Distance Plot:</p> <pre><code>from sklearn.neighbors import NearestNeighbors\nimport numpy as np\n\n# k-n\u00e4chste Nachbarn berechnen\nk = 5  # = min_samples\nnn = NearestNeighbors(n_neighbors=k)\nnn.fit(X_scaled)\ndistances, _ = nn.kneighbors(X_scaled)\n\n# Sortierte Distanzen plotten\ndistances = np.sort(distances[:, k-1])\nplt.figure(figsize=(10, 6))\nplt.plot(distances)\nplt.xlabel('Punkte (sortiert)')\nplt.ylabel(f'Distanz zum {k}. Nachbarn')\nplt.title('k-Distance Plot zur eps-Bestimmung')\nplt.axhline(y=0.5, color='r', linestyle='--', label='M\u00f6gliches eps')\nplt.legend()\nplt.show()\n</code></pre> <p>eps w\u00e4hlen</p> <p>Der \"Ellbogen\" im k-Distance Plot zeigt einen guten eps-Wert.</p>"},{"location":"infoblaetter/weitere-algorithmen/#vor-und-nachteile","title":"Vor- und Nachteile","text":"Vorteile Nachteile \u2705 Findet beliebige Clusterformen \u274c Parameter eps &amp; min_samples kritisch \u2705 Erkennt Ausrei\u00dfer automatisch \u274c Problematisch bei unterschiedlichen Dichten \u2705 k muss nicht gew\u00e4hlt werden \u274c Langsamer als K-Means"},{"location":"infoblaetter/weitere-algorithmen/#gaussian-mixture-models-gmm","title":"Gaussian Mixture Models (GMM)","text":"<p>GMM modelliert Daten als Mischung von Gau\u00dfverteilungen. Anders als K-Means gibt GMM Wahrscheinlichkeiten f\u00fcr die Clusterzugeh\u00f6rigkeit.</p>"},{"location":"infoblaetter/weitere-algorithmen/#grundidee","title":"Grundidee","text":"<ul> <li>Jedes Cluster ist eine Gau\u00dfverteilung (Normalverteilung)</li> <li>Jeder Punkt hat eine Wahrscheinlichkeit f\u00fcr jedes Cluster</li> <li>Weiche Zuordnung statt harter Labels</li> </ul>"},{"location":"infoblaetter/weitere-algorithmen/#code-beispiel_1","title":"Code-Beispiel","text":"<pre><code>from sklearn.mixture import GaussianMixture\n\n# GMM anwenden\ngmm = GaussianMixture(n_components=3, random_state=42)\ngmm.fit(X_scaled)\n\n# Harte Labels\nlabels = gmm.predict(X_scaled)\n\n# Wahrscheinlichkeiten pro Cluster\nprobabilities = gmm.predict_proba(X_scaled)\nprint(\"Wahrscheinlichkeiten f\u00fcr ersten Punkt:\")\nprint(probabilities[0])  # z.B. [0.02, 0.95, 0.03]\n</code></pre>"},{"location":"infoblaetter/weitere-algorithmen/#kovarianz-typen","title":"Kovarianz-Typen","text":"<pre><code># Verschiedene Kovarianz-Typen\ngmm_full = GaussianMixture(n_components=3, covariance_type='full')    # Flexibel\ngmm_diag = GaussianMixture(n_components=3, covariance_type='diag')    # Achsenparallel\ngmm_spherical = GaussianMixture(n_components=3, covariance_type='spherical')  # Kugelf\u00f6rmig\n</code></pre> Typ Beschreibung Freiheitsgrade <code>full</code> Beliebige Ellipsen Hoch <code>tied</code> Alle Cluster gleiche Form Mittel <code>diag</code> Achsenparallele Ellipsen Niedrig <code>spherical</code> Kugeln (wie K-Means) Sehr niedrig"},{"location":"infoblaetter/weitere-algorithmen/#modellauswahl-mit-bicaic","title":"Modellauswahl mit BIC/AIC","text":"<pre><code>import numpy as np\n\n# Verschiedene Clusteranzahlen testen\nbic_scores = []\naic_scores = []\nK_range = range(1, 10)\n\nfor k in K_range:\n    gmm = GaussianMixture(n_components=k, random_state=42)\n    gmm.fit(X_scaled)\n    bic_scores.append(gmm.bic(X_scaled))\n    aic_scores.append(gmm.aic(X_scaled))\n\n# Bestes k ist das mit niedrigstem BIC\nbest_k = K_range[np.argmin(bic_scores)]\nprint(f\"Optimale Clusteranzahl (BIC): {best_k}\")\n</code></pre>"},{"location":"infoblaetter/weitere-algorithmen/#vor-und-nachteile_1","title":"Vor- und Nachteile","text":"Vorteile Nachteile \u2705 Weiche Zuordnung (Wahrscheinlichkeiten) \u274c Langsamer als K-Means \u2705 Elliptische Cluster m\u00f6glich \u274c Anf\u00e4llig f\u00fcr lokale Optima \u2705 BIC/AIC f\u00fcr Modellwahl \u274c Ben\u00f6tigt mehr Daten"},{"location":"infoblaetter/weitere-algorithmen/#algorithmen-vergleich","title":"Algorithmen-Vergleich","text":"Aspekt K-Means Hierarchisch DBSCAN GMM k w\u00e4hlen Ja Nein* Nein Ja Ausrei\u00dfer Nein Nein Ja! Bedingt Cluster-Form Kugel Flexibel Beliebig Ellipse Zuordnung Hart Hart Hart Weich Geschwindigkeit Sehr schnell Langsam Mittel Mittel Gro\u00dfe Daten Gut Schlecht Mittel Mittel <p>*Kann nachtr\u00e4glich aus Dendrogramm abgelesen werden</p>"},{"location":"infoblaetter/weitere-algorithmen/#wann-welchen-algorithmus","title":"Wann welchen Algorithmus?","text":"<pre><code>flowchart TD\n    A[Start: Welches Clustering?] --&gt; B{Cluster-Form&lt;br/&gt;bekannt?}\n    B --&gt;|Kugelf\u00f6rmig| C[K-Means]\n    B --&gt;|Beliebig| D{Ausrei\u00dfer&lt;br/&gt;wichtig?}\n    D --&gt;|Ja| E[DBSCAN]\n    D --&gt;|Nein| F{Gro\u00dfe Daten?}\n    F --&gt;|Ja| C\n    F --&gt;|Nein| G[Hierarchisch]\n\n    A --&gt; H{Wahrscheinlichkeiten&lt;br/&gt;ben\u00f6tigt?}\n    H --&gt;|Ja| I[GMM]</code></pre> <p>Faustregel</p> <ol> <li>Starte mit K-Means \u2013 schnell und oft gut genug</li> <li>DBSCAN wenn Ausrei\u00dfer wichtig oder Cluster nicht kugelf\u00f6rmig</li> <li>Hierarchisch f\u00fcr Exploration und kleine Datens\u00e4tze</li> <li>GMM wenn Wahrscheinlichkeiten oder elliptische Cluster n\u00f6tig</li> </ol>"},{"location":"infoblaetter/weitere-algorithmen/#zusammenfassung","title":"Zusammenfassung","text":"<p>Das Wichtigste</p> <ul> <li>DBSCAN: Dichtebasiert, findet Ausrei\u00dfer, beliebige Formen</li> <li>GMM: Probabilistisch, weiche Zuordnung, elliptische Cluster</li> <li>Kein Algorithmus ist universell \u00fcberlegen</li> <li>Starte mit K-Means, dann Alternativen testen</li> </ul> Selbstkontrolle <ol> <li>Was bedeutet Label = -1 bei DBSCAN?</li> <li>Was ist der Vorteil von GMM gegen\u00fcber K-Means?</li> <li>Wann ist DBSCAN K-Means vorzuziehen?</li> <li>Wie findest du den eps-Parameter f\u00fcr DBSCAN?</li> </ol> Antworten <ol> <li>Ausrei\u00dfer/Noise \u2013 der Punkt geh\u00f6rt zu keinem Cluster</li> <li>Weiche Zuordnung (Wahrscheinlichkeiten) statt harter Labels</li> <li>Bei nicht-kugelf\u00f6rmigen Clustern oder wenn Ausrei\u00dfer-Erkennung wichtig ist</li> <li>k-Distance Plot: Distanz zum k-ten Nachbarn sortiert plotten, Ellbogen finden</li> </ol>"}]}